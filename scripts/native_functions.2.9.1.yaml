





<!DOCTYPE html>
<html
  lang="en"
  
  data-color-mode="auto" data-light-theme="light" data-dark-theme="dark"
  data-a11y-animated-images="system" data-a11y-link-underlines="true"
  
  >




  <head>
    <meta charset="utf-8">
  <link rel="dns-prefetch" href="https://github.githubassets.com">
  <link rel="dns-prefetch" href="https://avatars.githubusercontent.com">
  <link rel="dns-prefetch" href="https://github-cloud.s3.amazonaws.com">
  <link rel="dns-prefetch" href="https://user-images.githubusercontent.com/">
  <link rel="preconnect" href="https://github.githubassets.com" crossorigin>
  <link rel="preconnect" href="https://avatars.githubusercontent.com">

  


  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/light-dac525bbd821.css" /><link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/light_high_contrast-56ccf4057897.css" /><link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/dark-784387e86ac0.css" /><link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/dark_high_contrast-79bd5fd84a86.css" /><link data-color-theme="light" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light-dac525bbd821.css" /><link data-color-theme="light_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_high_contrast-56ccf4057897.css" /><link data-color-theme="light_colorblind" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_colorblind-0e24752a7d2b.css" /><link data-color-theme="light_colorblind_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_colorblind_high_contrast-412af2517363.css" /><link data-color-theme="light_tritanopia" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_tritanopia-6186e83663dc.css" /><link data-color-theme="light_tritanopia_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/light_tritanopia_high_contrast-9d33c7aea2e7.css" /><link data-color-theme="dark" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark-784387e86ac0.css" /><link data-color-theme="dark_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_high_contrast-79bd5fd84a86.css" /><link data-color-theme="dark_colorblind" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_colorblind-75db11311555.css" /><link data-color-theme="dark_colorblind_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_colorblind_high_contrast-f2c1045899a2.css" /><link data-color-theme="dark_tritanopia" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_tritanopia-f46d293c6ff3.css" /><link data-color-theme="dark_tritanopia_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_tritanopia_high_contrast-e4b5684db29d.css" /><link data-color-theme="dark_dimmed" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_dimmed-72c58078e707.css" /><link data-color-theme="dark_dimmed_high_contrast" crossorigin="anonymous" media="all" rel="stylesheet" data-href="https://github.githubassets.com/assets/dark_dimmed_high_contrast-956cb5dfcb85.css" />

  <style type="text/css">
    :root {
      --tab-size-preference: 4;
    }

    pre, code {
      tab-size: var(--tab-size-preference);
    }
  </style>

    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-primitives-c37d781e2da5.css" />
    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-dc3bfaf4b78e.css" />
    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/global-b22d9c324b1b.css" />
    <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/github-f7230554fa20.css" />
  <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/repository-5d735668c600.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/code-9c9b8dc61e74.css" />

  

  <script type="application/json" id="client-env">{"locale":"en","featureFlags":["a11y_status_checks_ruleset","actions_custom_images_public_preview_visibility","actions_custom_images_storage_billing_ui_visibility","actions_enable_snapshot_keyword","actions_image_version_event","allow_react_navs_in_turbo","alternate_user_config_repo","api_insights_show_missing_data_banner","arianotify_comprehensive_migration","arianotify_partial_migration","client_version_header","codespaces_prebuild_region_target_update","coding_agent_model_selection","contentful_lp_footnotes","copilot_agent_cli_public_preview","copilot_agent_sessions_alive_updates","copilot_agent_task_list_v2","copilot_agent_tasks_btn_code_nav","copilot_agent_tasks_btn_code_view","copilot_agent_tasks_btn_code_view_lines","copilot_agent_tasks_btn_repo","copilot_api_agentic_issue_marshal_yaml","copilot_api_draft_issue_reference_with_project_id","copilot_api_github_draft_update_issue_skill","copilot_chat_agents_empty_state","copilot_chat_attach_multiple_images","copilot_chat_clear_model_selection_for_default_change","copilot_chat_file_redirect","copilot_chat_input_commands","copilot_chat_opening_thread_switch","copilot_chat_reduce_quota_checks","copilot_chat_search_bar_redirect","copilot_chat_selection_attachments","copilot_chat_vision_in_claude","copilot_chat_vision_preview_gate","copilot_coding_agent_task_response","copilot_custom_copilots","copilot_custom_copilots_feature_preview","copilot_duplicate_thread","copilot_extensions_hide_in_dotcom_chat","copilot_extensions_removal_on_marketplace","copilot_features_raycast_logo","copilot_file_block_ref_matching","copilot_ftp_hyperspace_upgrade_prompt","copilot_icebreakers_experiment_dashboard","copilot_icebreakers_experiment_hyperspace","copilot_immersive_generate_thread_name_async","copilot_immersive_job_result_preview","copilot_immersive_structured_model_picker","copilot_immersive_task_hyperlinking","copilot_immersive_task_within_chat_thread","copilot_org_policy_page_focus_mode","copilot_redirect_header_button_to_agents","copilot_security_alert_assignee_options","copilot_share_active_subthread","copilot_spaces_ga","copilot_spaces_individual_policies_ga","copilot_spaces_public_access_to_user_owned_spaces","copilot_spaces_read_access_to_user_owned_spaces","copilot_spaces_report_abuse","copilot_spark_empty_state","copilot_spark_handle_nil_friendly_name","copilot_spark_loading_webgl","copilot_stable_conversation_view","copilot_swe_agent_progress_commands","copilot_swe_agent_use_subagents","copilot_unconfigured_is_inherited","dashboard_universe_2025_feedback_dialog","direct_to_salesforce","dom_node_counts","dotcom_chat_client_side_skills","enterprise_ai_controls","failbot_report_error_react_apps_on_page","fetch_graphql_improved_error_serialization","flex_cta_groups_mvp","global_nav_react_edit_status_dialog","global_nav_react_feature_preview","global_nav_react_teams_settings_page","global_nav_react_top_repos_api_caching","hyperspace_2025_logged_out_batch_1","initial_per_page_pagination_updates","issue_fields_global_search","issue_fields_report_usage","issue_fields_timeline_events","issues_cca_assign_actor_with_agent","issues_expanded_file_types","issues_lazy_load_comment_box_suggestions","issues_react_bots_timeline_pagination","issues_react_chrome_container_query_fix","issues_react_client_side_caching_analytics","issues_react_prohibit_title_fallback","issues_react_use_turbo_for_cross_repo_navigation","issues_report_sidebar_interactions","lifecycle_label_name_updates","link_contact_sales_swp_marketo","marketing_pages_search_explore_provider","memex_default_issue_create_repository","memex_grouped_by_edit_route","memex_mwl_filter_field_delimiter","mission_control_use_body_html","new_traffic_page_banner","open_agent_session_in_vscode_insiders","open_agent_session_in_vscode_stable","projects_assignee_max_limit","react_fetch_graphql_ignore_expected_errors","react_next","render_user_display_name","repo_traffic_job","repos_insights_remove_new_url","ruleset_deletion_confirmation","sample_network_conn_type","scheduled_reminders_updated_limits","site_calculator_actions_2025","site_features_copilot_universe","site_homepage_collaborate_video","site_homepage_contentful","site_homepage_eyebrow_banner","site_homepage_universe_animations","site_msbuild_webgl_hero","spark_prompt_secret_scanning","swe_agent_member_requests","viewscreen_sandbox","webp_support","workbench_store_readonly"],"copilotApiOverrideUrl":"https://api.githubcopilot.com"}</script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/high-contrast-cookie-ff2c933fbe48.js"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/wp-runtime-55b80416cdea.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/913-ca2305638c53.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/6488-de87864e6818.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/environment-d5e2105084ee.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/69676-3e4d0020216a.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/43784-4652ae97a661.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/4712-6fc930a63a4b.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/81028-5b8c5e07a4fa.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/97292-9890ce009c4e.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/91853-2ed22fb46437.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/78143-31968346cf4c.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/52430-2f44a4a36933.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/github-elements-8b5d9b8ccf1f.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/element-registry-9b7daecde35f.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/react-lib-dd121329c564.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/react-core-c401b07c6782.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/28546-ee41c9313871.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/17688-a9e16fb5ed13.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/2869-a4ba8f17edb3.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/70191-5122bf27bf3e.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/7332-5ea4ccf72018.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/3561-5983d983527e.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/24077-adc459723b71.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/51519-fbe386e3932c.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/27376-9825e0a74a24.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/96384-750ef5263abe.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/19718-676a65610616.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/behaviors-7b6a98bc4afa.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/48011-1f20a5c80dd7.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/notifications-global-54f7f2032e0d.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/31615-236504c8966f.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/14155-9ff42e66b353.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/code-menu-fa1d4025778b.js" defer="defer"></script>
  
  <script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/primer-react-c3b562318f66.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/octicons-react-531a0b40e156.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/31475-22e3fd4a7b87.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/48775-3cc79d2cd30e.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/42892-86c578f4fa0a.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/23832-ab2822f14123.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/99418-9d4961969e0d.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/33915-05ba9b3edc31.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/96537-8e29101f7d81.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/51220-1dc35ec0bcd6.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/14439-a1591d60e882.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/59403-eeb82acf55b3.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/9288-3a5a3d16c30f.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/25407-0bcfbb5d10a7.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/40771-25ebf1ba365b.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/66990-bfcd07235bcc.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/29665-19fe77f202dd.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/91232-8a32c5527a07.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/4202-0661543791f2.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/6623-89af6bf0656a.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/46247-9c416bb1d111.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/55754-8bd9708f984d.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/36584-9e7b7b0f92f6.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/3966-df7f182500fc.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/99451-7c90b83d9758.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/react-code-view-9fda2c98709d.js" defer="defer"></script>
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.ab16af462b02f294298e.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/react-code-view.a3c8b35c12a7c333d171.module.css" />

  <script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/57972-9f6e8ea3635d.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/notifications-subscriptions-menu-d002081209f2.js" defer="defer"></script>
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.ab16af462b02f294298e.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/notifications-subscriptions-menu.933100a30c03fd4e8ae4.module.css" />


  <title>pytorch/aten/src/ATen/native/native_functions.yaml at v2.9.1 · pytorch/pytorch · GitHub</title>



  <meta name="route-pattern" content="/:user_id/:repository/blob/*name(/*path)" data-turbo-transient>
  <meta name="route-controller" content="blob" data-turbo-transient>
  <meta name="route-action" content="show" data-turbo-transient>
  <meta name="fetch-nonce" content="v2:5d07e3ca-7011-6282-c0d7-c515aa051d42">

    
  <meta name="current-catalog-service-hash" content="f3abb0cc802f3d7b95fc8762b94bdcb13bf39634c40c357301c4aa1d67a256fb">


  <meta name="request-id" content="A651:11646A:55045A1:460D120:695C45B3" data-pjax-transient="true"/><meta name="html-safe-nonce" content="f3437d6df19d572cd7507024c8c390acd65d25e354f31ace9db491fb761e5aff" data-pjax-transient="true"/><meta name="visitor-payload" content="eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiJBNjUxOjExNjQ2QTo1NTA0NUExOjQ2MEQxMjA6Njk1QzQ1QjMiLCJ2aXNpdG9yX2lkIjoiMzAyODY2MDAyNjY0MDk3NTI4MyIsInJlZ2lvbl9lZGdlIjoiZnJhIiwicmVnaW9uX3JlbmRlciI6ImZyYSJ9" data-pjax-transient="true"/><meta name="visitor-hmac" content="0eb1532b9a25f4c2f5fddbcd098e1c061f2224a239d10be52b31aae710a92ad8" data-pjax-transient="true"/>


    <meta name="hovercard-subject-tag" content="repository:65600975" data-turbo-transient>


  <meta name="github-keyboard-shortcuts" content="repository,source-code,file-tree,copilot" data-turbo-transient="true" />
  

  <meta name="selected-link" value="repo_source" data-turbo-transient>
  <link rel="assets" href="https://github.githubassets.com/">

    <meta name="google-site-verification" content="Apib7-x98H0j5cPqHWwSMm6dNU4GmODRoqxLiDzdx9I">

<meta name="octolytics-url" content="https://collector.github.com/github/collect" />

  <meta name="analytics-location" content="/&lt;user-name&gt;/&lt;repo-name&gt;/blob/show" data-turbo-transient="true" />

  




    <meta name="user-login" content="">

  

    <meta name="viewport" content="width=device-width">

    

      <meta name="description" content="Tensors and Dynamic neural networks in Python with strong GPU acceleration - pytorch/aten/src/ATen/native/native_functions.yaml at v2.9.1 · pytorch/pytorch">

      <link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="GitHub">

    <link rel="fluid-icon" href="https://github.com/fluidicon.png" title="GitHub">
    <meta property="fb:app_id" content="1401488693436528">
    <meta name="apple-itunes-app" content="app-id=1477376905, app-argument=https://github.com/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml" />

      <meta name="twitter:image" content="https://opengraph.githubassets.com/e3d7e2ac265bc22259237f7015f28a18386c2fe6011f9ffb78a46e3c8ecdfa35/pytorch/pytorch" /><meta name="twitter:site" content="@github" /><meta name="twitter:card" content="summary_large_image" /><meta name="twitter:title" content="pytorch/aten/src/ATen/native/native_functions.yaml at v2.9.1 · pytorch/pytorch" /><meta name="twitter:description" content="Tensors and Dynamic neural networks in Python with strong GPU acceleration - pytorch/pytorch" />
  <meta property="og:image" content="https://opengraph.githubassets.com/e3d7e2ac265bc22259237f7015f28a18386c2fe6011f9ffb78a46e3c8ecdfa35/pytorch/pytorch" /><meta property="og:image:alt" content="Tensors and Dynamic neural networks in Python with strong GPU acceleration - pytorch/pytorch" /><meta property="og:image:width" content="1200" /><meta property="og:image:height" content="600" /><meta property="og:site_name" content="GitHub" /><meta property="og:type" content="object" /><meta property="og:title" content="pytorch/aten/src/ATen/native/native_functions.yaml at v2.9.1 · pytorch/pytorch" /><meta property="og:url" content="https://github.com/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml" /><meta property="og:description" content="Tensors and Dynamic neural networks in Python with strong GPU acceleration - pytorch/pytorch" />
  




      <meta name="hostname" content="github.com">



        <meta name="expected-hostname" content="github.com">


  <meta http-equiv="x-pjax-version" content="3f9ef5a1d8765c01ff5186553efbd434e7717d65d4db513f48d8fc0eabda7825" data-turbo-track="reload">
  <meta http-equiv="x-pjax-csp-version" content="21a43568025709b66240454fc92d4f09335a96863f8ab1c46b4a07f6a5b67102" data-turbo-track="reload">
  <meta http-equiv="x-pjax-css-version" content="2de9bf631fd41d5d00cd89b86b9a2dddf0d56865031ac6705d4d28c28e096903" data-turbo-track="reload">
  <meta http-equiv="x-pjax-js-version" content="d9e6a8ec0e89bc316282697ba133c6f8c19e540f91cbcaf227d0880724faeb14" data-turbo-track="reload">

  <meta name="turbo-cache-control" content="no-preview" data-turbo-transient="">

      <meta name="turbo-cache-control" content="no-cache" data-turbo-transient>

    <meta data-hydrostats="publish">

  <meta name="go-import" content="github.com/pytorch/pytorch git https://github.com/pytorch/pytorch.git">

  <meta name="octolytics-dimension-user_id" content="21003710" /><meta name="octolytics-dimension-user_login" content="pytorch" /><meta name="octolytics-dimension-repository_id" content="65600975" /><meta name="octolytics-dimension-repository_nwo" content="pytorch/pytorch" /><meta name="octolytics-dimension-repository_public" content="true" /><meta name="octolytics-dimension-repository_is_fork" content="false" /><meta name="octolytics-dimension-repository_network_root_id" content="65600975" /><meta name="octolytics-dimension-repository_network_root_nwo" content="pytorch/pytorch" />



    

    <meta name="turbo-body-classes" content="logged-out env-production page-responsive">
  <meta name="disable-turbo" content="false">


  <meta name="browser-stats-url" content="https://api.github.com/_private/browser/stats">

  <meta name="browser-errors-url" content="https://api.github.com/_private/browser/errors">

  <meta name="release" content="1631e7fda67db5c28eefc0ae8efdcac937ddd258">
  <meta name="ui-target" content="canary-1">

  <link rel="mask-icon" href="https://github.githubassets.com/assets/pinned-octocat-093da3e6fa40.svg" color="#000000">
  <link rel="alternate icon" class="js-site-favicon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png">
  <link rel="icon" class="js-site-favicon" type="image/svg+xml" href="https://github.githubassets.com/favicons/favicon.svg" data-base-href="https://github.githubassets.com/favicons/favicon">

<meta name="theme-color" content="#1e2327">
<meta name="color-scheme" content="light dark" />


  <link rel="manifest" href="/manifest.json" crossOrigin="use-credentials">

  </head>

  <body class="logged-out env-production page-responsive" style="word-wrap: break-word;" >
    <div data-turbo-body class="logged-out env-production page-responsive" style="word-wrap: break-word;" >
      <div id="__primerPortalRoot__" role="region" style="z-index: 1000; position: absolute; width: 100%;" data-turbo-permanent></div>
      



    <div class="position-relative header-wrapper js-header-wrapper ">
      <a href="#start-of-content" data-skip-target-assigned="false" class="px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content">Skip to content</a>

      <span data-view-component="true" class="progress-pjax-loader Progress position-fixed width-full">
    <span style="width: 0%;" data-view-component="true" class="Progress-item progress-pjax-loader-bar left-0 top-0 color-bg-accent-emphasis"></span>
</span>      
      
      <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.ab16af462b02f294298e.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/keyboard-shortcuts-dialog.29aaeaafa90f007c6f61.module.css" />

<react-partial
  partial-name="keyboard-shortcuts-dialog"
  data-ssr="false"
  data-attempted-ssr="false"
  data-react-profiling="false"
>
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{"docsUrl":"https://docs.github.com/get-started/accessibility/keyboard-shortcuts"}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>





      

          

              
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/43862-5c4df3ba1119.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/85110-f7be2f54525a.js" defer="defer"></script>
<script crossorigin="anonymous" type="application/javascript" src="https://github.githubassets.com/assets/sessions-1562f1f56545.js" defer="defer"></script>

<style>
  /* Override primer focus outline color for marketing header dropdown links for better contrast */
  [data-color-mode="light"] .HeaderMenu-dropdown-link:focus-visible,
  [data-color-mode="light"] .HeaderMenu-trailing-link a:focus-visible {
    outline-color: var(--color-accent-fg);
  }
</style>

<header class="HeaderMktg header-logged-out js-details-container js-header Details f4 py-3" role="banner" data-is-top="true" data-color-mode=light data-light-theme=light data-dark-theme=dark>
  <h2 class="sr-only">Navigation Menu</h2>

  <button type="button" class="HeaderMktg-backdrop d-lg-none border-0 position-fixed top-0 left-0 width-full height-full js-details-target" aria-label="Toggle navigation">
    <span class="d-none">Toggle navigation</span>
  </button>

  <div class="d-flex flex-column flex-lg-row flex-items-center px-3 px-md-4 px-lg-5 height-full position-relative z-1">
    <div class="d-flex flex-justify-between flex-items-center width-full width-lg-auto">
      <div class="flex-1">
        <button aria-label="Toggle navigation" aria-expanded="false" type="button" data-view-component="true" class="js-details-target js-nav-padding-recalculate js-header-menu-toggle Button--link Button--medium Button d-lg-none color-fg-inherit p-1">  <span class="Button-content">
    <span class="Button-label"><div class="HeaderMenu-toggle-bar rounded my-1"></div>
            <div class="HeaderMenu-toggle-bar rounded my-1"></div>
            <div class="HeaderMenu-toggle-bar rounded my-1"></div></span>
  </span>
</button>
      </div>

      <a class="mr-lg-3 color-fg-inherit flex-order-2 js-prevent-focus-on-mobile-nav"
        href="/"
        aria-label="Homepage"
        data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to go to homepage&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Logomark;ref_loc:Header&quot;}">
        <svg height="32" aria-hidden="true" viewBox="0 0 24 24" version="1.1" width="32" data-view-component="true" class="octicon octicon-mark-github">
    <path d="M12 1C5.923 1 1 5.923 1 12c0 4.867 3.149 8.979 7.521 10.436.55.096.756-.233.756-.522 0-.262-.013-1.128-.013-2.049-2.764.509-3.479-.674-3.699-1.292-.124-.317-.66-1.293-1.127-1.554-.385-.207-.936-.715-.014-.729.866-.014 1.485.797 1.691 1.128.99 1.663 2.571 1.196 3.204.907.096-.715.385-1.196.701-1.471-2.448-.275-5.005-1.224-5.005-5.432 0-1.196.426-2.186 1.128-2.956-.111-.275-.496-1.402.11-2.915 0 0 .921-.288 3.024 1.128a10.193 10.193 0 0 1 2.75-.371c.936 0 1.871.123 2.75.371 2.104-1.43 3.025-1.128 3.025-1.128.605 1.513.221 2.64.111 2.915.701.77 1.127 1.747 1.127 2.956 0 4.222-2.571 5.157-5.019 5.432.399.344.743 1.004.743 2.035 0 1.471-.014 2.654-.014 3.025 0 .289.206.632.756.522C19.851 20.979 23 16.854 23 12c0-6.077-4.922-11-11-11Z"></path>
</svg>
      </a>

      <div class="d-flex flex-1 flex-order-2 text-right d-lg-none gap-2 flex-justify-end">
          <a
            href="/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fblob%2Fv2.9.1%2Faten%2Fsrc%2FATen%2Fnative%2Fnative_functions.yaml"
            class="HeaderMenu-link HeaderMenu-button d-inline-flex f5 no-underline border color-border-default rounded-2 px-2 py-1 color-fg-inherit js-prevent-focus-on-mobile-nav"
            data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="6e483a8da0bfe44650a29bc61282708a1adbb4fa59703afbf93c6ca516fbd561"
            data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to Sign in&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Sign in;ref_loc:Header&quot;}"
          >
            Sign in
          </a>
              <div class="AppHeader-appearanceSettings">
    <react-partial-anchor>
      <button data-target="react-partial-anchor.anchor" id="icon-button-1d8c5d6b-dc0c-4866-9f4c-cfbc82a09f12" aria-labelledby="tooltip-6ece6cf4-3bdf-4955-a9e8-3d0e8b7b6b6d" type="button" disabled="disabled" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium AppHeader-button HeaderMenu-link border cursor-wait">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-sliders Button-visual">
    <path d="M15 2.75a.75.75 0 0 1-.75.75h-4a.75.75 0 0 1 0-1.5h4a.75.75 0 0 1 .75.75Zm-8.5.75v1.25a.75.75 0 0 0 1.5 0v-4a.75.75 0 0 0-1.5 0V2H1.75a.75.75 0 0 0 0 1.5H6.5Zm1.25 5.25a.75.75 0 0 0 0-1.5h-6a.75.75 0 0 0 0 1.5h6ZM15 8a.75.75 0 0 1-.75.75H11.5V10a.75.75 0 1 1-1.5 0V6a.75.75 0 0 1 1.5 0v1.25h2.75A.75.75 0 0 1 15 8Zm-9 5.25v-2a.75.75 0 0 0-1.5 0v1.25H1.75a.75.75 0 0 0 0 1.5H4.5v1.25a.75.75 0 0 0 1.5 0v-2Zm9 0a.75.75 0 0 1-.75.75h-6a.75.75 0 0 1 0-1.5h6a.75.75 0 0 1 .75.75Z"></path>
</svg>
</button><tool-tip id="tooltip-6ece6cf4-3bdf-4955-a9e8-3d0e8b7b6b6d" for="icon-button-1d8c5d6b-dc0c-4866-9f4c-cfbc82a09f12" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.ab16af462b02f294298e.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.753d458774a2f782559b.module.css" />

<react-partial
  partial-name="appearance-settings"
  data-ssr="false"
  data-attempted-ssr="false"
  data-react-profiling="false"
>
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </div>

      </div>
    </div>


    <div class="HeaderMenu js-header-menu height-fit position-lg-relative d-lg-flex flex-column flex-auto top-0">
      <div class="HeaderMenu-wrapper d-flex flex-column flex-self-start flex-lg-row flex-auto rounded rounded-lg-0">
            <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.ab16af462b02f294298e.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/marketing-navigation.8284bdfe1ee4804a58c1.module.css" />

<react-partial
  partial-name="marketing-navigation"
  data-ssr="true"
  data-attempted-ssr="true"
  data-react-profiling="false"
>
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{"should_use_dotcom_links":true}}</script>
  <div data-target="react-partial.reactRoot"><nav class="MarketingNavigation-module__nav--jA9Zq" aria-label="Global"><ul class="MarketingNavigation-module__list--r_vr2"><li><div class="NavDropdown-module__container--bmXM2 js-details-container js-header-menu-item"><button type="button" class="NavDropdown-module__button--Hq9UR js-details-target" aria-expanded="false">Platform<svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavDropdown-module__buttonIcon--SR0Ke" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></button><div class="NavDropdown-module__dropdown--Ig57Y"><ul class="NavDropdown-module__list--RwSSK"><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">AI CODE CREATION</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/features/copilot" data-analytics-event="{&quot;action&quot;:&quot;github_copilot&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-copilot NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z"></path><path d="M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Copilot</span><span class="NavLink-module__subtitle--qC15H">Write better code with AI</span></div></a></li><li><a href="https://github.com/features/spark" data-analytics-event="{&quot;action&quot;:&quot;github_spark&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-sparkle-fill NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M11.296 1.924c.24-.656 1.168-.656 1.408 0l.717 1.958a11.25 11.25 0 0 0 6.697 6.697l1.958.717c.657.24.657 1.168 0 1.408l-1.958.717a11.25 11.25 0 0 0-6.697 6.697l-.717 1.958c-.24.657-1.168.657-1.408 0l-.717-1.958a11.25 11.25 0 0 0-6.697-6.697l-1.958-.717c-.656-.24-.656-1.168 0-1.408l1.958-.717a11.25 11.25 0 0 0 6.697-6.697l.717-1.958Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Spark</span><span class="NavLink-module__subtitle--qC15H">Build and deploy intelligent apps</span></div></a></li><li><a href="https://github.com/features/models" data-analytics-event="{&quot;action&quot;:&quot;github_models&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-ai-model NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M19.375 8.5a3.25 3.25 0 1 1-3.163 4h-3a3.252 3.252 0 0 1-4.443 2.509L7.214 17.76a3.25 3.25 0 1 1-1.342-.674l1.672-2.957A3.238 3.238 0 0 1 6.75 12c0-.907.371-1.727.97-2.316L6.117 6.846A3.253 3.253 0 0 1 1.875 3.75a3.25 3.25 0 1 1 5.526 2.32l1.603 2.836A3.25 3.25 0 0 1 13.093 11h3.119a3.252 3.252 0 0 1 3.163-2.5ZM10 10.25a1.75 1.75 0 1 0-.001 3.499A1.75 1.75 0 0 0 10 10.25ZM5.125 2a1.75 1.75 0 1 0 0 3.5 1.75 1.75 0 0 0 0-3.5Zm12.5 9.75a1.75 1.75 0 1 0 3.5 0 1.75 1.75 0 0 0-3.5 0Zm-14.25 8.5a1.75 1.75 0 1 0 3.501-.001 1.75 1.75 0 0 0-3.501.001Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Models</span><span class="NavLink-module__subtitle--qC15H">Manage and compare prompts</span></div></a></li><li><a href="https://github.com/mcp" data-analytics-event="{&quot;action&quot;:&quot;mcp_registry&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;mcp_registry_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-mcp NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M9.795 1.694a4.287 4.287 0 0 1 6.061 0 4.28 4.28 0 0 1 1.181 3.819 4.282 4.282 0 0 1 3.819 1.181 4.287 4.287 0 0 1 0 6.061l-6.793 6.793a.249.249 0 0 0 0 .353l2.617 2.618a.75.75 0 1 1-1.061 1.061l-2.617-2.618a1.75 1.75 0 0 1 0-2.475l6.793-6.793a2.785 2.785 0 1 0-3.939-3.939l-5.9 5.9a.734.734 0 0 1-.249.165.749.749 0 0 1-.812-1.225l5.9-5.901a2.785 2.785 0 1 0-3.939-3.939L2.931 10.68A.75.75 0 1 1 1.87 9.619l7.925-7.925Z"></path><path d="M12.42 4.069a.752.752 0 0 1 1.061 0 .752.752 0 0 1 0 1.061L7.33 11.28a2.788 2.788 0 0 0 0 3.94 2.788 2.788 0 0 0 3.94 0l6.15-6.151a.752.752 0 0 1 1.061 0 .752.752 0 0 1 0 1.061l-6.151 6.15a4.285 4.285 0 1 1-6.06-6.06l6.15-6.151Z"></path></svg><span class="NavLink-module__title--xw3ok">MCP Registry<sup class="NavLink-module__label--MrIhm">New</sup></span><span class="NavLink-module__subtitle--qC15H">Integrate external tools</span></div></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">DEVELOPER WORKFLOWS</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/features/actions" data-analytics-event="{&quot;action&quot;:&quot;actions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;actions_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-workflow NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M1 3a2 2 0 0 1 2-2h6.5a2 2 0 0 1 2 2v6.5a2 2 0 0 1-2 2H7v4.063C7 16.355 7.644 17 8.438 17H12.5v-2.5a2 2 0 0 1 2-2H21a2 2 0 0 1 2 2V21a2 2 0 0 1-2 2h-6.5a2 2 0 0 1-2-2v-2.5H8.437A2.939 2.939 0 0 1 5.5 15.562V11.5H3a2 2 0 0 1-2-2Zm2-.5a.5.5 0 0 0-.5.5v6.5a.5.5 0 0 0 .5.5h6.5a.5.5 0 0 0 .5-.5V3a.5.5 0 0 0-.5-.5ZM14.5 14a.5.5 0 0 0-.5.5V21a.5.5 0 0 0 .5.5H21a.5.5 0 0 0 .5-.5v-6.5a.5.5 0 0 0-.5-.5Z"></path></svg><span class="NavLink-module__title--xw3ok">Actions</span><span class="NavLink-module__subtitle--qC15H">Automate any workflow</span></div></a></li><li><a href="https://github.com/features/codespaces" data-analytics-event="{&quot;action&quot;:&quot;codespaces&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;codespaces_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-codespaces NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.5 3.75C3.5 2.784 4.284 2 5.25 2h13.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 18.75 13H5.25a1.75 1.75 0 0 1-1.75-1.75Zm-2 12c0-.966.784-1.75 1.75-1.75h17.5c.966 0 1.75.784 1.75 1.75v4a1.75 1.75 0 0 1-1.75 1.75H3.25a1.75 1.75 0 0 1-1.75-1.75ZM5.25 3.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h13.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Zm-2 12a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h17.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25Z"></path><path d="M10 17.75a.75.75 0 0 1 .75-.75h6.5a.75.75 0 0 1 0 1.5h-6.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z"></path></svg><span class="NavLink-module__title--xw3ok">Codespaces</span><span class="NavLink-module__subtitle--qC15H">Instant dev environments</span></div></a></li><li><a href="https://github.com/features/issues" data-analytics-event="{&quot;action&quot;:&quot;issues&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;issues_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-issue-opened NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1ZM2.5 12a9.5 9.5 0 0 0 9.5 9.5 9.5 9.5 0 0 0 9.5-9.5A9.5 9.5 0 0 0 12 2.5 9.5 9.5 0 0 0 2.5 12Zm9.5 2a2 2 0 1 1-.001-3.999A2 2 0 0 1 12 14Z"></path></svg><span class="NavLink-module__title--xw3ok">Issues</span><span class="NavLink-module__subtitle--qC15H">Plan and track work</span></div></a></li><li><a href="https://github.com/features/code-review" data-analytics-event="{&quot;action&quot;:&quot;code_review&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;code_review_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-code NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M15.22 4.97a.75.75 0 0 1 1.06 0l6.5 6.5a.75.75 0 0 1 0 1.06l-6.5 6.5a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L21.19 12l-5.97-5.97a.75.75 0 0 1 0-1.06Zm-6.44 0a.75.75 0 0 1 0 1.06L2.81 12l5.97 5.97a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-6.5-6.5a.75.75 0 0 1 0-1.06l6.5-6.5a.75.75 0 0 1 1.06 0Z"></path></svg><span class="NavLink-module__title--xw3ok">Code Review</span><span class="NavLink-module__subtitle--qC15H">Manage code changes</span></div></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">APPLICATION SECURITY</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/security/advanced-security" data-analytics-event="{&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_advanced_security_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-shield-check NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z"></path><path d="m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Advanced Security</span><span class="NavLink-module__subtitle--qC15H">Find and fix vulnerabilities</span></div></a></li><li><a href="https://github.com/security/advanced-security/code-security" data-analytics-event="{&quot;action&quot;:&quot;code_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;code_security_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-code-square NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M10.3 8.24a.75.75 0 0 1-.04 1.06L7.352 12l2.908 2.7a.75.75 0 1 1-1.02 1.1l-3.5-3.25a.75.75 0 0 1 0-1.1l3.5-3.25a.75.75 0 0 1 1.06.04Zm3.44 1.06a.75.75 0 1 1 1.02-1.1l3.5 3.25a.75.75 0 0 1 0 1.1l-3.5 3.25a.75.75 0 1 1-1.02-1.1l2.908-2.7-2.908-2.7Z"></path><path d="M2 3.75C2 2.784 2.784 2 3.75 2h16.5c.966 0 1.75.784 1.75 1.75v16.5A1.75 1.75 0 0 1 20.25 22H3.75A1.75 1.75 0 0 1 2 20.25Zm1.75-.25a.25.25 0 0 0-.25.25v16.5c0 .138.112.25.25.25h16.5a.25.25 0 0 0 .25-.25V3.75a.25.25 0 0 0-.25-.25Z"></path></svg><span class="NavLink-module__title--xw3ok">Code security</span><span class="NavLink-module__subtitle--qC15H">Secure your code as you build</span></div></a></li><li><a href="https://github.com/security/advanced-security/secret-protection" data-analytics-event="{&quot;action&quot;:&quot;secret_protection&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;secret_protection_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-lock NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6 9V7.25C6 3.845 8.503 1 12 1s6 2.845 6 6.25V9h.5a2.5 2.5 0 0 1 2.5 2.5v8a2.5 2.5 0 0 1-2.5 2.5h-13A2.5 2.5 0 0 1 3 19.5v-8A2.5 2.5 0 0 1 5.5 9Zm-1.5 2.5v8a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-8a1 1 0 0 0-1-1h-13a1 1 0 0 0-1 1Zm3-4.25V9h9V7.25c0-2.67-1.922-4.75-4.5-4.75-2.578 0-4.5 2.08-4.5 4.75Z"></path></svg><span class="NavLink-module__title--xw3ok">Secret protection</span><span class="NavLink-module__subtitle--qC15H">Stop leaks before they start</span></div></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n NavGroup-module__hasSeparator--AJeNz"><span class="NavGroup-module__title--TdKyz">EXPLORE</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/why-github" data-analytics-event="{&quot;action&quot;:&quot;why_github&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;why_github_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Why GitHub</span></a></li><li><a href="https://docs.github.com" data-analytics-event="{&quot;action&quot;:&quot;documentation&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;documentation_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Documentation</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://github.blog" data-analytics-event="{&quot;action&quot;:&quot;blog&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;blog_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Blog</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://github.blog/changelog" data-analytics-event="{&quot;action&quot;:&quot;changelog&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;changelog_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Changelog</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://github.com/marketplace" data-analytics-event="{&quot;action&quot;:&quot;marketplace&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;marketplace_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Marketplace</span></a></li></ul></div></li></ul><div class="NavDropdown-module__trailingLinkContainer--MNB5T"><a href="https://github.com/features" data-analytics-event="{&quot;action&quot;:&quot;view_all_features&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">View all features</span><svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavLink-module__arrowIcon--g6Lip" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></a></div></div></div></li><li><div class="NavDropdown-module__container--bmXM2 js-details-container js-header-menu-item"><button type="button" class="NavDropdown-module__button--Hq9UR js-details-target" aria-expanded="false">Solutions<svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavDropdown-module__buttonIcon--SR0Ke" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></button><div class="NavDropdown-module__dropdown--Ig57Y"><ul class="NavDropdown-module__list--RwSSK"><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">BY COMPANY SIZE</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/enterprise" data-analytics-event="{&quot;action&quot;:&quot;enterprises&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;enterprises_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Enterprises</span></a></li><li><a href="https://github.com/team" data-analytics-event="{&quot;action&quot;:&quot;small_and_medium_teams&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;small_and_medium_teams_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Small and medium teams</span></a></li><li><a href="https://github.com/enterprise/startups" data-analytics-event="{&quot;action&quot;:&quot;startups&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;startups_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Startups</span></a></li><li><a href="https://github.com/solutions/industry/nonprofits" data-analytics-event="{&quot;action&quot;:&quot;nonprofits&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;nonprofits_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Nonprofits</span></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">BY USE CASE</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/solutions/use-case/app-modernization" data-analytics-event="{&quot;action&quot;:&quot;app_modernization&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;app_modernization_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">App Modernization</span></a></li><li><a href="https://github.com/solutions/use-case/devsecops" data-analytics-event="{&quot;action&quot;:&quot;devsecops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devsecops_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">DevSecOps</span></a></li><li><a href="https://github.com/solutions/use-case/devops" data-analytics-event="{&quot;action&quot;:&quot;devops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devops_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">DevOps</span></a></li><li><a href="https://github.com/solutions/use-case/ci-cd" data-analytics-event="{&quot;action&quot;:&quot;ci/cd&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ci/cd_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">CI/CD</span></a></li><li><a href="https://github.com/solutions/use-case" data-analytics-event="{&quot;action&quot;:&quot;view_all_use_cases&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_use_cases_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">View all use cases</span><svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavLink-module__arrowIcon--g6Lip" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">BY INDUSTRY</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/solutions/industry/healthcare" data-analytics-event="{&quot;action&quot;:&quot;healthcare&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;healthcare_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Healthcare</span></a></li><li><a href="https://github.com/solutions/industry/financial-services" data-analytics-event="{&quot;action&quot;:&quot;financial_services&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;financial_services_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Financial services</span></a></li><li><a href="https://github.com/solutions/industry/manufacturing" data-analytics-event="{&quot;action&quot;:&quot;manufacturing&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;manufacturing_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Manufacturing</span></a></li><li><a href="https://github.com/solutions/industry/government" data-analytics-event="{&quot;action&quot;:&quot;government&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;government_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Government</span></a></li><li><a href="https://github.com/solutions/industry" data-analytics-event="{&quot;action&quot;:&quot;view_all_industries&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_industries_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">View all industries</span><svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavLink-module__arrowIcon--g6Lip" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></a></li></ul></div></li></ul><div class="NavDropdown-module__trailingLinkContainer--MNB5T"><a href="https://github.com/solutions" data-analytics-event="{&quot;action&quot;:&quot;view_all_solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_solutions_link_solutions_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">View all solutions</span><svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavLink-module__arrowIcon--g6Lip" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></a></div></div></div></li><li><div class="NavDropdown-module__container--bmXM2 js-details-container js-header-menu-item"><button type="button" class="NavDropdown-module__button--Hq9UR js-details-target" aria-expanded="false">Resources<svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavDropdown-module__buttonIcon--SR0Ke" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></button><div class="NavDropdown-module__dropdown--Ig57Y"><ul class="NavDropdown-module__list--RwSSK"><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">EXPLORE BY TOPIC</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/resources/articles?topic=ai" data-analytics-event="{&quot;action&quot;:&quot;ai&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ai_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">AI</span></a></li><li><a href="https://github.com/resources/articles?topic=software-development" data-analytics-event="{&quot;action&quot;:&quot;software_development&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;software_development_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Software Development</span></a></li><li><a href="https://github.com/resources/articles?topic=devops" data-analytics-event="{&quot;action&quot;:&quot;devops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devops_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">DevOps</span></a></li><li><a href="https://github.com/resources/articles?topic=security" data-analytics-event="{&quot;action&quot;:&quot;security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;security_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Security</span></a></li><li><a href="https://github.com/resources/articles" data-analytics-event="{&quot;action&quot;:&quot;view_all_topics&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_topics_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">View all topics</span><svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavLink-module__arrowIcon--g6Lip" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">EXPLORE BY TYPE</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/customer-stories" data-analytics-event="{&quot;action&quot;:&quot;customer_stories&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Customer stories</span></a></li><li><a href="https://github.com/resources/events" data-analytics-event="{&quot;action&quot;:&quot;events__webinars&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;events__webinars_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Events &amp; webinars</span></a></li><li><a href="https://github.com/resources/whitepapers" data-analytics-event="{&quot;action&quot;:&quot;ebooks__reports&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ebooks__reports_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Ebooks &amp; reports</span></a></li><li><a href="https://github.com/solutions/executive-insights" data-analytics-event="{&quot;action&quot;:&quot;business_insights&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;business_insights_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Business insights</span></a></li><li><a href="https://skills.github.com" data-analytics-event="{&quot;action&quot;:&quot;github_skills&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_skills_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">GitHub Skills</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">SUPPORT &amp; SERVICES</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://docs.github.com" data-analytics-event="{&quot;action&quot;:&quot;documentation&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;documentation_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Documentation</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://support.github.com" data-analytics-event="{&quot;action&quot;:&quot;customer_support&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;customer_support_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Customer support</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://github.com/orgs/community/discussions" data-analytics-event="{&quot;action&quot;:&quot;community_forum&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;community_forum_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Community forum</span></a></li><li><a href="https://github.com/trust-center" data-analytics-event="{&quot;action&quot;:&quot;trust_center&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;trust_center_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Trust center</span></a></li><li><a href="https://github.com/partners" data-analytics-event="{&quot;action&quot;:&quot;partners&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Partners</span></a></li></ul></div></li></ul></div></div></li><li><div class="NavDropdown-module__container--bmXM2 js-details-container js-header-menu-item"><button type="button" class="NavDropdown-module__button--Hq9UR js-details-target" aria-expanded="false">Open Source<svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavDropdown-module__buttonIcon--SR0Ke" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></button><div class="NavDropdown-module__dropdown--Ig57Y"><ul class="NavDropdown-module__list--RwSSK"><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">COMMUNITY</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/sponsors" data-analytics-event="{&quot;action&quot;:&quot;github_sponsors&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-sponsor-tiers NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M16.004 1.25C18.311 1.25 20 3.128 20 5.75c0 2.292-1.23 4.464-3.295 6.485-.481.47-.98.909-1.482 1.31l.265 1.32 1.375 7.5a.75.75 0 0 1-.982.844l-3.512-1.207a.75.75 0 0 0-.488 0L8.37 23.209a.75.75 0 0 1-.982-.844l1.378-7.512.261-1.309c-.5-.4-1-.838-1.481-1.31C5.479 10.215 4.25 8.043 4.25 5.75c0-2.622 1.689-4.5 3.996-4.5 1.55 0 2.947.752 3.832 1.967l.047.067.047-.067a4.726 4.726 0 0 1 3.612-1.962l.22-.005ZM13.89 14.531c-.418.285-.828.542-1.218.77l-.18.103a.75.75 0 0 1-.734 0l-.071-.04-.46-.272c-.282-.173-.573-.36-.868-.562l-.121.605-1.145 6.239 2.3-.79a2.248 2.248 0 0 1 1.284-.054l.18.053 2.299.79-1.141-6.226-.125-.616ZM16.004 2.75c-1.464 0-2.731.983-3.159 2.459-.209.721-1.231.721-1.44 0-.428-1.476-1.695-2.459-3.16-2.459-1.44 0-2.495 1.173-2.495 3 0 1.811 1.039 3.647 2.844 5.412a19.624 19.624 0 0 0 3.734 2.84l-.019-.011-.184-.111.147-.088a19.81 19.81 0 0 0 3.015-2.278l.37-.352C17.46 9.397 18.5 7.561 18.5 5.75c0-1.827-1.055-3-2.496-3Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Sponsors</span><span class="NavLink-module__subtitle--qC15H">Fund open source developers</span></div></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">PROGRAMS</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://securitylab.github.com" data-analytics-event="{&quot;action&quot;:&quot;security_lab&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;security_lab_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Security Lab</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://maintainers.github.com" data-analytics-event="{&quot;action&quot;:&quot;maintainer_community&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;maintainer_community_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Maintainer Community</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li><li><a href="https://github.com/accelerator" data-analytics-event="{&quot;action&quot;:&quot;accelerator&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;accelerator_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Accelerator</span></a></li><li><a href="https://archiveprogram.github.com" data-analytics-event="{&quot;action&quot;:&quot;archive_program&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;archive_program_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB" target="_blank" rel="noreferrer"><span class="NavLink-module__title--xw3ok">Archive Program</span><svg aria-hidden="true" focusable="false" class="octicon octicon-link-external NavLink-module__externalIcon--JurQ9" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M3.75 2h3.5a.75.75 0 0 1 0 1.5h-3.5a.25.25 0 0 0-.25.25v8.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-3.5a.75.75 0 0 1 1.5 0v3.5A1.75 1.75 0 0 1 12.25 14h-8.5A1.75 1.75 0 0 1 2 12.25v-8.5C2 2.784 2.784 2 3.75 2Zm6.854-1h4.146a.25.25 0 0 1 .25.25v4.146a.25.25 0 0 1-.427.177L13.03 4.03 9.28 7.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.75-3.75-1.543-1.543A.25.25 0 0 1 10.604 1Z"></path></svg></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">REPOSITORIES</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/topics" data-analytics-event="{&quot;action&quot;:&quot;topics&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;topics_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Topics</span></a></li><li><a href="https://github.com/trending" data-analytics-event="{&quot;action&quot;:&quot;trending&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;trending_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Trending</span></a></li><li><a href="https://github.com/collections" data-analytics-event="{&quot;action&quot;:&quot;collections&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;collections_link_open_source_navbar&quot;}" class="NavLink-module__link--n48VB"><span class="NavLink-module__title--xw3ok">Collections</span></a></li></ul></div></li></ul></div></div></li><li><div class="NavDropdown-module__container--bmXM2 js-details-container js-header-menu-item"><button type="button" class="NavDropdown-module__button--Hq9UR js-details-target" aria-expanded="false">Enterprise<svg aria-hidden="true" focusable="false" class="octicon octicon-chevron-right NavDropdown-module__buttonIcon--SR0Ke" viewBox="0 0 16 16" width="16" height="16" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M6.22 3.22a.75.75 0 0 1 1.06 0l4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L9.94 8 6.22 4.28a.75.75 0 0 1 0-1.06Z"></path></svg></button><div class="NavDropdown-module__dropdown--Ig57Y"><ul class="NavDropdown-module__list--RwSSK"><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">ENTERPRISE SOLUTIONS</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/enterprise" data-analytics-event="{&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-stack NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M11.063 1.456a1.749 1.749 0 0 1 1.874 0l8.383 5.316a1.751 1.751 0 0 1 0 2.956l-8.383 5.316a1.749 1.749 0 0 1-1.874 0L2.68 9.728a1.751 1.751 0 0 1 0-2.956Zm1.071 1.267a.25.25 0 0 0-.268 0L3.483 8.039a.25.25 0 0 0 0 .422l8.383 5.316a.25.25 0 0 0 .268 0l8.383-5.316a.25.25 0 0 0 0-.422Z"></path><path d="M1.867 12.324a.75.75 0 0 1 1.035-.232l8.964 5.685a.25.25 0 0 0 .268 0l8.964-5.685a.75.75 0 0 1 .804 1.267l-8.965 5.685a1.749 1.749 0 0 1-1.874 0l-8.965-5.685a.75.75 0 0 1-.231-1.035Z"></path><path d="M1.867 16.324a.75.75 0 0 1 1.035-.232l8.964 5.685a.25.25 0 0 0 .268 0l8.964-5.685a.75.75 0 0 1 .804 1.267l-8.965 5.685a1.749 1.749 0 0 1-1.874 0l-8.965-5.685a.75.75 0 0 1-.231-1.035Z"></path></svg><span class="NavLink-module__title--xw3ok">Enterprise platform</span><span class="NavLink-module__subtitle--qC15H">AI-powered developer platform</span></div></a></li></ul></div></li><li><div class="NavGroup-module__group--T925n"><span class="NavGroup-module__title--TdKyz">AVAILABLE ADD-ONS</span><ul class="NavGroup-module__list--M8eGv"><li><a href="https://github.com/security/advanced-security" data-analytics-event="{&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_advanced_security_link_enterprise_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-shield-check NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M16.53 9.78a.75.75 0 0 0-1.06-1.06L11 13.19l-1.97-1.97a.75.75 0 0 0-1.06 1.06l2.5 2.5a.75.75 0 0 0 1.06 0l5-5Z"></path><path d="m12.54.637 8.25 2.675A1.75 1.75 0 0 1 22 4.976V10c0 6.19-3.771 10.704-9.401 12.83a1.704 1.704 0 0 1-1.198 0C5.77 20.705 2 16.19 2 10V4.976c0-.758.489-1.43 1.21-1.664L11.46.637a1.748 1.748 0 0 1 1.08 0Zm-.617 1.426-8.25 2.676a.249.249 0 0 0-.173.237V10c0 5.46 3.28 9.483 8.43 11.426a.199.199 0 0 0 .14 0C17.22 19.483 20.5 15.461 20.5 10V4.976a.25.25 0 0 0-.173-.237l-8.25-2.676a.253.253 0 0 0-.154 0Z"></path></svg><span class="NavLink-module__title--xw3ok">GitHub Advanced Security</span><span class="NavLink-module__subtitle--qC15H">Enterprise-grade security features</span></div></a></li><li><a href="https://github.com/features/copilot/copilot-business" data-analytics-event="{&quot;action&quot;:&quot;copilot_for_business&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;copilot_for_business_link_enterprise_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-copilot NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M23.922 16.992c-.861 1.495-5.859 5.023-11.922 5.023-6.063 0-11.061-3.528-11.922-5.023A.641.641 0 0 1 0 16.736v-2.869a.841.841 0 0 1 .053-.22c.372-.935 1.347-2.292 2.605-2.656.167-.429.414-1.055.644-1.517a10.195 10.195 0 0 1-.052-1.086c0-1.331.282-2.499 1.132-3.368.397-.406.89-.717 1.474-.952 1.399-1.136 3.392-2.093 6.122-2.093 2.731 0 4.767.957 6.166 2.093.584.235 1.077.546 1.474.952.85.869 1.132 2.037 1.132 3.368 0 .368-.014.733-.052 1.086.23.462.477 1.088.644 1.517 1.258.364 2.233 1.721 2.605 2.656a.832.832 0 0 1 .053.22v2.869a.641.641 0 0 1-.078.256ZM12.172 11h-.344a4.323 4.323 0 0 1-.355.508C10.703 12.455 9.555 13 7.965 13c-1.725 0-2.989-.359-3.782-1.259a2.005 2.005 0 0 1-.085-.104L4 11.741v6.585c1.435.779 4.514 2.179 8 2.179 3.486 0 6.565-1.4 8-2.179v-6.585l-.098-.104s-.033.045-.085.104c-.793.9-2.057 1.259-3.782 1.259-1.59 0-2.738-.545-3.508-1.492a4.323 4.323 0 0 1-.355-.508h-.016.016Zm.641-2.935c.136 1.057.403 1.913.878 2.497.442.544 1.134.938 2.344.938 1.573 0 2.292-.337 2.657-.751.384-.435.558-1.15.558-2.361 0-1.14-.243-1.847-.705-2.319-.477-.488-1.319-.862-2.824-1.025-1.487-.161-2.192.138-2.533.529-.269.307-.437.808-.438 1.578v.021c0 .265.021.562.063.893Zm-1.626 0c.042-.331.063-.628.063-.894v-.02c-.001-.77-.169-1.271-.438-1.578-.341-.391-1.046-.69-2.533-.529-1.505.163-2.347.537-2.824 1.025-.462.472-.705 1.179-.705 2.319 0 1.211.175 1.926.558 2.361.365.414 1.084.751 2.657.751 1.21 0 1.902-.394 2.344-.938.475-.584.742-1.44.878-2.497Z"></path><path d="M14.5 14.25a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Zm-5 0a1 1 0 0 1 1 1v2a1 1 0 0 1-2 0v-2a1 1 0 0 1 1-1Z"></path></svg><span class="NavLink-module__title--xw3ok">Copilot for Business</span><span class="NavLink-module__subtitle--qC15H">Enterprise-grade AI features</span></div></a></li><li><a href="https://github.com/premium-support" data-analytics-event="{&quot;action&quot;:&quot;premium_support&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;premium_support_link_enterprise_navbar&quot;}" class="NavLink-module__link--n48VB"><div class="NavLink-module__text--SdWkb"><svg aria-hidden="true" focusable="false" class="octicon octicon-comment-discussion NavLink-module__icon--h0sw7" viewBox="0 0 24 24" width="24" height="24" fill="currentColor" display="inline-block" overflow="visible" style="vertical-align:text-bottom"><path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 14.25 14H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 15.543V14H1.75A1.75 1.75 0 0 1 0 12.25v-9.5C0 1.784.784 1 1.75 1ZM1.5 2.75v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Z"></path><path d="M22.5 8.75a.25.25 0 0 0-.25-.25h-3.5a.75.75 0 0 1 0-1.5h3.5c.966 0 1.75.784 1.75 1.75v9.5A1.75 1.75 0 0 1 22.25 20H21v1.543a1.457 1.457 0 0 1-2.487 1.03L15.939 20H10.75A1.75 1.75 0 0 1 9 18.25v-1.465a.75.75 0 0 1 1.5 0v1.465c0 .138.112.25.25.25h5.5a.75.75 0 0 1 .53.22l2.72 2.72v-2.19a.75.75 0 0 1 .75-.75h2a.25.25 0 0 0 .25-.25v-9.5Z"></path></svg><span class="NavLink-module__title--xw3ok">Premium Support</span><span class="NavLink-module__subtitle--qC15H">Enterprise-grade 24/7 support</span></div></a></li></ul></div></li></ul></div></div></li><li><a href="https://github.com/pricing" data-analytics-event="{&quot;action&quot;:&quot;pricing&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;pricing&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;pricing_link_pricing_navbar&quot;}" class="NavLink-module__link--n48VB MarketingNavigation-module__navLink--U9Uuk"><span class="NavLink-module__title--xw3ok">Pricing</span></a></li></ul></nav><script type="application/json" id="__PRIMER_DATA_:R0:__">{"resolvedServerColorMode":"day"}</script></div>
</react-partial>



        <div class="d-flex flex-column flex-lg-row width-full flex-justify-end flex-lg-items-center text-center mt-3 mt-lg-0 text-lg-left ml-lg-3">
                


<qbsearch-input class="search-input" data-scope="repo:pytorch/pytorch" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="DRCWBE1dVO6eWqAAvr-Tju-6wgyxKGdU5KTnlJIVsWotmK1xkIOdqH4EipXEoC5_ErtED3izXnc99L2wozrZKQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="pytorch/pytorch" data-current-org="pytorch" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div
    class="search-input-container search-with-dialog position-relative d-flex flex-row flex-items-center mr-4 rounded"
    data-action="click:qbsearch-input#searchInputContainerClicked"
  >
      <button
        type="button"
        class="header-search-button placeholder  input-button form-control d-flex flex-1 flex-self-stretch flex-items-center no-wrap width-full py-0 pl-2 pr-0 text-left border-0 box-shadow-none"
        data-target="qbsearch-input.inputButton"
        aria-label="Search or jump to…"
        aria-haspopup="dialog"
        placeholder="Search or jump to..."
        data-hotkey=s,/
        autocapitalize="off"
        data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;searchbar&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;input&quot;,&quot;label&quot;:&quot;searchbar_input_global_navbar&quot;}"
        data-action="click:qbsearch-input#handleExpand"
      >
        <div class="mr-2 color-fg-muted">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search">
    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
</svg>
        </div>
        <span class="flex-1" data-target="qbsearch-input.inputButtonText">Search or jump to...</span>
          <div class="d-flex" data-target="qbsearch-input.hotkeyIndicator">
            <svg xmlns="http://www.w3.org/2000/svg" width="22" height="20" aria-hidden="true" class="mr-1"><path fill="none" stroke="#979A9C" opacity=".4" d="M3.5.5h12c1.7 0 3 1.3 3 3v13c0 1.7-1.3 3-3 3h-12c-1.7 0-3-1.3-3-3v-13c0-1.7 1.3-3 3-3z"></path><path fill="#979A9C" d="M11.8 6L8 15.1h-.9L10.8 6h1z"></path></svg>
          </div>
      </button>

    <input type="hidden" name="type" class="js-site-search-type-field">

    
<div class="Overlay--hidden " data-modal-dialog-overlay>
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true" class="Overlay Overlay--width-large Overlay--height-auto">
      <h1 id="search-suggestions-dialog-header" class="sr-only">Search code, repositories, users, issues, pull requests...</h1>
    <div class="Overlay-body Overlay-body--paddingNone">
      
          <div data-view-component="true">        <div class="search-suggestions position-fixed width-full color-shadow-large border color-fg-default color-bg-default overflow-hidden d-flex flex-column query-builder-container"
          style="border-radius: 12px;"
          data-target="qbsearch-input.queryBuilderContainer"
          hidden
        >
          <!-- '"` --><!-- </textarea></xmp> --></option></form><form id="query-builder-test-form" action="" accept-charset="UTF-8" method="get">
  <query-builder data-target="qbsearch-input.queryBuilder" id="query-builder-query-builder-test" data-filter-key=":" data-view-component="true" class="QueryBuilder search-query-builder">
    <div class="FormControl FormControl--fullWidth">
      <label id="query-builder-test-label" for="query-builder-test" class="FormControl-label sr-only">
        Search
      </label>
      <div
        class="QueryBuilder-StyledInput width-fit "
        data-target="query-builder.styledInput"
      >
          <span id="query-builder-test-leadingvisual-wrap" class="FormControl-input-leadingVisualWrap QueryBuilder-leadingVisualWrap">
            <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search FormControl-input-leadingVisual">
    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
</svg>
          </span>
        <div data-target="query-builder.styledInputContainer" class="QueryBuilder-StyledInputContainer">
          <div
            aria-hidden="true"
            class="QueryBuilder-StyledInputContent"
            data-target="query-builder.styledInputContent"
          ></div>
          <div class="QueryBuilder-InputWrapper">
            <div aria-hidden="true" class="QueryBuilder-Sizer" data-target="query-builder.sizer"></div>
            <input id="query-builder-test" name="query-builder-test" value="" autocomplete="off" type="text" role="combobox" spellcheck="false" aria-expanded="false" aria-describedby="validation-b8bf687c-57f2-4e31-aad1-acc7b36599c9" data-target="query-builder.input" data-action="
          input:query-builder#inputChange
          blur:query-builder#inputBlur
          keydown:query-builder#inputKeydown
          focus:query-builder#inputFocus
        " data-view-component="true" class="FormControl-input QueryBuilder-Input FormControl-medium" />
          </div>
        </div>
          <span class="sr-only" id="query-builder-test-clear">Clear</span>
          <button role="button" id="query-builder-test-clear-button" aria-labelledby="query-builder-test-clear query-builder-test-label" data-target="query-builder.clearButton" data-action="
                click:query-builder#clear
                focus:query-builder#clearButtonFocus
                blur:query-builder#clearButtonBlur
              " variant="small" hidden="hidden" type="button" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium mr-1 px-2 py-0 d-flex flex-items-center rounded-1 color-fg-muted">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x-circle-fill Button-visual">
    <path d="M2.343 13.657A8 8 0 1 1 13.658 2.343 8 8 0 0 1 2.343 13.657ZM6.03 4.97a.751.751 0 0 0-1.042.018.751.751 0 0 0-.018 1.042L6.94 8 4.97 9.97a.749.749 0 0 0 .326 1.275.749.749 0 0 0 .734-.215L8 9.06l1.97 1.97a.749.749 0 0 0 1.275-.326.749.749 0 0 0-.215-.734L9.06 8l1.97-1.97a.749.749 0 0 0-.326-1.275.749.749 0 0 0-.734.215L8 6.94Z"></path>
</svg>
</button>

      </div>
      <template id="search-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-search">
    <path d="M10.68 11.74a6 6 0 0 1-7.922-8.982 6 6 0 0 1 8.982 7.922l3.04 3.04a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215ZM11.5 7a4.499 4.499 0 1 0-8.997 0A4.499 4.499 0 0 0 11.5 7Z"></path>
</svg>
</template>

<template id="code-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code">
    <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path>
</svg>
</template>

<template id="file-code-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-file-code">
    <path d="M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0 1 14.25 15h-9a.75.75 0 0 1 0-1.5h9a.25.25 0 0 0 .25-.25V6h-2.75A1.75 1.75 0 0 1 10 4.25V1.5H5.75a.25.25 0 0 0-.25.25v2.5a.75.75 0 0 1-1.5 0Zm1.72 4.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734l1.47-1.47-1.47-1.47a.75.75 0 0 1 0-1.06ZM3.28 7.78 1.81 9.25l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Zm8.22-6.218V4.25c0 .138.112.25.25.25h2.688l-.011-.013-2.914-2.914-.013-.011Z"></path>
</svg>
</template>

<template id="history-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-history">
    <path d="m.427 1.927 1.215 1.215a8.002 8.002 0 1 1-1.6 5.685.75.75 0 1 1 1.493-.154 6.5 6.5 0 1 0 1.18-4.458l1.358 1.358A.25.25 0 0 1 3.896 6H.25A.25.25 0 0 1 0 5.75V2.104a.25.25 0 0 1 .427-.177ZM7.75 4a.75.75 0 0 1 .75.75v2.992l2.028.812a.75.75 0 0 1-.557 1.392l-2.5-1A.751.751 0 0 1 7 8.25v-3.5A.75.75 0 0 1 7.75 4Z"></path>
</svg>
</template>

<template id="repo-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo">
    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
</svg>
</template>

<template id="bookmark-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-bookmark">
    <path d="M3 2.75C3 1.784 3.784 1 4.75 1h6.5c.966 0 1.75.784 1.75 1.75v11.5a.75.75 0 0 1-1.227.579L8 11.722l-3.773 3.107A.751.751 0 0 1 3 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.91l3.023-2.489a.75.75 0 0 1 .954 0l3.023 2.49V2.75a.25.25 0 0 0-.25-.25Z"></path>
</svg>
</template>

<template id="plus-circle-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-plus-circle">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm7.25-3.25v2.5h2.5a.75.75 0 0 1 0 1.5h-2.5v2.5a.75.75 0 0 1-1.5 0v-2.5h-2.5a.75.75 0 0 1 0-1.5h2.5v-2.5a.75.75 0 0 1 1.5 0Z"></path>
</svg>
</template>

<template id="circle-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-dot-fill">
    <path d="M8 4a4 4 0 1 1 0 8 4 4 0 0 1 0-8Z"></path>
</svg>
</template>

<template id="trash-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-trash">
    <path d="M11 1.75V3h2.25a.75.75 0 0 1 0 1.5H2.75a.75.75 0 0 1 0-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75ZM4.496 6.675l.66 6.6a.25.25 0 0 0 .249.225h5.19a.25.25 0 0 0 .249-.225l.66-6.6a.75.75 0 0 1 1.492.149l-.66 6.6A1.748 1.748 0 0 1 10.595 15h-5.19a1.75 1.75 0 0 1-1.741-1.575l-.66-6.6a.75.75 0 1 1 1.492-.15ZM6.5 1.75V3h3V1.75a.25.25 0 0 0-.25-.25h-2.5a.25.25 0 0 0-.25.25Z"></path>
</svg>
</template>

<template id="team-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-people">
    <path d="M2 5.5a3.5 3.5 0 1 1 5.898 2.549 5.508 5.508 0 0 1 3.034 4.084.75.75 0 1 1-1.482.235 4 4 0 0 0-7.9 0 .75.75 0 0 1-1.482-.236A5.507 5.507 0 0 1 3.102 8.05 3.493 3.493 0 0 1 2 5.5ZM11 4a3.001 3.001 0 0 1 2.22 5.018 5.01 5.01 0 0 1 2.56 3.012.749.749 0 0 1-.885.954.752.752 0 0 1-.549-.514 3.507 3.507 0 0 0-2.522-2.372.75.75 0 0 1-.574-.73v-.352a.75.75 0 0 1 .416-.672A1.5 1.5 0 0 0 11 5.5.75.75 0 0 1 11 4Zm-5.5-.5a2 2 0 1 0-.001 3.999A2 2 0 0 0 5.5 3.5Z"></path>
</svg>
</template>

<template id="project-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-project">
    <path d="M1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0ZM1.5 1.75v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25ZM11.75 3a.75.75 0 0 1 .75.75v7.5a.75.75 0 0 1-1.5 0v-7.5a.75.75 0 0 1 .75-.75Zm-8.25.75a.75.75 0 0 1 1.5 0v5.5a.75.75 0 0 1-1.5 0ZM8 3a.75.75 0 0 1 .75.75v3.5a.75.75 0 0 1-1.5 0v-3.5A.75.75 0 0 1 8 3Z"></path>
</svg>
</template>

<template id="pencil-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-pencil">
    <path d="M11.013 1.427a1.75 1.75 0 0 1 2.474 0l1.086 1.086a1.75 1.75 0 0 1 0 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 0 1-.927-.928l.929-3.25c.081-.286.235-.547.445-.758l8.61-8.61Zm.176 4.823L9.75 4.81l-6.286 6.287a.253.253 0 0 0-.064.108l-.558 1.953 1.953-.558a.253.253 0 0 0 .108-.064Zm1.238-3.763a.25.25 0 0 0-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 0 0 0-.354Z"></path>
</svg>
</template>

<template id="copilot-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copilot">
    <path d="M7.998 15.035c-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.201-.508-.254-1.084-.254-1.656 0-.87.128-1.769.693-2.484.579-.733 1.494-1.124 2.724-1.261 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095v1.872c0 .766-3.351 3.795-8.002 3.795Zm0-1.485c2.28 0 4.584-1.11 5.002-1.433V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-1.146 0-2.059-.327-2.71-.991A3.222 3.222 0 0 1 8 6.303a3.24 3.24 0 0 1-.544.743c-.65.664-1.563.991-2.71.991-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433ZM6.762 2.83c-.193-.206-.637-.413-1.682-.297-1.019.113-1.479.404-1.713.7-.247.312-.369.789-.369 1.554 0 .793.129 1.171.308 1.371.162.181.519.379 1.442.379.853 0 1.339-.235 1.638-.54.315-.322.527-.827.617-1.553.117-.935-.037-1.395-.241-1.614Zm4.155-.297c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Z"></path><path d="M6.25 9.037a.75.75 0 0 1 .75.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 .75-.75Zm4.25.75v1.501a.75.75 0 0 1-1.5 0V9.787a.75.75 0 0 1 1.5 0Z"></path>
</svg>
</template>

<template id="copilot-error-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copilot-error">
    <path d="M16 11.24c0 .112-.072.274-.21.467L13 9.688V7.862l-.023-.116c-.49.21-1.075.291-1.727.291-.198 0-.388-.009-.571-.029L6.833 5.226a4.01 4.01 0 0 0 .17-.782c.117-.935-.037-1.395-.241-1.614-.193-.206-.637-.413-1.682-.297-.683.076-1.115.231-1.395.415l-1.257-.91c.579-.564 1.413-.877 2.485-.996 1.206-.134 2.262.034 2.944.765.05.053.096.108.139.165.044-.057.094-.112.143-.165.682-.731 1.738-.899 2.944-.765 1.23.137 2.145.528 2.724 1.261.566.715.693 1.614.693 2.484 0 .572-.053 1.148-.254 1.656.066.228.098.429.126.612.012.076.024.148.037.218.924.385 1.522 1.471 1.591 2.095Zm-5.083-8.707c-1.044-.116-1.488.091-1.681.297-.204.219-.359.679-.242 1.614.091.726.303 1.231.618 1.553.299.305.784.54 1.638.54.922 0 1.28-.198 1.442-.379.179-.2.308-.578.308-1.371 0-.765-.123-1.242-.37-1.554-.233-.296-.693-.587-1.713-.7Zm2.511 11.074c-1.393.776-3.272 1.428-5.43 1.428-4.562 0-7.873-2.914-7.998-3.749V9.338c.085-.628.677-1.686 1.588-2.065.013-.07.024-.143.036-.218.029-.183.06-.384.126-.612-.18-.455-.241-.963-.252-1.475L.31 4.107A.747.747 0 0 1 0 3.509V3.49a.748.748 0 0 1 .625-.73c.156-.026.306.047.435.139l14.667 10.578a.592.592 0 0 1 .227.264.752.752 0 0 1 .046.249v.022a.75.75 0 0 1-1.19.596Zm-1.367-.991L5.635 7.964a5.128 5.128 0 0 1-.889.073c-.652 0-1.236-.081-1.727-.291l-.023.116v4.255c.419.323 2.722 1.433 5.002 1.433 1.539 0 3.089-.505 4.063-.934Z"></path>
</svg>
</template>

<template id="workflow-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-workflow">
    <path d="M0 1.75C0 .784.784 0 1.75 0h3.5C6.216 0 7 .784 7 1.75v3.5A1.75 1.75 0 0 1 5.25 7H4v4a1 1 0 0 0 1 1h4v-1.25C9 9.784 9.784 9 10.75 9h3.5c.966 0 1.75.784 1.75 1.75v3.5A1.75 1.75 0 0 1 14.25 16h-3.5A1.75 1.75 0 0 1 9 14.25v-.75H5A2.5 2.5 0 0 1 2.5 11V7h-.75A1.75 1.75 0 0 1 0 5.25Zm1.75-.25a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Zm9 9a.25.25 0 0 0-.25.25v3.5c0 .138.112.25.25.25h3.5a.25.25 0 0 0 .25-.25v-3.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
</template>

<template id="book-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-book">
    <path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z"></path>
</svg>
</template>

<template id="code-review-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code-review">
    <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 13H8.061l-2.574 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25v-8.5C0 1.784.784 1 1.75 1ZM1.5 2.75v8.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-8.5a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm5.28 1.72a.75.75 0 0 1 0 1.06L5.31 7l1.47 1.47a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-2-2a.75.75 0 0 1 0-1.06l2-2a.75.75 0 0 1 1.06 0Zm2.44 0a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L10.69 7 9.22 5.53a.75.75 0 0 1 0-1.06Z"></path>
</svg>
</template>

<template id="codespaces-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-codespaces">
    <path d="M0 11.25c0-.966.784-1.75 1.75-1.75h12.5c.966 0 1.75.784 1.75 1.75v3A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm2-9.5C2 .784 2.784 0 3.75 0h8.5C13.216 0 14 .784 14 1.75v5a1.75 1.75 0 0 1-1.75 1.75h-8.5A1.75 1.75 0 0 1 2 6.75Zm1.75-.25a.25.25 0 0 0-.25.25v5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25v-5a.25.25 0 0 0-.25-.25Zm-2 9.5a.25.25 0 0 0-.25.25v3c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-3a.25.25 0 0 0-.25-.25Z"></path><path d="M7 12.75a.75.75 0 0 1 .75-.75h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1-.75-.75Zm-4 0a.75.75 0 0 1 .75-.75h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1-.75-.75Z"></path>
</svg>
</template>

<template id="comment-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-comment">
    <path d="M1 2.75C1 1.784 1.784 1 2.75 1h10.5c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 13.25 12H9.06l-2.573 2.573A1.458 1.458 0 0 1 4 13.543V12H2.75A1.75 1.75 0 0 1 1 10.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h4.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
</template>

<template id="comment-discussion-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-comment-discussion">
    <path d="M1.75 1h8.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 10.25 10H7.061l-2.574 2.573A1.458 1.458 0 0 1 2 11.543V10h-.25A1.75 1.75 0 0 1 0 8.25v-5.5C0 1.784.784 1 1.75 1ZM1.5 2.75v5.5c0 .138.112.25.25.25h1a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h3.5a.25.25 0 0 0 .25-.25v-5.5a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25Zm13 2a.25.25 0 0 0-.25-.25h-.5a.75.75 0 0 1 0-1.5h.5c.966 0 1.75.784 1.75 1.75v5.5A1.75 1.75 0 0 1 14.25 12H14v1.543a1.458 1.458 0 0 1-2.487 1.03L9.22 12.28a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l2.22 2.22v-2.19a.75.75 0 0 1 .75-.75h1a.25.25 0 0 0 .25-.25Z"></path>
</svg>
</template>

<template id="organization-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-organization">
    <path d="M1.75 16A1.75 1.75 0 0 1 0 14.25V1.75C0 .784.784 0 1.75 0h8.5C11.216 0 12 .784 12 1.75v12.5c0 .085-.006.168-.018.25h2.268a.25.25 0 0 0 .25-.25V8.285a.25.25 0 0 0-.111-.208l-1.055-.703a.749.749 0 1 1 .832-1.248l1.055.703c.487.325.779.871.779 1.456v5.965A1.75 1.75 0 0 1 14.25 16h-3.5a.766.766 0 0 1-.197-.026c-.099.017-.2.026-.303.026h-3a.75.75 0 0 1-.75-.75V14h-1v1.25a.75.75 0 0 1-.75.75Zm-.25-1.75c0 .138.112.25.25.25H4v-1.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 .75.75v1.25h2.25a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM3.75 6h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 3.75A.75.75 0 0 1 3.75 3h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 3.75Zm4 3A.75.75 0 0 1 7.75 6h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 7 6.75ZM7.75 3h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5ZM3 9.75A.75.75 0 0 1 3.75 9h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 9.75ZM7.75 9h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z"></path>
</svg>
</template>

<template id="rocket-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-rocket">
    <path d="M14.064 0h.186C15.216 0 16 .784 16 1.75v.186a8.752 8.752 0 0 1-2.564 6.186l-.458.459c-.314.314-.641.616-.979.904v3.207c0 .608-.315 1.172-.833 1.49l-2.774 1.707a.749.749 0 0 1-1.11-.418l-.954-3.102a1.214 1.214 0 0 1-.145-.125L3.754 9.816a1.218 1.218 0 0 1-.124-.145L.528 8.717a.749.749 0 0 1-.418-1.11l1.71-2.774A1.748 1.748 0 0 1 3.31 4h3.204c.288-.338.59-.665.904-.979l.459-.458A8.749 8.749 0 0 1 14.064 0ZM8.938 3.623h-.002l-.458.458c-.76.76-1.437 1.598-2.02 2.5l-1.5 2.317 2.143 2.143 2.317-1.5c.902-.583 1.74-1.26 2.499-2.02l.459-.458a7.25 7.25 0 0 0 2.123-5.127V1.75a.25.25 0 0 0-.25-.25h-.186a7.249 7.249 0 0 0-5.125 2.123ZM3.56 14.56c-.732.732-2.334 1.045-3.005 1.148a.234.234 0 0 1-.201-.064.234.234 0 0 1-.064-.201c.103-.671.416-2.273 1.15-3.003a1.502 1.502 0 1 1 2.12 2.12Zm6.94-3.935c-.088.06-.177.118-.266.175l-2.35 1.521.548 1.783 1.949-1.2a.25.25 0 0 0 .119-.213ZM3.678 8.116 5.2 5.766c.058-.09.117-.178.176-.266H3.309a.25.25 0 0 0-.213.119l-1.2 1.95ZM12 5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
</template>

<template id="shield-check-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-shield-check">
    <path d="m8.533.133 5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667l5.25-1.68a1.748 1.748 0 0 1 1.066 0Zm-.61 1.429.001.001-5.25 1.68a.251.251 0 0 0-.174.237V7c0 1.36.275 2.666 1.057 3.859.784 1.194 2.121 2.342 4.366 3.298a.196.196 0 0 0 .154 0c2.245-.957 3.582-2.103 4.366-3.297C13.225 9.666 13.5 8.358 13.5 7V3.48a.25.25 0 0 0-.174-.238l-5.25-1.68a.25.25 0 0 0-.153 0ZM11.28 6.28l-3.5 3.5a.75.75 0 0 1-1.06 0l-1.5-1.5a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215l.97.97 2.97-2.97a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
</svg>
</template>

<template id="heart-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-heart">
    <path d="m8 14.25.345.666a.75.75 0 0 1-.69 0l-.008-.004-.018-.01a7.152 7.152 0 0 1-.31-.17 22.055 22.055 0 0 1-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.066 22.066 0 0 1-3.744 2.584l-.018.01-.006.003h-.002ZM4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.58 20.58 0 0 0 8 13.393a20.58 20.58 0 0 0 3.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.749.749 0 0 1-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5Z"></path>
</svg>
</template>

<template id="server-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-server">
    <path d="M1.75 1h12.5c.966 0 1.75.784 1.75 1.75v4c0 .372-.116.717-.314 1 .198.283.314.628.314 1v4a1.75 1.75 0 0 1-1.75 1.75H1.75A1.75 1.75 0 0 1 0 12.75v-4c0-.358.109-.707.314-1a1.739 1.739 0 0 1-.314-1v-4C0 1.784.784 1 1.75 1ZM1.5 2.75v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25Zm.25 5.75a.25.25 0 0 0-.25.25v4c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-4a.25.25 0 0 0-.25-.25ZM7 4.75A.75.75 0 0 1 7.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5A.75.75 0 0 1 7 4.75ZM7.75 10h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM3 4.75A.75.75 0 0 1 3.75 4h.5a.75.75 0 0 1 0 1.5h-.5A.75.75 0 0 1 3 4.75ZM3.75 10h.5a.75.75 0 0 1 0 1.5h-.5a.75.75 0 0 1 0-1.5Z"></path>
</svg>
</template>

<template id="globe-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-globe">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM5.78 8.75a9.64 9.64 0 0 0 1.363 4.177c.255.426.542.832.857 1.215.245-.296.551-.705.857-1.215A9.64 9.64 0 0 0 10.22 8.75Zm4.44-1.5a9.64 9.64 0 0 0-1.363-4.177c-.307-.51-.612-.919-.857-1.215a9.927 9.927 0 0 0-.857 1.215A9.64 9.64 0 0 0 5.78 7.25Zm-5.944 1.5H1.543a6.507 6.507 0 0 0 4.666 5.5c-.123-.181-.24-.365-.352-.552-.715-1.192-1.437-2.874-1.581-4.948Zm-2.733-1.5h2.733c.144-2.074.866-3.756 1.58-4.948.12-.197.237-.381.353-.552a6.507 6.507 0 0 0-4.666 5.5Zm10.181 1.5c-.144 2.074-.866 3.756-1.58 4.948-.12.197-.237.381-.353.552a6.507 6.507 0 0 0 4.666-5.5Zm2.733-1.5a6.507 6.507 0 0 0-4.666-5.5c.123.181.24.365.353.552.714 1.192 1.436 2.874 1.58 4.948Z"></path>
</svg>
</template>

<template id="issue-opened-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-issue-opened">
    <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path>
</svg>
</template>

<template id="device-mobile-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-device-mobile">
    <path d="M3.75 0h8.5C13.216 0 14 .784 14 1.75v12.5A1.75 1.75 0 0 1 12.25 16h-8.5A1.75 1.75 0 0 1 2 14.25V1.75C2 .784 2.784 0 3.75 0ZM3.5 1.75v12.5c0 .138.112.25.25.25h8.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25ZM8 13a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path>
</svg>
</template>

<template id="package-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-package">
    <path d="m8.878.392 5.25 3.045c.54.314.872.89.872 1.514v6.098a1.75 1.75 0 0 1-.872 1.514l-5.25 3.045a1.75 1.75 0 0 1-1.756 0l-5.25-3.045A1.75 1.75 0 0 1 1 11.049V4.951c0-.624.332-1.201.872-1.514L7.122.392a1.75 1.75 0 0 1 1.756 0ZM7.875 1.69l-4.63 2.685L8 7.133l4.755-2.758-4.63-2.685a.248.248 0 0 0-.25 0ZM2.5 5.677v5.372c0 .09.047.171.125.216l4.625 2.683V8.432Zm6.25 8.271 4.625-2.683a.25.25 0 0 0 .125-.216V5.677L8.75 8.432Z"></path>
</svg>
</template>

<template id="credit-card-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-credit-card">
    <path d="M10.75 9a.75.75 0 0 0 0 1.5h1.5a.75.75 0 0 0 0-1.5h-1.5Z"></path><path d="M0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0 1 14.25 14H1.75A1.75 1.75 0 0 1 0 12.25ZM14.5 6.5h-13v5.75c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25Zm0-2.75a.25.25 0 0 0-.25-.25H1.75a.25.25 0 0 0-.25.25V5h13Z"></path>
</svg>
</template>

<template id="play-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-play">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z"></path>
</svg>
</template>

<template id="gift-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-gift">
    <path d="M2 2.75A2.75 2.75 0 0 1 4.75 0c.983 0 1.873.42 2.57 1.232.268.318.497.668.68 1.042.183-.375.411-.725.68-1.044C9.376.42 10.266 0 11.25 0a2.75 2.75 0 0 1 2.45 4h.55c.966 0 1.75.784 1.75 1.75v2c0 .698-.409 1.301-1 1.582v4.918A1.75 1.75 0 0 1 13.25 16H2.75A1.75 1.75 0 0 1 1 14.25V9.332C.409 9.05 0 8.448 0 7.75v-2C0 4.784.784 4 1.75 4h.55c-.192-.375-.3-.8-.3-1.25ZM7.25 9.5H2.5v4.75c0 .138.112.25.25.25h4.5Zm1.5 0v5h4.5a.25.25 0 0 0 .25-.25V9.5Zm0-4V8h5.5a.25.25 0 0 0 .25-.25v-2a.25.25 0 0 0-.25-.25Zm-7 0a.25.25 0 0 0-.25.25v2c0 .138.112.25.25.25h5.5V5.5h-5.5Zm3-4a1.25 1.25 0 0 0 0 2.5h2.309c-.233-.818-.542-1.401-.878-1.793-.43-.502-.915-.707-1.431-.707ZM8.941 4h2.309a1.25 1.25 0 0 0 0-2.5c-.516 0-1 .205-1.43.707-.337.392-.646.975-.879 1.793Z"></path>
</svg>
</template>

<template id="code-square-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code-square">
    <path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25Zm1.75-.25a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25V1.75a.25.25 0 0 0-.25-.25Zm7.47 3.97a.75.75 0 0 1 1.06 0l2 2a.75.75 0 0 1 0 1.06l-2 2a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L10.69 8 9.22 6.53a.75.75 0 0 1 0-1.06ZM6.78 6.53 5.31 8l1.47 1.47a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-2-2a.75.75 0 0 1 0-1.06l2-2a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
</svg>
</template>

<template id="device-desktop-icon">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-device-desktop">
    <path d="M14.25 1c.966 0 1.75.784 1.75 1.75v7.5A1.75 1.75 0 0 1 14.25 12h-3.727c.099 1.041.52 1.872 1.292 2.757A.752.752 0 0 1 11.25 16h-6.5a.75.75 0 0 1-.565-1.243c.772-.885 1.192-1.716 1.292-2.757H1.75A1.75 1.75 0 0 1 0 10.25v-7.5C0 1.784.784 1 1.75 1ZM1.75 2.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h12.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25ZM9.018 12H6.982a5.72 5.72 0 0 1-.765 2.5h3.566a5.72 5.72 0 0 1-.765-2.5Z"></path>
</svg>
</template>

        <div class="position-relative">
                <ul
                  role="listbox"
                  class="ActionListWrap QueryBuilder-ListWrap"
                  aria-label="Suggestions"
                  data-action="
                    combobox-commit:query-builder#comboboxCommit
                    mousedown:query-builder#resultsMousedown
                  "
                  data-target="query-builder.resultsList"
                  data-persist-list=false
                  id="query-builder-test-results"
                  tabindex="-1"
                ></ul>
        </div>
      <div class="FormControl-inlineValidation" id="validation-b8bf687c-57f2-4e31-aad1-acc7b36599c9" hidden="hidden">
        <span class="FormControl-inlineValidation--visual">
          <svg aria-hidden="true" height="12" viewBox="0 0 12 12" version="1.1" width="12" data-view-component="true" class="octicon octicon-alert-fill">
    <path d="M4.855.708c.5-.896 1.79-.896 2.29 0l4.675 8.351a1.312 1.312 0 0 1-1.146 1.954H1.33A1.313 1.313 0 0 1 .183 9.058ZM7 7V3H5v4Zm-1 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2Z"></path>
</svg>
        </span>
        <span></span>
</div>    </div>
    <div data-target="query-builder.screenReaderFeedback" aria-live="polite" aria-atomic="true" class="sr-only"></div>
</query-builder></form>
          <div class="d-flex flex-row color-fg-muted px-3 text-small color-bg-default search-feedback-prompt">
            <a target="_blank" href="https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax" data-view-component="true" class="Link color-fg-accent text-normal ml-2">Search syntax tips</a>            <div class="d-flex flex-1"></div>
          </div>
        </div>
</div>

    </div>
</modal-dialog></div>
  </div>
  <div data-action="click:qbsearch-input#retract" class="dark-backdrop position-fixed" hidden data-target="qbsearch-input.darkBackdrop"></div>
  <div class="color-fg-default">
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true" class="Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade Overlay--disableScroll">
    <div data-view-component="true" class="Overlay-header">
  <div class="Overlay-headerContentWrap">
    <div class="Overlay-titleWrap">
      <h1 class="Overlay-title " id="feedback-dialog-title">
        Provide feedback
      </h1>
        
    </div>
    <div class="Overlay-actionWrap">
      <button data-close-dialog-id="feedback-dialog" aria-label="Close" aria-label="Close" type="button" data-view-component="true" class="close-button Overlay-closeButton"><svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg></button>
    </div>
  </div>
  
</div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        <div data-view-component="true" class="Overlay-body">        <!-- '"` --><!-- </textarea></xmp> --></option></form><form id="code-search-feedback-form" data-turbo="false" action="/search/feedback" accept-charset="UTF-8" method="post"><input type="hidden" data-csrf="true" name="authenticity_token" value="4DU8n2i1wx/2Yx7VsG1JclTd5q0ULs6+S1hHonqfPvOWGUBgRhQyInTaH9vqq95UGpJDPn+rbzD+pPLv7o8T/Q==" />
          <p>We read every piece of feedback, and take your input very seriously.</p>
          <textarea name="feedback" class="form-control width-full mb-2" style="height: 120px" id="feedback"></textarea>
          <input name="include_email" id="include_email" aria-label="Include my email address so I can be contacted" class="form-control mr-2" type="checkbox">
          <label for="include_email" style="font-weight: normal">Include my email address so I can be contacted</label>
</form></div>
      </scrollable-region>
      <div data-view-component="true" class="Overlay-footer Overlay-footer--alignEnd">          <button data-close-dialog-id="feedback-dialog" type="button" data-view-component="true" class="btn">    Cancel
</button>
          <button form="code-search-feedback-form" data-action="click:qbsearch-input#submitFeedback" type="submit" data-view-component="true" class="btn-primary btn">    Submit feedback
</button>
</div>
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true" class="Overlay Overlay-whenNarrow Overlay--size-medium Overlay--motion-scaleFade Overlay--disableScroll">
    <div data-view-component="true" class="Overlay-header Overlay-header--divided">
  <div class="Overlay-headerContentWrap">
    <div class="Overlay-titleWrap">
      <h1 class="Overlay-title " id="custom-scopes-dialog-title">
        Saved searches
      </h1>
        <h2 id="custom-scopes-dialog-description" class="Overlay-description">Use saved searches to filter your results more quickly</h2>
    </div>
    <div class="Overlay-actionWrap">
      <button data-close-dialog-id="custom-scopes-dialog" aria-label="Close" aria-label="Close" type="button" data-view-component="true" class="close-button Overlay-closeButton"><svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg></button>
    </div>
  </div>
  
</div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        <div data-view-component="true" class="Overlay-body">        <div data-target="custom-scopes.customScopesModalDialogFlash"></div>

        <div hidden class="create-custom-scope-form" data-target="custom-scopes.createCustomScopeForm">
        <!-- '"` --><!-- </textarea></xmp> --></option></form><form id="custom-scopes-dialog-form" data-turbo="false" action="/search/custom_scopes" accept-charset="UTF-8" method="post"><input type="hidden" data-csrf="true" name="authenticity_token" value="oU7dhdQG/Mjmq5N5ffGMKItpPN42ARiwmb9EDxMt1Jqp/7pT5u6SLTlgPAEMIf4q7Zsb7RJXERNcyX1sG0lszw==" />
          <div data-target="custom-scopes.customScopesModalDialogFlash"></div>

          <input type="hidden" id="custom_scope_id" name="custom_scope_id" data-target="custom-scopes.customScopesIdField">

          <div class="form-group">
            <label for="custom_scope_name">Name</label>
            <auto-check src="/search/custom_scopes/check_name" required>
              <input
                type="text"
                name="custom_scope_name"
                id="custom_scope_name"
                data-target="custom-scopes.customScopesNameField"
                class="form-control"
                autocomplete="off"
                placeholder="github-ruby"
                required
                maxlength="50">
              <input type="hidden" data-csrf="true" value="aXeIf4H2y2G9tmP/Xqj3vSJ8QXQ3RRRjrFpP8SOGXTfbdbKrvm8bskPj6m8Zg9zOk2HdlvZ8QFFtAfoWZ0juUw==" />
            </auto-check>
          </div>

          <div class="form-group">
            <label for="custom_scope_query">Query</label>
            <input
              type="text"
              name="custom_scope_query"
              id="custom_scope_query"
              data-target="custom-scopes.customScopesQueryField"
              class="form-control"
              autocomplete="off"
              placeholder="(repo:mona/a OR repo:mona/b) AND lang:python"
              required
              maxlength="500">
          </div>

          <p class="text-small color-fg-muted">
            To see all available qualifiers, see our <a class="Link--inTextBlock" href="https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax">documentation</a>.
          </p>
</form>        </div>

        <div data-target="custom-scopes.manageCustomScopesForm">
          <div data-target="custom-scopes.list"></div>
        </div>

</div>
      </scrollable-region>
      <div data-view-component="true" class="Overlay-footer Overlay-footer--alignEnd Overlay-footer--divided">          <button data-action="click:custom-scopes#customScopesCancel" type="button" data-view-component="true" class="btn">    Cancel
</button>
          <button form="custom-scopes-dialog-form" data-action="click:custom-scopes#customScopesSubmit" data-target="custom-scopes.customScopesSubmitButton" type="submit" data-view-component="true" class="btn-primary btn">    Create saved search
</button>
</div>
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            <div class="position-relative HeaderMenu-link-wrap d-lg-inline-block">
              <a
                href="/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fblob%2Fv2.9.1%2Faten%2Fsrc%2FATen%2Fnative%2Fnative_functions.yaml"
                class="HeaderMenu-link HeaderMenu-link--sign-in HeaderMenu-button flex-shrink-0 no-underline d-none d-lg-inline-flex border border-lg-0 rounded px-2 py-1"
                style="margin-left: 12px;"
                data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="6e483a8da0bfe44650a29bc61282708a1adbb4fa59703afbf93c6ca516fbd561"
                data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to go to homepage&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Sign in;ref_loc:Header&quot;}"
              >
                Sign in
              </a>
            </div>

              <a href="/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=pytorch%2Fpytorch"
                class="HeaderMenu-link HeaderMenu-link--sign-up HeaderMenu-button flex-shrink-0 d-flex d-lg-inline-flex no-underline border color-border-default rounded px-2 py-1"
                data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="6e483a8da0bfe44650a29bc61282708a1adbb4fa59703afbf93c6ca516fbd561"
                data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}"
              >
                Sign up
              </a>

                <div class="AppHeader-appearanceSettings">
    <react-partial-anchor>
      <button data-target="react-partial-anchor.anchor" id="icon-button-fa32ee62-a03c-4735-b26f-f6442ac00084" aria-labelledby="tooltip-6ad770c4-2ad5-43a5-95bd-d85324da4c6b" type="button" disabled="disabled" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium AppHeader-button HeaderMenu-link border cursor-wait">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-sliders Button-visual">
    <path d="M15 2.75a.75.75 0 0 1-.75.75h-4a.75.75 0 0 1 0-1.5h4a.75.75 0 0 1 .75.75Zm-8.5.75v1.25a.75.75 0 0 0 1.5 0v-4a.75.75 0 0 0-1.5 0V2H1.75a.75.75 0 0 0 0 1.5H6.5Zm1.25 5.25a.75.75 0 0 0 0-1.5h-6a.75.75 0 0 0 0 1.5h6ZM15 8a.75.75 0 0 1-.75.75H11.5V10a.75.75 0 1 1-1.5 0V6a.75.75 0 0 1 1.5 0v1.25h2.75A.75.75 0 0 1 15 8Zm-9 5.25v-2a.75.75 0 0 0-1.5 0v1.25H1.75a.75.75 0 0 0 0 1.5H4.5v1.25a.75.75 0 0 0 1.5 0v-2Zm9 0a.75.75 0 0 1-.75.75h-6a.75.75 0 0 1 0-1.5h6a.75.75 0 0 1 .75.75Z"></path>
</svg>
</button><tool-tip id="tooltip-6ad770c4-2ad5-43a5-95bd-d85324da4c6b" for="icon-button-fa32ee62-a03c-4735-b26f-f6442ac00084" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.ab16af462b02f294298e.module.css" />
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.753d458774a2f782559b.module.css" />

<react-partial
  partial-name="appearance-settings"
  data-ssr="false"
  data-attempted-ssr="false"
  data-react-profiling="false"
>
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </div>

          <button type="button" class="sr-only js-header-menu-focus-trap d-block d-lg-none">Resetting focus</button>
        </div>
      </div>
    </div>
  </div>
</header>

      <div hidden="hidden" data-view-component="true" class="js-stale-session-flash stale-session-flash flash flash-warn flash-full">
  
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
        <span class="js-stale-session-flash-signed-in" hidden>You signed in with another tab or window. <a class="Link--inTextBlock" href="">Reload</a> to refresh your session.</span>
        <span class="js-stale-session-flash-signed-out" hidden>You signed out in another tab or window. <a class="Link--inTextBlock" href="">Reload</a> to refresh your session.</span>
        <span class="js-stale-session-flash-switched" hidden>You switched accounts on another tab or window. <a class="Link--inTextBlock" href="">Reload</a> to refresh your session.</span>

    <button id="icon-button-1a3432bd-f5c1-412f-91b5-d554e78ff2eb" aria-labelledby="tooltip-9d2b438c-a6cc-4ec1-b5ec-a2d89e471847" type="button" data-view-component="true" class="Button Button--iconOnly Button--invisible Button--medium flash-close js-flash-close">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x Button-visual">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
</button><tool-tip id="tooltip-9d2b438c-a6cc-4ec1-b5ec-a2d89e471847" for="icon-button-1a3432bd-f5c1-412f-91b5-d554e78ff2eb" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Dismiss alert</tool-tip>


  
</div>
    </div>

  <div id="start-of-content" class="show-on-focus"></div>








    <div id="js-flash-container" class="flash-container" data-turbo-replace>




  <template class="js-flash-template">
    
<div class="flash flash-full   {{ className }}">
  <div >
    <button autofocus class="flash-close js-flash-close" type="button" aria-label="Dismiss this message">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
    </button>
    <div aria-atomic="true" role="alert" class="js-flash-alert">
      
      <div>{{ message }}</div>

    </div>
  </div>
</div>
  </template>
</div>


    






  <div
    class="application-main "
    data-commit-hovercards-enabled
    data-discussion-hovercards-enabled
    data-issue-and-pr-hovercards-enabled
    data-project-hovercards-enabled
  >
        <div itemscope itemtype="http://schema.org/SoftwareSourceCode" class="">
    <main id="js-repo-pjax-container" >
      
      
    

    






  

  <div id="repository-container-header"  class="pt-3 hide-full-screen" style="background-color: var(--page-header-bgColor, var(--color-page-header-bg));" data-turbo-replace>

      <div class="d-flex flex-nowrap flex-justify-end mb-3  px-3 px-lg-5" style="gap: 1rem;">

        <div class="flex-auto min-width-0 width-fit">
            
  <div class=" d-flex flex-wrap flex-items-center wb-break-word f3 text-normal">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo color-fg-muted mr-2">
    <path d="M2 2.5A2.5 2.5 0 0 1 4.5 0h8.75a.75.75 0 0 1 .75.75v12.5a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h1.75v-2h-8a1 1 0 0 0-.714 1.7.75.75 0 1 1-1.072 1.05A2.495 2.495 0 0 1 2 11.5Zm10.5-1h-8a1 1 0 0 0-1 1v6.708A2.486 2.486 0 0 1 4.5 9h8ZM5 12.25a.25.25 0 0 1 .25-.25h3.5a.25.25 0 0 1 .25.25v3.25a.25.25 0 0 1-.4.2l-1.45-1.087a.249.249 0 0 0-.3 0L5.4 15.7a.25.25 0 0 1-.4-.2Z"></path>
</svg>
    
    <span class="author flex-self-stretch" itemprop="author">
      <a class="url fn" rel="author" data-hovercard-type="organization" data-hovercard-url="/orgs/pytorch/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="/pytorch">
        pytorch
</a>    </span>
    <span class="mx-1 flex-self-stretch color-fg-muted">/</span>
    <strong itemprop="name" class="mr-2 flex-self-stretch">
      <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="/pytorch/pytorch">pytorch</a>
    </strong>

    <span></span><span class="Label Label--secondary v-align-middle mr-1">Public</span>
  </div>


        </div>

        <div id="repository-details-container" class="flex-shrink-0" data-turbo-replace style="max-width: 70%;">
            <ul class="pagehead-actions flex-shrink-0 d-none d-md-inline" style="padding: 2px 0;">
    
      

  <li>
            <a href="/login?return_to=%2Fpytorch%2Fpytorch" rel="nofollow" id="repository-details-watch-button" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="0e9d61fdb20213436b4e0d2b4afe86d537cf60d760c3aaaba908ce58894bf156" aria-label="You must be signed in to change notification settings" data-view-component="true" class="btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-bell mr-2">
    <path d="M8 16a2 2 0 0 0 1.985-1.75c.017-.137-.097-.25-.235-.25h-3.5c-.138 0-.252.113-.235.25A2 2 0 0 0 8 16ZM3 5a5 5 0 0 1 10 0v2.947c0 .05.015.098.042.139l1.703 2.555A1.519 1.519 0 0 1 13.482 13H2.518a1.516 1.516 0 0 1-1.263-2.36l1.703-2.554A.255.255 0 0 0 3 7.947Zm5-3.5A3.5 3.5 0 0 0 4.5 5v2.947c0 .346-.102.683-.294.97l-1.703 2.556a.017.017 0 0 0-.003.01l.001.006c0 .002.002.004.004.006l.006.004.007.001h10.964l.007-.001.006-.004.004-.006.001-.007a.017.017 0 0 0-.003-.01l-1.703-2.554a1.745 1.745 0 0 1-.294-.97V5A3.5 3.5 0 0 0 8 1.5Z"></path>
</svg>Notifications
</a>    <tool-tip id="tooltip-e3890f70-77b3-4edf-b769-223a4becb57c" for="repository-details-watch-button" popover="manual" data-direction="s" data-type="description" data-view-component="true" class="sr-only position-absolute">You must be signed in to change notification settings</tool-tip>

  </li>

  <li>
          <a icon="repo-forked" id="fork-button" href="/login?return_to=%2Fpytorch%2Fpytorch" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:65600975,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="ae0c8e3e200f79c223681b5de549cdb08c4704192855e8633d69bd391851902b" data-view-component="true" class="btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-repo-forked mr-2">
    <path d="M5 5.372v.878c0 .414.336.75.75.75h4.5a.75.75 0 0 0 .75-.75v-.878a2.25 2.25 0 1 1 1.5 0v.878a2.25 2.25 0 0 1-2.25 2.25h-1.5v2.128a2.251 2.251 0 1 1-1.5 0V8.5h-1.5A2.25 2.25 0 0 1 3.5 6.25v-.878a2.25 2.25 0 1 1 1.5 0ZM5 3.25a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Zm6.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm-3 8.75a.75.75 0 1 0-1.5 0 .75.75 0 0 0 1.5 0Z"></path>
</svg>Fork
    <span id="repo-network-counter" data-pjax-replace="true" data-turbo-replace="true" title="26,419" data-view-component="true" class="Counter">26.4k</span>
</a>
  </li>

  <li>
        <div data-view-component="true" class="BtnGroup d-flex">
        <a href="/login?return_to=%2Fpytorch%2Fpytorch" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:65600975,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f653cee6f033956d97cd011759c46c0c6b3ee86ca726255c954247d7e74512fe" aria-label="You must be signed in to star a repository" data-view-component="true" class="tooltipped tooltipped-sw btn-sm btn">    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-star v-align-text-bottom d-inline-block mr-2">
    <path d="M8 .25a.75.75 0 0 1 .673.418l1.882 3.815 4.21.612a.75.75 0 0 1 .416 1.279l-3.046 2.97.719 4.192a.751.751 0 0 1-1.088.791L8 12.347l-3.766 1.98a.75.75 0 0 1-1.088-.79l.72-4.194L.818 6.374a.75.75 0 0 1 .416-1.28l4.21-.611L7.327.668A.75.75 0 0 1 8 .25Zm0 2.445L6.615 5.5a.75.75 0 0 1-.564.41l-3.097.45 2.24 2.184a.75.75 0 0 1 .216.664l-.528 3.084 2.769-1.456a.75.75 0 0 1 .698 0l2.77 1.456-.53-3.084a.75.75 0 0 1 .216-.664l2.24-2.183-3.096-.45a.75.75 0 0 1-.564-.41L8 2.694Z"></path>
</svg><span data-view-component="true" class="d-inline">
          Star
</span>          <span id="repo-stars-counter-star" aria-label="96349 users starred this repository" data-singular-suffix="user starred this repository" data-plural-suffix="users starred this repository" data-turbo-replace="true" title="96,349" data-view-component="true" class="Counter js-social-count">96.3k</span>
</a></div>
  </li>

</ul>

        </div>
      </div>

        <div id="responsive-meta-container" data-turbo-replace>
</div>


          <nav data-pjax="#js-repo-pjax-container" aria-label="Repository" data-view-component="true" class="js-repo-nav js-sidenav-container-pjax js-responsive-underlinenav overflow-hidden UnderlineNav px-3 px-md-4 px-lg-5">

  <ul data-view-component="true" class="UnderlineNav-body list-style-none">
      <li data-view-component="true" class="d-inline-flex">
  <a id="code-tab" href="/pytorch/pytorch/tree/v2.9.1" data-tab-item="i0code-tab" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments repo_attestations /pytorch/pytorch/tree/v2.9.1" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g c" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Code&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" aria-current="page" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item selected">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code UnderlineNav-octicon d-none d-sm-inline">
    <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path>
</svg>
        <span data-content="Code">Code</span>
          <span id="code-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="issues-tab" href="/pytorch/pytorch/issues" data-tab-item="i1issues-tab" data-selected-links="repo_issues repo_labels repo_milestones /pytorch/pytorch/issues" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g i" data-react-nav="issues-react" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Issues&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-issue-opened UnderlineNav-octicon d-none d-sm-inline">
    <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path>
</svg>
        <span data-content="Issues">Issues</span>
          <span id="issues-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="5,000+" data-view-component="true" class="Counter">5k+</span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="pull-requests-tab" href="/pytorch/pytorch/pulls" data-tab-item="i2pull-requests-tab" data-selected-links="repo_pulls checks /pytorch/pytorch/pulls" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g p" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Pull requests&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-git-pull-request UnderlineNav-octicon d-none d-sm-inline">
    <path d="M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z"></path>
</svg>
        <span data-content="Pull requests">Pull requests</span>
          <span id="pull-requests-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="1,752" data-view-component="true" class="Counter">1.8k</span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="actions-tab" href="/pytorch/pytorch/actions" data-tab-item="i3actions-tab" data-selected-links="repo_actions /pytorch/pytorch/actions" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g a" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Actions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-play UnderlineNav-octicon d-none d-sm-inline">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z"></path>
</svg>
        <span data-content="Actions">Actions</span>
          <span id="actions-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="projects-tab" href="/pytorch/pytorch/projects" data-tab-item="i4projects-tab" data-selected-links="repo_projects new_repo_project repo_project /pytorch/pytorch/projects" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g b" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Projects&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-table UnderlineNav-octicon d-none d-sm-inline">
    <path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z"></path>
</svg>
        <span data-content="Projects">Projects</span>
          <span id="projects-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="12" data-view-component="true" class="Counter">12</span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="wiki-tab" href="/pytorch/pytorch/wiki" data-tab-item="i5wiki-tab" data-selected-links="repo_wiki /pytorch/pytorch/wiki" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g w" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Wiki&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-book UnderlineNav-octicon d-none d-sm-inline">
    <path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z"></path>
</svg>
        <span data-content="Wiki">Wiki</span>
          <span id="wiki-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="security-tab" href="/pytorch/pytorch/security" data-tab-item="i6security-tab" data-selected-links="security overview alerts policy token_scanning code_scanning /pytorch/pytorch/security" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g s" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Security&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-shield UnderlineNav-octicon d-none d-sm-inline">
    <path d="M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
        <span data-content="Security">Security</span>
          <include-fragment src="/pytorch/pytorch/security/overall-count" accept="text/fragment+html" data-nonce="v2:5d07e3ca-7011-6282-c0d7-c515aa051d42" data-view-component="true">
  
  <div data-show-on-forbidden-error hidden>
    <div class="Box">
  <div class="blankslate-container">
    <div data-view-component="true" class="blankslate blankslate-spacious color-bg-default rounded-2">
      

      <h3 data-view-component="true" class="blankslate-heading">        Uh oh!
</h3>
      <p data-view-component="true">        <p class="color-fg-muted my-2 mb-2 ws-normal">There was an error while loading. <a class="Link--inTextBlock" data-turbo="false" href="" aria-label="Please reload this page">Please reload this page</a>.</p>
</p>

</div>  </div>
</div>  </div>
</include-fragment>

    
</a></li>
      <li data-view-component="true" class="d-inline-flex">
  <a id="insights-tab" href="/pytorch/pytorch/pulse" data-tab-item="i7insights-tab" data-selected-links="repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /pytorch/pytorch/pulse" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Insights&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true" class="UnderlineNav-item no-wrap js-responsive-underlinenav-item js-selected-navigation-item">
    
              <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-graph UnderlineNav-octicon d-none d-sm-inline">
    <path d="M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
</svg>
        <span data-content="Insights">Insights</span>
          <span id="insights-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true" class="Counter"></span>


    
</a></li>
</ul>
    <div style="visibility:hidden;" data-view-component="true" class="UnderlineNav-actions js-responsive-underlinenav-overflow position-absolute pr-3 pr-md-4 pr-lg-5 right-0">      <action-menu data-select-variant="none" data-view-component="true">
  <focus-group direction="vertical" mnemonics retain>
    <button id="action-menu-04c425fa-5e15-443e-908c-81d66eb6b7c4-button" popovertarget="action-menu-04c425fa-5e15-443e-908c-81d66eb6b7c4-overlay" aria-controls="action-menu-04c425fa-5e15-443e-908c-81d66eb6b7c4-list" aria-haspopup="true" aria-labelledby="tooltip-0e787b40-48aa-4766-8a67-e6934dcf081b" type="button" data-view-component="true" class="Button Button--iconOnly Button--secondary Button--medium UnderlineNav-item">  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-kebab-horizontal Button-visual">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg>
</button><tool-tip id="tooltip-0e787b40-48aa-4766-8a67-e6934dcf081b" for="action-menu-04c425fa-5e15-443e-908c-81d66eb6b7c4-button" popover="manual" data-direction="s" data-type="label" data-view-component="true" class="sr-only position-absolute">Additional navigation options</tool-tip>


<anchored-position data-target="action-menu.overlay" id="action-menu-04c425fa-5e15-443e-908c-81d66eb6b7c4-overlay" anchor="action-menu-04c425fa-5e15-443e-908c-81d66eb6b7c4-button" align="start" side="outside-bottom" anchor-offset="normal" popover="auto" data-view-component="true">
  <div data-view-component="true" class="Overlay Overlay--size-auto">
    
      <div data-view-component="true" class="Overlay-body Overlay-body--paddingNone">          <action-list>
  <div data-view-component="true">
    <ul aria-labelledby="action-menu-04c425fa-5e15-443e-908c-81d66eb6b7c4-button" id="action-menu-04c425fa-5e15-443e-908c-81d66eb6b7c4-list" role="menu" data-view-component="true" class="ActionListWrap--inset ActionListWrap">
        <li hidden="hidden" data-menu-item="i0code-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-505deda5-92ce-4dae-a48a-63dfa69ad9f1" href="/pytorch/pytorch/tree/v2.9.1" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-code">
    <path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Code
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i1issues-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-d497eaa8-c1ac-40e3-8ee5-7ea072372cb8" href="/pytorch/pytorch/issues" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-issue-opened">
    <path d="M8 9.5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path><path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Issues
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i2pull-requests-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-309d0c1f-2ba7-400a-be31-2b48055ad148" href="/pytorch/pytorch/pulls" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-git-pull-request">
    <path d="M1.5 3.25a2.25 2.25 0 1 1 3 2.122v5.256a2.251 2.251 0 1 1-1.5 0V5.372A2.25 2.25 0 0 1 1.5 3.25Zm5.677-.177L9.573.677A.25.25 0 0 1 10 .854V2.5h1A2.5 2.5 0 0 1 13.5 5v5.628a2.251 2.251 0 1 1-1.5 0V5a1 1 0 0 0-1-1h-1v1.646a.25.25 0 0 1-.427.177L7.177 3.427a.25.25 0 0 1 0-.354ZM3.75 2.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm0 9.5a.75.75 0 1 0 0 1.5.75.75 0 0 0 0-1.5Zm8.25.75a.75.75 0 1 0 1.5 0 .75.75 0 0 0-1.5 0Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Pull requests
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i3actions-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-72ce61cb-7e54-4f03-9451-7eb72f4c0dcb" href="/pytorch/pytorch/actions" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-play">
    <path d="M8 0a8 8 0 1 1 0 16A8 8 0 0 1 8 0ZM1.5 8a6.5 6.5 0 1 0 13 0 6.5 6.5 0 0 0-13 0Zm4.879-2.773 4.264 2.559a.25.25 0 0 1 0 .428l-4.264 2.559A.25.25 0 0 1 6 10.559V5.442a.25.25 0 0 1 .379-.215Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Actions
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i4projects-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-e48e4ee0-7f20-47cc-a078-7a4707c08db5" href="/pytorch/pytorch/projects" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-table">
    <path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v12.5A1.75 1.75 0 0 1 14.25 16H1.75A1.75 1.75 0 0 1 0 14.25ZM6.5 6.5v8h7.75a.25.25 0 0 0 .25-.25V6.5Zm8-1.5V1.75a.25.25 0 0 0-.25-.25H6.5V5Zm-13 1.5v7.75c0 .138.112.25.25.25H5v-8ZM5 5V1.5H1.75a.25.25 0 0 0-.25.25V5Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Projects
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i5wiki-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-ff43cfcf-910e-4a6b-8370-c433b3d58806" href="/pytorch/pytorch/wiki" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-book">
    <path d="M0 1.75A.75.75 0 0 1 .75 1h4.253c1.227 0 2.317.59 3 1.501A3.743 3.743 0 0 1 11.006 1h4.245a.75.75 0 0 1 .75.75v10.5a.75.75 0 0 1-.75.75h-4.507a2.25 2.25 0 0 0-1.591.659l-.622.621a.75.75 0 0 1-1.06 0l-.622-.621A2.25 2.25 0 0 0 5.258 13H.75a.75.75 0 0 1-.75-.75Zm7.251 10.324.004-5.073-.002-2.253A2.25 2.25 0 0 0 5.003 2.5H1.5v9h3.757a3.75 3.75 0 0 1 1.994.574ZM8.755 4.75l-.004 7.322a3.752 3.752 0 0 1 1.992-.572H14.5v-9h-3.495a2.25 2.25 0 0 0-2.25 2.25Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Wiki
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i6security-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-f5e81d14-c39c-4703-bee8-14f2e3cbe9f0" href="/pytorch/pytorch/security" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-shield">
    <path d="M7.467.133a1.748 1.748 0 0 1 1.066 0l5.25 1.68A1.75 1.75 0 0 1 15 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.697 1.697 0 0 1-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 0 1 1.217-1.667Zm.61 1.429a.25.25 0 0 0-.153 0l-5.25 1.68a.25.25 0 0 0-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.196.196 0 0 0 .154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.251.251 0 0 0-.174-.237l-5.25-1.68ZM8.75 4.75v3a.75.75 0 0 1-1.5 0v-3a.75.75 0 0 1 1.5 0ZM9 10.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Security
</span>      
</a>
  
</li>
        <li hidden="hidden" data-menu-item="i7insights-tab" data-targets="action-list.items" role="none" data-view-component="true" class="ActionListItem">
    
    
    <a tabindex="-1" id="item-ae400e97-d248-4485-bfe3-7165546a3719" href="/pytorch/pytorch/pulse" role="menuitem" data-view-component="true" class="ActionListContent ActionListContent--visual16">
        <span class="ActionListItem-visual ActionListItem-visual--leading">
          <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-graph">
    <path d="M1.5 1.75V13.5h13.75a.75.75 0 0 1 0 1.5H.75a.75.75 0 0 1-.75-.75V1.75a.75.75 0 0 1 1.5 0Zm14.28 2.53-5.25 5.25a.75.75 0 0 1-1.06 0L7 7.06 4.28 9.78a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l3.25-3.25a.75.75 0 0 1 1.06 0L10 7.94l4.72-4.72a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042Z"></path>
</svg>
        </span>
      
        <span data-view-component="true" class="ActionListItem-label">
          Insights
</span>      
</a>
  
</li>
</ul>    
</div></action-list>


</div>
      
</div></anchored-position>  </focus-group>
</action-menu></div>
</nav>

  </div>
  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance" class="">
    <div id="repo-content-pjax-container" class="repository-content " >
    



    
      
    








<react-app
  app-name="react-code-view"
  initial-path="/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml"
    style="display: block; min-height: calc(100vh - 64px);"
  data-attempted-ssr="false"
  data-ssr="false"
  data-lazy="false"
  data-alternate="false"
  data-data-router-enabled="false"
  data-react-profiling="false"
>
  
  <script type="application/json" data-target="react-app.embeddedData">{"payload":{"allShortcutsEnabled":false,"fileTree":{"aten/src/ATen/native":{"items":[{"name":"ao_sparse","path":"aten/src/ATen/native/ao_sparse","contentType":"directory"},{"name":"cpu","path":"aten/src/ATen/native/cpu","contentType":"directory"},{"name":"cuda","path":"aten/src/ATen/native/cuda","contentType":"directory"},{"name":"cudnn","path":"aten/src/ATen/native/cudnn","contentType":"directory"},{"name":"hip","path":"aten/src/ATen/native/hip","contentType":"directory"},{"name":"kleidiai","path":"aten/src/ATen/native/kleidiai","contentType":"directory"},{"name":"metal","path":"aten/src/ATen/native/metal","contentType":"directory"},{"name":"miopen","path":"aten/src/ATen/native/miopen","contentType":"directory"},{"name":"mkl","path":"aten/src/ATen/native/mkl","contentType":"directory"},{"name":"mkldnn","path":"aten/src/ATen/native/mkldnn","contentType":"directory"},{"name":"mps","path":"aten/src/ATen/native/mps","contentType":"directory"},{"name":"mtia","path":"aten/src/ATen/native/mtia","contentType":"directory"},{"name":"nested","path":"aten/src/ATen/native/nested","contentType":"directory"},{"name":"quantized","path":"aten/src/ATen/native/quantized","contentType":"directory"},{"name":"sparse","path":"aten/src/ATen/native/sparse","contentType":"directory"},{"name":"transformers","path":"aten/src/ATen/native/transformers","contentType":"directory"},{"name":"ufunc","path":"aten/src/ATen/native/ufunc","contentType":"directory"},{"name":"utils","path":"aten/src/ATen/native/utils","contentType":"directory"},{"name":"vulkan","path":"aten/src/ATen/native/vulkan","contentType":"directory"},{"name":"xnnpack","path":"aten/src/ATen/native/xnnpack","contentType":"directory"},{"name":"Activation.cpp","path":"aten/src/ATen/native/Activation.cpp","contentType":"file"},{"name":"Activation.h","path":"aten/src/ATen/native/Activation.h","contentType":"file"},{"name":"AdaptiveAveragePooling.cpp","path":"aten/src/ATen/native/AdaptiveAveragePooling.cpp","contentType":"file"},{"name":"AdaptiveAveragePooling3d.cpp","path":"aten/src/ATen/native/AdaptiveAveragePooling3d.cpp","contentType":"file"},{"name":"AdaptiveMaxPooling2d.cpp","path":"aten/src/ATen/native/AdaptiveMaxPooling2d.cpp","contentType":"file"},{"name":"AdaptiveMaxPooling3d.cpp","path":"aten/src/ATen/native/AdaptiveMaxPooling3d.cpp","contentType":"file"},{"name":"AdaptivePooling.h","path":"aten/src/ATen/native/AdaptivePooling.h","contentType":"file"},{"name":"AffineGridGenerator.cpp","path":"aten/src/ATen/native/AffineGridGenerator.cpp","contentType":"file"},{"name":"AmpKernels.cpp","path":"aten/src/ATen/native/AmpKernels.cpp","contentType":"file"},{"name":"AmpKernels.h","path":"aten/src/ATen/native/AmpKernels.h","contentType":"file"},{"name":"AutogradComposite.cpp","path":"aten/src/ATen/native/AutogradComposite.cpp","contentType":"file"},{"name":"AveragePool2d.cpp","path":"aten/src/ATen/native/AveragePool2d.cpp","contentType":"file"},{"name":"AveragePool3d.cpp","path":"aten/src/ATen/native/AveragePool3d.cpp","contentType":"file"},{"name":"BatchLinearAlgebra.cpp","path":"aten/src/ATen/native/BatchLinearAlgebra.cpp","contentType":"file"},{"name":"BatchLinearAlgebra.h","path":"aten/src/ATen/native/BatchLinearAlgebra.h","contentType":"file"},{"name":"BatchLinearAlgebraKernel.cpp","path":"aten/src/ATen/native/BatchLinearAlgebraKernel.cpp","contentType":"file"},{"name":"BinaryOps.cpp","path":"aten/src/ATen/native/BinaryOps.cpp","contentType":"file"},{"name":"BinaryOps.h","path":"aten/src/ATen/native/BinaryOps.h","contentType":"file"},{"name":"Blas.cpp","path":"aten/src/ATen/native/Blas.cpp","contentType":"file"},{"name":"BlasKernel.cpp","path":"aten/src/ATen/native/BlasKernel.cpp","contentType":"file"},{"name":"Bucketization.cpp","path":"aten/src/ATen/native/Bucketization.cpp","contentType":"file"},{"name":"BucketizationUtils.h","path":"aten/src/ATen/native/BucketizationUtils.h","contentType":"file"},{"name":"CPUBlas.cpp","path":"aten/src/ATen/native/CPUBlas.cpp","contentType":"file"},{"name":"CPUBlas.h","path":"aten/src/ATen/native/CPUBlas.h","contentType":"file"},{"name":"CPUFallback.cpp","path":"aten/src/ATen/native/CPUFallback.cpp","contentType":"file"},{"name":"CPUFallback.h","path":"aten/src/ATen/native/CPUFallback.h","contentType":"file"},{"name":"CanUse32BitIndexMath.h","path":"aten/src/ATen/native/CanUse32BitIndexMath.h","contentType":"file"},{"name":"ChanelShuffle.cpp","path":"aten/src/ATen/native/ChanelShuffle.cpp","contentType":"file"},{"name":"Col2Im.cpp","path":"aten/src/ATen/native/Col2Im.cpp","contentType":"file"},{"name":"ComparisonUtils.cpp","path":"aten/src/ATen/native/ComparisonUtils.cpp","contentType":"file"},{"name":"ComplexHelper.h","path":"aten/src/ATen/native/ComplexHelper.h","contentType":"file"},{"name":"CompositeRandomAccessor.h","path":"aten/src/ATen/native/CompositeRandomAccessor.h","contentType":"file"},{"name":"CompositeRandomAccessorCommon.h","path":"aten/src/ATen/native/CompositeRandomAccessorCommon.h","contentType":"file"},{"name":"Constraints.cpp","path":"aten/src/ATen/native/Constraints.cpp","contentType":"file"},{"name":"ConvUtils.h","path":"aten/src/ATen/native/ConvUtils.h","contentType":"file"},{"name":"Convolution.cpp","path":"aten/src/ATen/native/Convolution.cpp","contentType":"file"},{"name":"ConvolutionMM2d.cpp","path":"aten/src/ATen/native/ConvolutionMM2d.cpp","contentType":"file"},{"name":"ConvolutionMM3d.cpp","path":"aten/src/ATen/native/ConvolutionMM3d.cpp","contentType":"file"},{"name":"ConvolutionMM3d.h","path":"aten/src/ATen/native/ConvolutionMM3d.h","contentType":"file"},{"name":"ConvolutionTBC.cpp","path":"aten/src/ATen/native/ConvolutionTBC.cpp","contentType":"file"},{"name":"Copy.cpp","path":"aten/src/ATen/native/Copy.cpp","contentType":"file"},{"name":"Copy.h","path":"aten/src/ATen/native/Copy.h","contentType":"file"},{"name":"Correlation.cpp","path":"aten/src/ATen/native/Correlation.cpp","contentType":"file"},{"name":"Cross.cpp","path":"aten/src/ATen/native/Cross.cpp","contentType":"file"},{"name":"Cross.h","path":"aten/src/ATen/native/Cross.h","contentType":"file"},{"name":"DilatedConvolutionUtils.h","path":"aten/src/ATen/native/DilatedConvolutionUtils.h","contentType":"file"},{"name":"DilatedMaxPool2d.cpp","path":"aten/src/ATen/native/DilatedMaxPool2d.cpp","contentType":"file"},{"name":"DilatedMaxPool3d.cpp","path":"aten/src/ATen/native/DilatedMaxPool3d.cpp","contentType":"file"},{"name":"DispatchStub.cpp","path":"aten/src/ATen/native/DispatchStub.cpp","contentType":"file"},{"name":"DispatchStub.h","path":"aten/src/ATen/native/DispatchStub.h","contentType":"file"},{"name":"Distance.cpp","path":"aten/src/ATen/native/Distance.cpp","contentType":"file"},{"name":"Distance.h","path":"aten/src/ATen/native/Distance.h","contentType":"file"},{"name":"DistributionTemplates.h","path":"aten/src/ATen/native/DistributionTemplates.h","contentType":"file"},{"name":"Distributions.cpp","path":"aten/src/ATen/native/Distributions.cpp","contentType":"file"},{"name":"Distributions.h","path":"aten/src/ATen/native/Distributions.h","contentType":"file"},{"name":"Dropout.cpp","path":"aten/src/ATen/native/Dropout.cpp","contentType":"file"},{"name":"Embedding.cpp","path":"aten/src/ATen/native/Embedding.cpp","contentType":"file"},{"name":"EmbeddingBag.cpp","path":"aten/src/ATen/native/EmbeddingBag.cpp","contentType":"file"},{"name":"EmbeddingBag.h","path":"aten/src/ATen/native/EmbeddingBag.h","contentType":"file"},{"name":"Fill.cpp","path":"aten/src/ATen/native/Fill.cpp","contentType":"file"},{"name":"Fill.h","path":"aten/src/ATen/native/Fill.h","contentType":"file"},{"name":"ForeachOpsKernels.cpp","path":"aten/src/ATen/native/ForeachOpsKernels.cpp","contentType":"file"},{"name":"ForeachUtils.h","path":"aten/src/ATen/native/ForeachUtils.h","contentType":"file"},{"name":"FractionalMaxPool2d.cpp","path":"aten/src/ATen/native/FractionalMaxPool2d.cpp","contentType":"file"},{"name":"FractionalMaxPool3d.cpp","path":"aten/src/ATen/native/FractionalMaxPool3d.cpp","contentType":"file"},{"name":"FractionalMaxPooling.h","path":"aten/src/ATen/native/FractionalMaxPooling.h","contentType":"file"},{"name":"FunctionOfAMatrixUtils.cpp","path":"aten/src/ATen/native/FunctionOfAMatrixUtils.cpp","contentType":"file"},{"name":"FunctionOfAMatrixUtils.h","path":"aten/src/ATen/native/FunctionOfAMatrixUtils.h","contentType":"file"},{"name":"FusedAdagrad.cpp","path":"aten/src/ATen/native/FusedAdagrad.cpp","contentType":"file"},{"name":"FusedAdagrad.h","path":"aten/src/ATen/native/FusedAdagrad.h","contentType":"file"},{"name":"FusedAdam.cpp","path":"aten/src/ATen/native/FusedAdam.cpp","contentType":"file"},{"name":"FusedAdam.h","path":"aten/src/ATen/native/FusedAdam.h","contentType":"file"},{"name":"FusedSGD.cpp","path":"aten/src/ATen/native/FusedSGD.cpp","contentType":"file"},{"name":"FusedSGD.h","path":"aten/src/ATen/native/FusedSGD.h","contentType":"file"},{"name":"GatedLinearUnit.cpp","path":"aten/src/ATen/native/GatedLinearUnit.cpp","contentType":"file"},{"name":"Gelu.h","path":"aten/src/ATen/native/Gelu.h","contentType":"file"},{"name":"GridSampler.cpp","path":"aten/src/ATen/native/GridSampler.cpp","contentType":"file"},{"name":"GridSampler.h","path":"aten/src/ATen/native/GridSampler.h","contentType":"file"},{"name":"GridSamplerUtils.h","path":"aten/src/ATen/native/GridSamplerUtils.h","contentType":"file"},{"name":"GroupedMMUtils.h","path":"aten/src/ATen/native/GroupedMMUtils.h","contentType":"file"},{"name":"Histogram.cpp","path":"aten/src/ATen/native/Histogram.cpp","contentType":"file"},{"name":"Histogram.h","path":"aten/src/ATen/native/Histogram.h","contentType":"file"},{"name":"Im2Col.cpp","path":"aten/src/ATen/native/Im2Col.cpp","contentType":"file"},{"name":"IndexKernel.h","path":"aten/src/ATen/native/IndexKernel.h","contentType":"file"},{"name":"IndexingUtils.cpp","path":"aten/src/ATen/native/IndexingUtils.cpp","contentType":"file"},{"name":"IndexingUtils.h","path":"aten/src/ATen/native/IndexingUtils.h","contentType":"file"},{"name":"Integration.cpp","path":"aten/src/ATen/native/Integration.cpp","contentType":"file"},{"name":"Itertools.cpp","path":"aten/src/ATen/native/Itertools.cpp","contentType":"file"},{"name":"LegacyBatching.cpp","path":"aten/src/ATen/native/LegacyBatching.cpp","contentType":"file"},{"name":"Lerp.cpp","path":"aten/src/ATen/native/Lerp.cpp","contentType":"file"},{"name":"Lerp.h","path":"aten/src/ATen/native/Lerp.h","contentType":"file"},{"name":"Linear.cpp","path":"aten/src/ATen/native/Linear.cpp","contentType":"file"},{"name":"LinearAlgebra.cpp","path":"aten/src/ATen/native/LinearAlgebra.cpp","contentType":"file"},{"name":"LinearAlgebra.h","path":"aten/src/ATen/native/LinearAlgebra.h","contentType":"file"},{"name":"LinearAlgebraUtils.h","path":"aten/src/ATen/native/LinearAlgebraUtils.h","contentType":"file"},{"name":"Loss.cpp","path":"aten/src/ATen/native/Loss.cpp","contentType":"file"},{"name":"LossCTC.cpp","path":"aten/src/ATen/native/LossCTC.cpp","contentType":"file"},{"name":"LossMulti.h","path":"aten/src/ATen/native/LossMulti.h","contentType":"file"},{"name":"LossMultiLabelMargin.cpp","path":"aten/src/ATen/native/LossMultiLabelMargin.cpp","contentType":"file"},{"name":"LossMultiMargin.cpp","path":"aten/src/ATen/native/LossMultiMargin.cpp","contentType":"file"},{"name":"LossNLL.cpp","path":"aten/src/ATen/native/LossNLL.cpp","contentType":"file"},{"name":"LossNLL2d.cpp","path":"aten/src/ATen/native/LossNLL2d.cpp","contentType":"file"},{"name":"Math.h","path":"aten/src/ATen/native/Math.h","contentType":"file"},{"name":"MathBitFallThroughLists.h","path":"aten/src/ATen/native/MathBitFallThroughLists.h","contentType":"file"},{"name":"MathBitsFallback.h","path":"aten/src/ATen/native/MathBitsFallback.h","contentType":"file"},{"name":"MaxPooling.cpp","path":"aten/src/ATen/native/MaxPooling.cpp","contentType":"file"},{"name":"MaxPooling.h","path":"aten/src/ATen/native/MaxPooling.h","contentType":"file"},{"name":"MaxUnpooling.cpp","path":"aten/src/ATen/native/MaxUnpooling.cpp","contentType":"file"},{"name":"Memory.cpp","path":"aten/src/ATen/native/Memory.cpp","contentType":"file"},{"name":"MetaTensor.cpp","path":"aten/src/ATen/native/MetaTensor.cpp","contentType":"file"},{"name":"NNPACK.cpp","path":"aten/src/ATen/native/NNPACK.cpp","contentType":"file"},{"name":"NaiveConvolutionTranspose2d.cpp","path":"aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp","contentType":"file"},{"name":"NaiveConvolutionTranspose3d.cpp","path":"aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp","contentType":"file"},{"name":"NaiveDilatedConvolution.cpp","path":"aten/src/ATen/native/NaiveDilatedConvolution.cpp","contentType":"file"},{"name":"NamedTensor.cpp","path":"aten/src/ATen/native/NamedTensor.cpp","contentType":"file"},{"name":"NegateFallback.cpp","path":"aten/src/ATen/native/NegateFallback.cpp","contentType":"file"},{"name":"NonEmptyUtils.h","path":"aten/src/ATen/native/NonEmptyUtils.h","contentType":"file"},{"name":"NonSymbolicBC.h","path":"aten/src/ATen/native/NonSymbolicBC.h","contentType":"file"},{"name":"Normalization.cpp","path":"aten/src/ATen/native/Normalization.cpp","contentType":"file"},{"name":"Normalization.h","path":"aten/src/ATen/native/Normalization.h","contentType":"file"},{"name":"Onehot.cpp","path":"aten/src/ATen/native/Onehot.cpp","contentType":"file"},{"name":"PackedSequence.cpp","path":"aten/src/ATen/native/PackedSequence.cpp","contentType":"file"},{"name":"PadNd.cpp","path":"aten/src/ATen/native/PadNd.cpp","contentType":"file"},{"name":"Padding.h","path":"aten/src/ATen/native/Padding.h","contentType":"file"},{"name":"PixelShuffle.cpp","path":"aten/src/ATen/native/PixelShuffle.cpp","contentType":"file"},{"name":"PixelShuffle.h","path":"aten/src/ATen/native/PixelShuffle.h","contentType":"file"},{"name":"PointwiseOps.cpp","path":"aten/src/ATen/native/PointwiseOps.cpp","contentType":"file"},{"name":"PointwiseOps.h","path":"aten/src/ATen/native/PointwiseOps.h","contentType":"file"},{"name":"Pool.h","path":"aten/src/ATen/native/Pool.h","contentType":"file"},{"name":"Pooling.cpp","path":"aten/src/ATen/native/Pooling.cpp","contentType":"file"},{"name":"Pow.cpp","path":"aten/src/ATen/native/Pow.cpp","contentType":"file"},{"name":"Pow.h","path":"aten/src/ATen/native/Pow.h","contentType":"file"},{"name":"QuantizedLinear.cpp","path":"aten/src/ATen/native/QuantizedLinear.cpp","contentType":"file"},{"name":"README.md","path":"aten/src/ATen/native/README.md","contentType":"file"},{"name":"RNN.cpp","path":"aten/src/ATen/native/RNN.cpp","contentType":"file"},{"name":"RNN.h","path":"aten/src/ATen/native/RNN.h","contentType":"file"},{"name":"RangeFactories.cpp","path":"aten/src/ATen/native/RangeFactories.cpp","contentType":"file"},{"name":"RangeFactories.h","path":"aten/src/ATen/native/RangeFactories.h","contentType":"file"},{"name":"RangeUtils.h","path":"aten/src/ATen/native/RangeUtils.h","contentType":"file"},{"name":"ReduceAllOps.cpp","path":"aten/src/ATen/native/ReduceAllOps.cpp","contentType":"file"},{"name":"ReduceAllOps.h","path":"aten/src/ATen/native/ReduceAllOps.h","contentType":"file"},{"name":"ReduceOps.cpp","path":"aten/src/ATen/native/ReduceOps.cpp","contentType":"file"},{"name":"ReduceOps.h","path":"aten/src/ATen/native/ReduceOps.h","contentType":"file"},{"name":"ReduceOpsUtils.h","path":"aten/src/ATen/native/ReduceOpsUtils.h","contentType":"file"},{"name":"ReductionType.h","path":"aten/src/ATen/native/ReductionType.h","contentType":"file"},{"name":"ReflectionPad.cpp","path":"aten/src/ATen/native/ReflectionPad.cpp","contentType":"file"},{"name":"Repeat.cpp","path":"aten/src/ATen/native/Repeat.cpp","contentType":"file"},{"name":"Repeat.h","path":"aten/src/ATen/native/Repeat.h","contentType":"file"},{"name":"ReplicationPadding.cpp","path":"aten/src/ATen/native/ReplicationPadding.cpp","contentType":"file"},{"name":"Resize.cpp","path":"aten/src/ATen/native/Resize.cpp","contentType":"file"},{"name":"Resize.h","path":"aten/src/ATen/native/Resize.h","contentType":"file"},{"name":"ResizeCommon.h","path":"aten/src/ATen/native/ResizeCommon.h","contentType":"file"},{"name":"RowwisePrune.cpp","path":"aten/src/ATen/native/RowwisePrune.cpp","contentType":"file"},{"name":"Scalar.cpp","path":"aten/src/ATen/native/Scalar.cpp","contentType":"file"},{"name":"ScatterGatherChecks.h","path":"aten/src/ATen/native/ScatterGatherChecks.h","contentType":"file"},{"name":"SegmentReduce.cpp","path":"aten/src/ATen/native/SegmentReduce.cpp","contentType":"file"},{"name":"SegmentReduce.h","path":"aten/src/ATen/native/SegmentReduce.h","contentType":"file"},{"name":"SharedReduceOps.h","path":"aten/src/ATen/native/SharedReduceOps.h","contentType":"file"},{"name":"SobolEngineOps.cpp","path":"aten/src/ATen/native/SobolEngineOps.cpp","contentType":"file"},{"name":"SobolEngineOpsUtils.cpp","path":"aten/src/ATen/native/SobolEngineOpsUtils.cpp","contentType":"file"},{"name":"SobolEngineOpsUtils.h","path":"aten/src/ATen/native/SobolEngineOpsUtils.h","contentType":"file"},{"name":"SoftMax.cpp","path":"aten/src/ATen/native/SoftMax.cpp","contentType":"file"},{"name":"Sorting.cpp","path":"aten/src/ATen/native/Sorting.cpp","contentType":"file"},{"name":"Sorting.h","path":"aten/src/ATen/native/Sorting.h","contentType":"file"},{"name":"SortingUtils.h","path":"aten/src/ATen/native/SortingUtils.h","contentType":"file"},{"name":"SparseTensorUtils.cpp","path":"aten/src/ATen/native/SparseTensorUtils.cpp","contentType":"file"},{"name":"SparseTensorUtils.h","path":"aten/src/ATen/native/SparseTensorUtils.h","contentType":"file"},{"name":"SpectralOps.cpp","path":"aten/src/ATen/native/SpectralOps.cpp","contentType":"file"},{"name":"SpectralOpsUtils.h","path":"aten/src/ATen/native/SpectralOpsUtils.h","contentType":"file"},{"name":"StridedRandomAccessor.h","path":"aten/src/ATen/native/StridedRandomAccessor.h","contentType":"file"},{"name":"SummaryOps.cpp","path":"aten/src/ATen/native/SummaryOps.cpp","contentType":"file"},{"name":"TensorAdvancedIndexing.cpp","path":"aten/src/ATen/native/TensorAdvancedIndexing.cpp","contentType":"file"},{"name":"TensorAdvancedIndexing.h","path":"aten/src/ATen/native/TensorAdvancedIndexing.h","contentType":"file"},{"name":"TensorAdvancedIndexingUtils.h","path":"aten/src/ATen/native/TensorAdvancedIndexingUtils.h","contentType":"file"},{"name":"TensorCompare.cpp","path":"aten/src/ATen/native/TensorCompare.cpp","contentType":"file"},{"name":"TensorCompare.h","path":"aten/src/ATen/native/TensorCompare.h","contentType":"file"},{"name":"TensorConversions.cpp","path":"aten/src/ATen/native/TensorConversions.cpp","contentType":"file"},{"name":"TensorConversions.h","path":"aten/src/ATen/native/TensorConversions.h","contentType":"file"},{"name":"TensorDimApply.h","path":"aten/src/ATen/native/TensorDimApply.h","contentType":"file"},{"name":"TensorFactories.cpp","path":"aten/src/ATen/native/TensorFactories.cpp","contentType":"file"},{"name":"TensorFactories.h","path":"aten/src/ATen/native/TensorFactories.h","contentType":"file"},{"name":"TensorIterator.h","path":"aten/src/ATen/native/TensorIterator.h","contentType":"file"},{"name":"TensorIteratorDynamicCasting.h","path":"aten/src/ATen/native/TensorIteratorDynamicCasting.h","contentType":"file"},{"name":"TensorIteratorReduce.cpp","path":"aten/src/ATen/native/TensorIteratorReduce.cpp","contentType":"file"},{"name":"TensorProperties.cpp","path":"aten/src/ATen/native/TensorProperties.cpp","contentType":"file"},{"name":"TensorProperties.h","path":"aten/src/ATen/native/TensorProperties.h","contentType":"file"},{"name":"TensorShape.cpp","path":"aten/src/ATen/native/TensorShape.cpp","contentType":"file"},{"name":"TensorShape.h","path":"aten/src/ATen/native/TensorShape.h","contentType":"file"},{"name":"TensorTransformations.cpp","path":"aten/src/ATen/native/TensorTransformations.cpp","contentType":"file"},{"name":"TensorTransformations.h","path":"aten/src/ATen/native/TensorTransformations.h","contentType":"file"},{"name":"TestOps.cpp","path":"aten/src/ATen/native/TestOps.cpp","contentType":"file"},{"name":"TopKImpl.h","path":"aten/src/ATen/native/TopKImpl.h","contentType":"file"},{"name":"TransposeType.h","path":"aten/src/ATen/native/TransposeType.h","contentType":"file"},{"name":"TriangularOps.cpp","path":"aten/src/ATen/native/TriangularOps.cpp","contentType":"file"},{"name":"TriangularOpsUtils.h","path":"aten/src/ATen/native/TriangularOpsUtils.h","contentType":"file"},{"name":"TypeProperties.cpp","path":"aten/src/ATen/native/TypeProperties.cpp","contentType":"file"},{"name":"TypeProperties.h","path":"aten/src/ATen/native/TypeProperties.h","contentType":"file"},{"name":"UnaryOps.cpp","path":"aten/src/ATen/native/UnaryOps.cpp","contentType":"file"},{"name":"UnaryOps.h","path":"aten/src/ATen/native/UnaryOps.h","contentType":"file"},{"name":"Unfold2d.cpp","path":"aten/src/ATen/native/Unfold2d.cpp","contentType":"file"},{"name":"Unfold2d.h","path":"aten/src/ATen/native/Unfold2d.h","contentType":"file"},{"name":"Unfold3d.cpp","path":"aten/src/ATen/native/Unfold3d.cpp","contentType":"file"},{"name":"Unfold3d.h","path":"aten/src/ATen/native/Unfold3d.h","contentType":"file"},{"name":"UnfoldBackward.cpp","path":"aten/src/ATen/native/UnfoldBackward.cpp","contentType":"file"},{"name":"UnfoldBackward.h","path":"aten/src/ATen/native/UnfoldBackward.h","contentType":"file"},{"name":"Unique.cpp","path":"aten/src/ATen/native/Unique.cpp","contentType":"file"},{"name":"UpSample.cpp","path":"aten/src/ATen/native/UpSample.cpp","contentType":"file"},{"name":"UpSample.h","path":"aten/src/ATen/native/UpSample.h","contentType":"file"},{"name":"UpSampleBicubic2d.cpp","path":"aten/src/ATen/native/UpSampleBicubic2d.cpp","contentType":"file"},{"name":"UpSampleBilinear2d.cpp","path":"aten/src/ATen/native/UpSampleBilinear2d.cpp","contentType":"file"},{"name":"UpSampleLinear1d.cpp","path":"aten/src/ATen/native/UpSampleLinear1d.cpp","contentType":"file"},{"name":"UpSampleNearest1d.cpp","path":"aten/src/ATen/native/UpSampleNearest1d.cpp","contentType":"file"},{"name":"UpSampleNearest2d.cpp","path":"aten/src/ATen/native/UpSampleNearest2d.cpp","contentType":"file"},{"name":"UpSampleNearest3d.cpp","path":"aten/src/ATen/native/UpSampleNearest3d.cpp","contentType":"file"},{"name":"UpSampleTrilinear3d.cpp","path":"aten/src/ATen/native/UpSampleTrilinear3d.cpp","contentType":"file"},{"name":"VariableMethodStubs.cpp","path":"aten/src/ATen/native/VariableMethodStubs.cpp","contentType":"file"},{"name":"WeightNorm.cpp","path":"aten/src/ATen/native/WeightNorm.cpp","contentType":"file"},{"name":"batch_norm.h","path":"aten/src/ATen/native/batch_norm.h","contentType":"file"},{"name":"group_norm.cpp","path":"aten/src/ATen/native/group_norm.cpp","contentType":"file"},{"name":"group_norm.h","path":"aten/src/ATen/native/group_norm.h","contentType":"file"},{"name":"im2col.h","path":"aten/src/ATen/native/im2col.h","contentType":"file"},{"name":"im2col_shape_check.h","path":"aten/src/ATen/native/im2col_shape_check.h","contentType":"file"},{"name":"layer_norm.cpp","path":"aten/src/ATen/native/layer_norm.cpp","contentType":"file"},{"name":"layer_norm.h","path":"aten/src/ATen/native/layer_norm.h","contentType":"file"},{"name":"native_functions.yaml","path":"aten/src/ATen/native/native_functions.yaml","contentType":"file"},{"name":"prim_native_functions.cpp","path":"aten/src/ATen/native/prim_native_functions.cpp","contentType":"file"},{"name":"tags.yaml","path":"aten/src/ATen/native/tags.yaml","contentType":"file"},{"name":"ts_native_functions.yaml","path":"aten/src/ATen/native/ts_native_functions.yaml","contentType":"file"},{"name":"verbose_wrapper.cpp","path":"aten/src/ATen/native/verbose_wrapper.cpp","contentType":"file"},{"name":"verbose_wrapper.h","path":"aten/src/ATen/native/verbose_wrapper.h","contentType":"file"},{"name":"vol2col.h","path":"aten/src/ATen/native/vol2col.h","contentType":"file"}],"totalCount":251},"aten/src/ATen":{"items":[{"name":"benchmarks","path":"aten/src/ATen/benchmarks","contentType":"directory"},{"name":"core","path":"aten/src/ATen/core","contentType":"directory"},{"name":"cpu","path":"aten/src/ATen/cpu","contentType":"directory"},{"name":"cuda","path":"aten/src/ATen/cuda","contentType":"directory"},{"name":"cudnn","path":"aten/src/ATen/cudnn","contentType":"directory"},{"name":"detail","path":"aten/src/ATen/detail","contentType":"directory"},{"name":"functorch","path":"aten/src/ATen/functorch","contentType":"directory"},{"name":"hip","path":"aten/src/ATen/hip","contentType":"directory"},{"name":"metal","path":"aten/src/ATen/metal","contentType":"directory"},{"name":"miopen","path":"aten/src/ATen/miopen","contentType":"directory"},{"name":"mkl","path":"aten/src/ATen/mkl","contentType":"directory"},{"name":"mps","path":"aten/src/ATen/mps","contentType":"directory"},{"name":"native","path":"aten/src/ATen/native","contentType":"directory"},{"name":"nnapi","path":"aten/src/ATen/nnapi","contentType":"directory"},{"name":"ops","path":"aten/src/ATen/ops","contentType":"directory"},{"name":"quantized","path":"aten/src/ATen/quantized","contentType":"directory"},{"name":"templates","path":"aten/src/ATen/templates","contentType":"directory"},{"name":"test","path":"aten/src/ATen/test","contentType":"directory"},{"name":"vulkan","path":"aten/src/ATen/vulkan","contentType":"directory"},{"name":"xpu","path":"aten/src/ATen/xpu","contentType":"directory"},{"name":".gitignore","path":"aten/src/ATen/.gitignore","contentType":"file"},{"name":"ATen.h","path":"aten/src/ATen/ATen.h","contentType":"file"},{"name":"ATenConfig.cmake.in","path":"aten/src/ATen/ATenConfig.cmake.in","contentType":"file"},{"name":"AccumulateType.cpp","path":"aten/src/ATen/AccumulateType.cpp","contentType":"file"},{"name":"AccumulateType.h","path":"aten/src/ATen/AccumulateType.h","contentType":"file"},{"name":"ArrayRef.h","path":"aten/src/ATen/ArrayRef.h","contentType":"file"},{"name":"Backend.h","path":"aten/src/ATen/Backend.h","contentType":"file"},{"name":"Backtrace.h","path":"aten/src/ATen/Backtrace.h","contentType":"file"},{"name":"BlasBackend.h","path":"aten/src/ATen/BlasBackend.h","contentType":"file"},{"name":"CMakeLists.txt","path":"aten/src/ATen/CMakeLists.txt","contentType":"file"},{"name":"CPUApplyUtils.h","path":"aten/src/ATen/CPUApplyUtils.h","contentType":"file"},{"name":"CPUFixedAllocator.h","path":"aten/src/ATen/CPUFixedAllocator.h","contentType":"file"},{"name":"CPUGeneratorImpl.cpp","path":"aten/src/ATen/CPUGeneratorImpl.cpp","contentType":"file"},{"name":"CPUGeneratorImpl.h","path":"aten/src/ATen/CPUGeneratorImpl.h","contentType":"file"},{"name":"CachedTensorUtils.cpp","path":"aten/src/ATen/CachedTensorUtils.cpp","contentType":"file"},{"name":"CachedTensorUtils.h","path":"aten/src/ATen/CachedTensorUtils.h","contentType":"file"},{"name":"CollapseDims.h","path":"aten/src/ATen/CollapseDims.h","contentType":"file"},{"name":"Config.h.in","path":"aten/src/ATen/Config.h.in","contentType":"file"},{"name":"ConjugateFallback.cpp","path":"aten/src/ATen/ConjugateFallback.cpp","contentType":"file"},{"name":"Context.cpp","path":"aten/src/ATen/Context.cpp","contentType":"file"},{"name":"Context.h","path":"aten/src/ATen/Context.h","contentType":"file"},{"name":"DLConvertor.cpp","path":"aten/src/ATen/DLConvertor.cpp","contentType":"file"},{"name":"DLConvertor.h","path":"aten/src/ATen/DLConvertor.h","contentType":"file"},{"name":"DTensorState.cpp","path":"aten/src/ATen/DTensorState.cpp","contentType":"file"},{"name":"DTensorState.h","path":"aten/src/ATen/DTensorState.h","contentType":"file"},{"name":"Device.h","path":"aten/src/ATen/Device.h","contentType":"file"},{"name":"DeviceAccelerator.cpp","path":"aten/src/ATen/DeviceAccelerator.cpp","contentType":"file"},{"name":"DeviceAccelerator.h","path":"aten/src/ATen/DeviceAccelerator.h","contentType":"file"},{"name":"DeviceGuard.h","path":"aten/src/ATen/DeviceGuard.h","contentType":"file"},{"name":"DimVector.h","path":"aten/src/ATen/DimVector.h","contentType":"file"},{"name":"Dimname.h","path":"aten/src/ATen/Dimname.h","contentType":"file"},{"name":"Dispatch.cpp","path":"aten/src/ATen/Dispatch.cpp","contentType":"file"},{"name":"Dispatch.h","path":"aten/src/ATen/Dispatch.h","contentType":"file"},{"name":"Dispatch_v2.h","path":"aten/src/ATen/Dispatch_v2.h","contentType":"file"},{"name":"DynamicLibrary.cpp","path":"aten/src/ATen/DynamicLibrary.cpp","contentType":"file"},{"name":"DynamicLibrary.h","path":"aten/src/ATen/DynamicLibrary.h","contentType":"file"},{"name":"EmptyTensor.cpp","path":"aten/src/ATen/EmptyTensor.cpp","contentType":"file"},{"name":"EmptyTensor.h","path":"aten/src/ATen/EmptyTensor.h","contentType":"file"},{"name":"ExpandBase.h","path":"aten/src/ATen/ExpandBase.h","contentType":"file"},{"name":"ExpandUtils.cpp","path":"aten/src/ATen/ExpandUtils.cpp","contentType":"file"},{"name":"ExpandUtils.h","path":"aten/src/ATen/ExpandUtils.h","contentType":"file"},{"name":"Formatting.h","path":"aten/src/ATen/Formatting.h","contentType":"file"},{"name":"FuncTorchTLS.cpp","path":"aten/src/ATen/FuncTorchTLS.cpp","contentType":"file"},{"name":"FuncTorchTLS.h","path":"aten/src/ATen/FuncTorchTLS.h","contentType":"file"},{"name":"FunctionalInverses.cpp","path":"aten/src/ATen/FunctionalInverses.cpp","contentType":"file"},{"name":"FunctionalStorageImpl.cpp","path":"aten/src/ATen/FunctionalStorageImpl.cpp","contentType":"file"},{"name":"FunctionalStorageImpl.h","path":"aten/src/ATen/FunctionalStorageImpl.h","contentType":"file"},{"name":"FunctionalTensorWrapper.cpp","path":"aten/src/ATen/FunctionalTensorWrapper.cpp","contentType":"file"},{"name":"FunctionalTensorWrapper.h","path":"aten/src/ATen/FunctionalTensorWrapper.h","contentType":"file"},{"name":"FunctionalizeFallbackKernel.cpp","path":"aten/src/ATen/FunctionalizeFallbackKernel.cpp","contentType":"file"},{"name":"FunctionalizeFallbackKernel.h","path":"aten/src/ATen/FunctionalizeFallbackKernel.h","contentType":"file"},{"name":"Generator.h","path":"aten/src/ATen/Generator.h","contentType":"file"},{"name":"InferSize.h","path":"aten/src/ATen/InferSize.h","contentType":"file"},{"name":"InitialTensorOptions.h","path":"aten/src/ATen/InitialTensorOptions.h","contentType":"file"},{"name":"Layout.h","path":"aten/src/ATen/Layout.h","contentType":"file"},{"name":"LegacyBatchedFallback.cpp","path":"aten/src/ATen/LegacyBatchedFallback.cpp","contentType":"file"},{"name":"LegacyBatchedFallback.h","path":"aten/src/ATen/LegacyBatchedFallback.h","contentType":"file"},{"name":"LegacyBatchedTensorImpl.cpp","path":"aten/src/ATen/LegacyBatchedTensorImpl.cpp","contentType":"file"},{"name":"LegacyBatchedTensorImpl.h","path":"aten/src/ATen/LegacyBatchedTensorImpl.h","contentType":"file"},{"name":"LegacyBatchingRegistrations.cpp","path":"aten/src/ATen/LegacyBatchingRegistrations.cpp","contentType":"file"},{"name":"LegacyVmapMode.cpp","path":"aten/src/ATen/LegacyVmapMode.cpp","contentType":"file"},{"name":"LegacyVmapMode.h","path":"aten/src/ATen/LegacyVmapMode.h","contentType":"file"},{"name":"LegacyVmapTransforms.cpp","path":"aten/src/ATen/LegacyVmapTransforms.cpp","contentType":"file"},{"name":"LegacyVmapTransforms.h","path":"aten/src/ATen/LegacyVmapTransforms.h","contentType":"file"},{"name":"LinalgBackend.h","path":"aten/src/ATen/LinalgBackend.h","contentType":"file"},{"name":"MapAllocator.cpp","path":"aten/src/ATen/MapAllocator.cpp","contentType":"file"},{"name":"MapAllocator.h","path":"aten/src/ATen/MapAllocator.h","contentType":"file"},{"name":"MatrixRef.h","path":"aten/src/ATen/MatrixRef.h","contentType":"file"},{"name":"MemoryOverlap.cpp","path":"aten/src/ATen/MemoryOverlap.cpp","contentType":"file"},{"name":"MemoryOverlap.h","path":"aten/src/ATen/MemoryOverlap.h","contentType":"file"},{"name":"NamedTensor.h","path":"aten/src/ATen/NamedTensor.h","contentType":"file"},{"name":"NamedTensorUtils.cpp","path":"aten/src/ATen/NamedTensorUtils.cpp","contentType":"file"},{"name":"NamedTensorUtils.h","path":"aten/src/ATen/NamedTensorUtils.h","contentType":"file"},{"name":"NestedTensorImpl.cpp","path":"aten/src/ATen/NestedTensorImpl.cpp","contentType":"file"},{"name":"NestedTensorImpl.h","path":"aten/src/ATen/NestedTensorImpl.h","contentType":"file"},{"name":"NumericUtils.h","path":"aten/src/ATen/NumericUtils.h","contentType":"file"},{"name":"OpMathType.h","path":"aten/src/ATen/OpMathType.h","contentType":"file"},{"name":"OpaqueTensorImpl.h","path":"aten/src/ATen/OpaqueTensorImpl.h","contentType":"file"},{"name":"PTThreadPool.h","path":"aten/src/ATen/PTThreadPool.h","contentType":"file"},{"name":"PadNd.h","path":"aten/src/ATen/PadNd.h","contentType":"file"},{"name":"Parallel-inl.h","path":"aten/src/ATen/Parallel-inl.h","contentType":"file"},{"name":"Parallel.h","path":"aten/src/ATen/Parallel.h","contentType":"file"},{"name":"ParallelCommon.cpp","path":"aten/src/ATen/ParallelCommon.cpp","contentType":"file"},{"name":"ParallelFuture.h","path":"aten/src/ATen/ParallelFuture.h","contentType":"file"},{"name":"ParallelNative.cpp","path":"aten/src/ATen/ParallelNative.cpp","contentType":"file"},{"name":"ParallelNative.h","path":"aten/src/ATen/ParallelNative.h","contentType":"file"},{"name":"ParallelOpenMP.cpp","path":"aten/src/ATen/ParallelOpenMP.cpp","contentType":"file"},{"name":"ParallelOpenMP.h","path":"aten/src/ATen/ParallelOpenMP.h","contentType":"file"},{"name":"ParallelThreadPoolNative.cpp","path":"aten/src/ATen/ParallelThreadPoolNative.cpp","contentType":"file"},{"name":"PythonTorchFunctionTLS.cpp","path":"aten/src/ATen/PythonTorchFunctionTLS.cpp","contentType":"file"},{"name":"PythonTorchFunctionTLS.h","path":"aten/src/ATen/PythonTorchFunctionTLS.h","contentType":"file"},{"name":"ROCmFABackend.h","path":"aten/src/ATen/ROCmFABackend.h","contentType":"file"},{"name":"SDPBackend.h","path":"aten/src/ATen/SDPBackend.h","contentType":"file"},{"name":"SavedTensorHooks.cpp","path":"aten/src/ATen/SavedTensorHooks.cpp","contentType":"file"},{"name":"SavedTensorHooks.h","path":"aten/src/ATen/SavedTensorHooks.h","contentType":"file"},{"name":"Scalar.h","path":"aten/src/ATen/Scalar.h","contentType":"file"},{"name":"ScalarOps.cpp","path":"aten/src/ATen/ScalarOps.cpp","contentType":"file"},{"name":"ScalarOps.h","path":"aten/src/ATen/ScalarOps.h","contentType":"file"},{"name":"ScalarType.h","path":"aten/src/ATen/ScalarType.h","contentType":"file"},{"name":"SequenceNumber.cpp","path":"aten/src/ATen/SequenceNumber.cpp","contentType":"file"},{"name":"SequenceNumber.h","path":"aten/src/ATen/SequenceNumber.h","contentType":"file"},{"name":"SmallVector.h","path":"aten/src/ATen/SmallVector.h","contentType":"file"},{"name":"SparseCsrTensorImpl.cpp","path":"aten/src/ATen/SparseCsrTensorImpl.cpp","contentType":"file"},{"name":"SparseCsrTensorImpl.h","path":"aten/src/ATen/SparseCsrTensorImpl.h","contentType":"file"},{"name":"SparseCsrTensorUtils.h","path":"aten/src/ATen/SparseCsrTensorUtils.h","contentType":"file"},{"name":"SparseTensorImpl.cpp","path":"aten/src/ATen/SparseTensorImpl.cpp","contentType":"file"},{"name":"SparseTensorImpl.h","path":"aten/src/ATen/SparseTensorImpl.h","contentType":"file"},{"name":"Storage.h","path":"aten/src/ATen/Storage.h","contentType":"file"},{"name":"StorageUtils.cpp","path":"aten/src/ATen/StorageUtils.cpp","contentType":"file"},{"name":"StorageUtils.h","path":"aten/src/ATen/StorageUtils.h","contentType":"file"},{"name":"Tensor.h","path":"aten/src/ATen/Tensor.h","contentType":"file"},{"name":"TensorAccessor.h","path":"aten/src/ATen/TensorAccessor.h","contentType":"file"},{"name":"TensorGeometry.cpp","path":"aten/src/ATen/TensorGeometry.cpp","contentType":"file"},{"name":"TensorGeometry.h","path":"aten/src/ATen/TensorGeometry.h","contentType":"file"},{"name":"TensorIndexing.cpp","path":"aten/src/ATen/TensorIndexing.cpp","contentType":"file"},{"name":"TensorIndexing.h","path":"aten/src/ATen/TensorIndexing.h","contentType":"file"},{"name":"TensorIterator.cpp","path":"aten/src/ATen/TensorIterator.cpp","contentType":"file"},{"name":"TensorIterator.h","path":"aten/src/ATen/TensorIterator.h","contentType":"file"},{"name":"TensorIteratorInternal.h","path":"aten/src/ATen/TensorIteratorInternal.h","contentType":"file"},{"name":"TensorMeta.cpp","path":"aten/src/ATen/TensorMeta.cpp","contentType":"file"},{"name":"TensorMeta.h","path":"aten/src/ATen/TensorMeta.h","contentType":"file"},{"name":"TensorNames.cpp","path":"aten/src/ATen/TensorNames.cpp","contentType":"file"},{"name":"TensorNames.h","path":"aten/src/ATen/TensorNames.h","contentType":"file"},{"name":"TensorOperators.h","path":"aten/src/ATen/TensorOperators.h","contentType":"file"},{"name":"TensorOptions.h","path":"aten/src/ATen/TensorOptions.h","contentType":"file"},{"name":"TensorSubclassLikeUtils.h","path":"aten/src/ATen/TensorSubclassLikeUtils.h","contentType":"file"},{"name":"TensorUtils.cpp","path":"aten/src/ATen/TensorUtils.cpp","contentType":"file"},{"name":"TensorUtils.h","path":"aten/src/ATen/TensorUtils.h","contentType":"file"},{"name":"ThreadLocalPythonObjects.cpp","path":"aten/src/ATen/ThreadLocalPythonObjects.cpp","contentType":"file"},{"name":"ThreadLocalPythonObjects.h","path":"aten/src/ATen/ThreadLocalPythonObjects.h","contentType":"file"},{"name":"ThreadLocalState.cpp","path":"aten/src/ATen/ThreadLocalState.cpp","contentType":"file"},{"name":"ThreadLocalState.h","path":"aten/src/ATen/ThreadLocalState.h","contentType":"file"},{"name":"TracerMode.h","path":"aten/src/ATen/TracerMode.h","contentType":"file"},{"name":"TypeDefault.h","path":"aten/src/ATen/TypeDefault.h","contentType":"file"},{"name":"Utils.cpp","path":"aten/src/ATen/Utils.cpp","contentType":"file"},{"name":"Utils.h","path":"aten/src/ATen/Utils.h","contentType":"file"},{"name":"Version.cpp","path":"aten/src/ATen/Version.cpp","contentType":"file"},{"name":"Version.h","path":"aten/src/ATen/Version.h","contentType":"file"},{"name":"VmapModeRegistrations.cpp","path":"aten/src/ATen/VmapModeRegistrations.cpp","contentType":"file"},{"name":"WrapDimUtils.h","path":"aten/src/ATen/WrapDimUtils.h","contentType":"file"},{"name":"WrapDimUtilsMulti.h","path":"aten/src/ATen/WrapDimUtilsMulti.h","contentType":"file"},{"name":"ZeroTensorFallback.cpp","path":"aten/src/ATen/ZeroTensorFallback.cpp","contentType":"file"},{"name":"autocast_mode.cpp","path":"aten/src/ATen/autocast_mode.cpp","contentType":"file"},{"name":"autocast_mode.h","path":"aten/src/ATen/autocast_mode.h","contentType":"file"},{"name":"ceil_div.h","path":"aten/src/ATen/ceil_div.h","contentType":"file"},{"name":"code_template.h","path":"aten/src/ATen/code_template.h","contentType":"file"},{"name":"cpp_custom_type_hack.h","path":"aten/src/ATen/cpp_custom_type_hack.h","contentType":"file"},{"name":"div_rtn.h","path":"aten/src/ATen/div_rtn.h","contentType":"file"},{"name":"dlpack.h","path":"aten/src/ATen/dlpack.h","contentType":"file"},{"name":"jit_macros.h","path":"aten/src/ATen/jit_macros.h","contentType":"file"},{"name":"jiterator_macros.h","path":"aten/src/ATen/jiterator_macros.h","contentType":"file"},{"name":"record_function.cpp","path":"aten/src/ATen/record_function.cpp","contentType":"file"},{"name":"record_function.h","path":"aten/src/ATen/record_function.h","contentType":"file"}],"totalCount":173},"aten/src":{"items":[{"name":"ATen","path":"aten/src/ATen","contentType":"directory"},{"name":"THC","path":"aten/src/THC","contentType":"directory"},{"name":"README.md","path":"aten/src/README.md","contentType":"file"}],"totalCount":3},"aten":{"items":[{"name":"conda","path":"aten/conda","contentType":"directory"},{"name":"src","path":"aten/src","contentType":"directory"},{"name":"tools","path":"aten/tools","contentType":"directory"},{"name":"CMakeLists.txt","path":"aten/CMakeLists.txt","contentType":"file"}],"totalCount":4},"":{"items":[{"name":".ci","path":".ci","contentType":"directory"},{"name":".circleci","path":".circleci","contentType":"directory"},{"name":".ctags.d","path":".ctags.d","contentType":"directory"},{"name":".devcontainer","path":".devcontainer","contentType":"directory"},{"name":".github","path":".github","contentType":"directory"},{"name":".vscode","path":".vscode","contentType":"directory"},{"name":"android","path":"android","contentType":"directory"},{"name":"aten","path":"aten","contentType":"directory"},{"name":"benchmarks","path":"benchmarks","contentType":"directory"},{"name":"binaries","path":"binaries","contentType":"directory"},{"name":"c10","path":"c10","contentType":"directory"},{"name":"caffe2","path":"caffe2","contentType":"directory"},{"name":"cmake","path":"cmake","contentType":"directory"},{"name":"docs","path":"docs","contentType":"directory"},{"name":"functorch","path":"functorch","contentType":"directory"},{"name":"mypy_plugins","path":"mypy_plugins","contentType":"directory"},{"name":"scripts","path":"scripts","contentType":"directory"},{"name":"test","path":"test","contentType":"directory"},{"name":"third_party","path":"third_party","contentType":"directory"},{"name":"tools","path":"tools","contentType":"directory"},{"name":"torch","path":"torch","contentType":"directory"},{"name":"torchgen","path":"torchgen","contentType":"directory"},{"name":".bazelignore","path":".bazelignore","contentType":"file"},{"name":".bazelrc","path":".bazelrc","contentType":"file"},{"name":".bazelversion","path":".bazelversion","contentType":"file"},{"name":".bc-linter.yml","path":".bc-linter.yml","contentType":"file"},{"name":".clang-format","path":".clang-format","contentType":"file"},{"name":".clang-tidy","path":".clang-tidy","contentType":"file"},{"name":".cmakelintrc","path":".cmakelintrc","contentType":"file"},{"name":".coveragerc","path":".coveragerc","contentType":"file"},{"name":".dockerignore","path":".dockerignore","contentType":"symlink_file"},{"name":".editorconfig","path":".editorconfig","contentType":"file"},{"name":".flake8","path":".flake8","contentType":"file"},{"name":".gdbinit","path":".gdbinit","contentType":"file"},{"name":".git-blame-ignore-revs","path":".git-blame-ignore-revs","contentType":"file"},{"name":".gitattributes","path":".gitattributes","contentType":"file"},{"name":".gitignore","path":".gitignore","contentType":"file"},{"name":".gitmodules","path":".gitmodules","contentType":"file"},{"name":".lintrunner.toml","path":".lintrunner.toml","contentType":"file"},{"name":".lldbinit","path":".lldbinit","contentType":"file"},{"name":"AGENTS.md","path":"AGENTS.md","contentType":"file"},{"name":"BUCK.oss","path":"BUCK.oss","contentType":"file"},{"name":"BUILD.bazel","path":"BUILD.bazel","contentType":"file"},{"name":"CITATION.cff","path":"CITATION.cff","contentType":"file"},{"name":"CLAUDE.md","path":"CLAUDE.md","contentType":"file"},{"name":"CMakeLists.txt","path":"CMakeLists.txt","contentType":"file"},{"name":"CODEOWNERS","path":"CODEOWNERS","contentType":"file"},{"name":"CODE_OF_CONDUCT.md","path":"CODE_OF_CONDUCT.md","contentType":"file"},{"name":"CONTRIBUTING.md","path":"CONTRIBUTING.md","contentType":"file"},{"name":"Dockerfile","path":"Dockerfile","contentType":"file"},{"name":"GLOSSARY.md","path":"GLOSSARY.md","contentType":"file"},{"name":"LICENSE","path":"LICENSE","contentType":"file"},{"name":"MANIFEST.in","path":"MANIFEST.in","contentType":"file"},{"name":"Makefile","path":"Makefile","contentType":"file"},{"name":"NOTICE","path":"NOTICE","contentType":"file"},{"name":"README.md","path":"README.md","contentType":"file"},{"name":"RELEASE.md","path":"RELEASE.md","contentType":"file"},{"name":"SECURITY.md","path":"SECURITY.md","contentType":"file"},{"name":"WORKSPACE","path":"WORKSPACE","contentType":"file"},{"name":"aten.bzl","path":"aten.bzl","contentType":"file"},{"name":"buckbuild.bzl","path":"buckbuild.bzl","contentType":"file"},{"name":"build.bzl","path":"build.bzl","contentType":"file"},{"name":"build_variables.bzl","path":"build_variables.bzl","contentType":"file"},{"name":"codex_setup.sh","path":"codex_setup.sh","contentType":"file"},{"name":"defs.bzl","path":"defs.bzl","contentType":"file"},{"name":"docker.Makefile","path":"docker.Makefile","contentType":"file"},{"name":"mypy-strict.ini","path":"mypy-strict.ini","contentType":"file"},{"name":"mypy.ini","path":"mypy.ini","contentType":"file"},{"name":"pt_ops.bzl","path":"pt_ops.bzl","contentType":"file"},{"name":"pt_template_srcs.bzl","path":"pt_template_srcs.bzl","contentType":"file"},{"name":"pyproject.toml","path":"pyproject.toml","contentType":"file"},{"name":"pyrefly.toml","path":"pyrefly.toml","contentType":"file"},{"name":"pytest.ini","path":"pytest.ini","contentType":"file"},{"name":"requirements-build.txt","path":"requirements-build.txt","contentType":"file"},{"name":"requirements.txt","path":"requirements.txt","contentType":"file"},{"name":"setup.py","path":"setup.py","contentType":"file"},{"name":"ubsan.supp","path":"ubsan.supp","contentType":"file"},{"name":"ufunc_defs.bzl","path":"ufunc_defs.bzl","contentType":"file"},{"name":"version.txt","path":"version.txt","contentType":"file"}],"totalCount":79}},"fileTreeProcessingTime":144.992647,"foldersToFetch":[],"incompleteFileTree":false,"repo":{"id":65600975,"defaultBranch":"main","name":"pytorch","ownerLogin":"pytorch","currentUserCanPush":false,"isFork":false,"isEmpty":false,"createdAt":"2016-08-13T05:26:41.000Z","ownerAvatar":"https://avatars.githubusercontent.com/u/21003710?v=4","public":true,"private":false,"isOrgOwned":true},"codeLineWrapEnabled":false,"symbolsExpanded":false,"treeExpanded":true,"refInfo":{"name":"v2.9.1","listCacheKey":"v0:1767654791.0","canEdit":false,"refType":"tag","currentOid":"d38164a545b4a4e4e0cf73ce67173f70574890b6","canEditOnDefaultBranch":false,"fileExistsOnDefault":true},"path":"aten/src/ATen/native/native_functions.yaml","currentUser":null,"blob":{"rawLines":["# See README.md in this directory for more guidance","","# *********NB: _cast_* operators are DEPRECATED and will be removed","# eventually. These were previously used before TorchScript IR supported","# representing ScalarType's. They are now superseded by usage of","# `aten::to()`. The ops remain here for backward compatibility purposes.","","# DEPRECATED. DO NOT USE","- func: _cast_Byte(Tensor self, bool non_blocking=False) -\u003e Tensor","  variants: function","","# DEPRECATED. DO NOT USE","- func: _cast_Char(Tensor self, bool non_blocking=False) -\u003e Tensor","  variants: function","","# DEPRECATED. DO NOT USE","- func: _cast_Double(Tensor self, bool non_blocking=False) -\u003e Tensor","  variants: function","","# DEPRECATED. DO NOT USE","- func: _cast_Float(Tensor self, bool non_blocking=False) -\u003e Tensor","  variants: function","","# DEPRECATED. DO NOT USE","- func: _cast_Int(Tensor self, bool non_blocking=False) -\u003e Tensor","  variants: function","","# DEPRECATED. DO NOT USE","- func: _cast_Long(Tensor self, bool non_blocking=False) -\u003e Tensor","  variants: function","","# DEPRECATED. DO NOT USE","- func: _cast_Short(Tensor self, bool non_blocking=False) -\u003e Tensor","  variants: function","","# DEPRECATED. DO NOT USE","- func: _cast_Half(Tensor self, bool non_blocking=False) -\u003e Tensor","  variants: function","","# Computes the gradient of current tensor w.r.t. graph leaves.","- func: _backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -\u003e ()","  manual_cpp_binding: True","  variants: method","","# DEPRECATED. Sets the tensor data held by this `Variable` to be the same as","# `new_data`.  It requires that `new_data` and `Variable` have compatible tensor","# type, by checking `_has_compatible_shallow_copy_type(this, new_data)`.","#","# This function is deprecated because it doesn't really make sense in a world","# where Variables *are* Tensors (as opposed to them containing tensors, which","# is what the previous interpretation was.)","- func: set_data(Tensor(a!) self, Tensor new_data) -\u003e ()","  manual_cpp_binding: True","  variants: method","","- func: data(Tensor self) -\u003e Tensor","  manual_cpp_binding: True","  variants: method","","# True if this `Variable` is a leaf and thus does not have a `grad_fn`.","- func: is_leaf(Tensor self) -\u003e bool","  manual_cpp_binding: True","  variants: method","","# Returns the output index of this variable from the forward operation that","# produced it.  Conversely, it returns the input index of the gradient `Node` to","# which this `Variable` is connected (because in the gradient computation,","# inputs and outputs switch meaning).  For example:","#","#   y0, y1, y2 = f(x)","#   assert y0.output_nr == 0","#   assert y1.output_nr == 1","#   assert y2.output_nr == 2","#","- func: output_nr(Tensor self) -\u003e int","  manual_cpp_binding: True","  variants: method","","- func: _version(Tensor self) -\u003e int","  manual_cpp_binding: True","  variants: method","","- func: requires_grad_(Tensor(a!) self, bool requires_grad=True) -\u003e Tensor(a!)","  manual_cpp_binding: True","  variants: method","","# Enables .grad attribute for non-leaf Tensors.","- func: retain_grad(Tensor(a!) self) -\u003e ()","  manual_cpp_binding: True","  variants: method","","- func: retains_grad(Tensor self) -\u003e bool","  manual_cpp_binding: True","  variants: method","","- func: _fw_primal(Tensor(a) self, int level) -\u003e Tensor(a)","  variants: method","  dispatch:","    CompositeExplicitAutograd: _fw_primal","","- func: _make_dual(Tensor(a) primal, Tensor tangent, int level) -\u003e Tensor(a)","  variants: function","  dispatch:","    CompositeExplicitAutograd: _make_dual","","- func: _unpack_dual(Tensor(a) dual, int level) -\u003e (Tensor(a) primal, Tensor tangent)","  variants: function","","# NOTE: [_new_zeros_with_same_feature_meta]","# This function creates a new tensor with the layout and TensorOptions","# of `other` but also takes into account the batch dimensions of `self`","#","# This function has a couple extra constraints because it is also used for `jvp`","# in functorch.","# - is used for forward AD because there is the restriction","#   that the primal and tangent must have the same layout","# - We cannot assume that `self` and `other` have the same sizes or even dim","#   because in the inplace over view case, `other` is the base tensor, and","#   `self` is the forward grad with respect to the view, which can have an","#   entirely different shape","# - takes the number of batch dims for `self` because we also handle","#   some batching logic. We handle that here instead of a batching rule because","#   we'd like to avoid calling as_strided in the batching rule (as to enable","#   nested vmap in functorch).","# - needs to be CompositeExplicitAutograd for jvp support in functorch.","#   functorch currently relies on TensorWrapper which does not have storage","#   CompositeExplicitAutograd makes sure the TensorWrapper is unwrapped.","# - this function may eventually take on another int argument to store the","#   the number of batch dims for other once we support that use case","- func: _new_zeros_with_same_feature_meta(Tensor self, Tensor other, *, int self_num_batch_dims=0) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: _new_zeros_with_same_feature_meta","  autogen: _new_zeros_with_same_feature_meta.out","","# This function compares the storage numel of self with that of other, where","# storage numel is computed as: `other.storage().nbytes() / other.itemsize()`.","# We create this function for composite compliance purposes. The batching rule","# always returns true because vmapped as_strided does not support accessing","# storage locations not indexable by the input tensor.","# See the note above for more information.","- func: _has_same_storage_numel(Tensor self, Tensor other) -\u003e bool","  variants: function","  dispatch:","    CompositeExplicitAutograd: _has_same_storage_numel","","- func: rename_(Tensor(a!) self, Dimname[]? names) -\u003e Tensor(a!)","  variants: method","  tags: inplace_view","","- func: rename(Tensor(a) self, Dimname[]? names) -\u003e Tensor(a)","  variants: method","","- func: align_to(Tensor(a) self, Dimname[] names) -\u003e Tensor(a)","  variants: method","","- func: align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -\u003e Tensor(a)","  variants: method","","- func: align_as(Tensor self, Tensor other) -\u003e Tensor","  variants: method","","- func: align_tensors(Tensor[] tensors) -\u003e Tensor[]","","# Not assert because it's a keyword; not Assert because FX already","# took that syntax","# TODO: need to specify this is side-effectful somehow","- func: _assert_async(Tensor self) -\u003e ()","  dispatch:","    CPU: _assert_async_cpu","    CUDA: _assert_async_cuda","","- func: _assert_async.msg(Tensor self, str assert_msg) -\u003e ()","  dispatch:","    CPU: _assert_async_msg_cpu","    CUDA: _assert_async_msg_cuda","","- func: _assert_scalar(Scalar self, str assert_msg) -\u003e ()","  dispatch:","    CompositeExplicitAutograd: _assert_scalar","","- func: _functional_assert_scalar(Scalar self, str assert_msg, Tensor dep_token) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _functional_assert_scalar","","- func: _functional_assert_async.msg(Tensor self, str assert_msg, Tensor dep_token) -\u003e Tensor","  dispatch:","    CPU: _functional_assert_async_msg_cpu","","- func: _assert_tensor_metadata(Tensor a, SymInt[]? size=None, SymInt[]? stride=None, ScalarType? dtype=None, *, Device? device=None, Layout? layout=None) -\u003e ()","  dispatch:","    CompositeExplicitAutograd: _assert_tensor_metadata","    Meta: _assert_tensor_metadata_meta_symint","","- func: _print(str s) -\u003e ()","  dispatch:","    CompositeExplicitAutograd: _print","","- func: sym_constrain_range(Scalar size, *, int? min=None, int? max=None) -\u003e ()","  dispatch:","    CompositeExplicitAutograd: sym_constrain_range","","- func: sym_constrain_range_for_size(Scalar size, *, int? min=None, int? max=None) -\u003e ()","  dispatch:","    CompositeExplicitAutograd: sym_constrain_range_for_size","","- func: _functional_sym_constrain_range(Scalar size, int? min, int? max, Tensor dep_token) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _functional_sym_constrain_range","","- func: _functional_sym_constrain_range_for_size(Scalar size, int? min, int? max, Tensor dep_token) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _functional_sym_constrain_range_for_size","","- func: _make_dep_token(*, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  dispatch:","    CPU: _make_dep_token_cpu","","- func: refine_names(Tensor(a) self, Dimname[] names) -\u003e Tensor(a)","  variants: method","","- func: _use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank) -\u003e bool","  device_check: NoCheck  # Tensor arguments allowed to be on different devices, see also _cudnn_ctc_loss","  dispatch:","    CUDA: _use_cudnn_ctc_loss","","- func: _use_cudnn_ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank) -\u003e bool","  device_check: NoCheck  # Tensor arguments allowed to be on different devices, see also _cudnn_ctc_loss","  dispatch:","    CUDA: _use_cudnn_ctc_loss_tensor","","- func: _cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank, bool deterministic, bool zero_infinity) -\u003e (Tensor, Tensor)","  device_check: NoCheck  # log_probs is expected to be on CUDA while targets is expected to be on CPU","  dispatch:","    CUDA: _cudnn_ctc_loss","  autogen: _cudnn_ctc_loss.out","","- func: _cudnn_ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank, bool deterministic, bool zero_infinity) -\u003e (Tensor, Tensor)","  device_check: NoCheck  # log_probs is expected to be on CUDA while targets is expected to be on CPU","  dispatch:","    CUDA: _cudnn_ctc_loss_tensor","","- func: _use_cudnn_rnn_flatten_weight() -\u003e bool","","- func: _cudnn_rnn_flatten_weight(Tensor[] weight_arr, int weight_stride0, SymInt input_size, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, bool bidirectional) -\u003e Tensor","  dispatch:","    CUDA: _cudnn_rnn_flatten_weight","  autogen: _cudnn_rnn_flatten_weight.out","","- func: _cudnn_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor? weight_buf, Tensor hx, Tensor? cx, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state) -\u003e (Tensor, Tensor, Tensor, Tensor, Tensor)","  # rnn_tanh may or may not redispatch to _cudnn_rnn based on algorithm and build. Thus it might hit dispatch or kernel device check.","  # Disable dispatch time device check for consistent behavior.","  device_check: NoCheck","  dispatch:","    CUDA: _cudnn_rnn","  autogen: _cudnn_rnn.out","  tags: nondeterministic_seeded","","- func: _cudnn_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, SymInt hidden_size, SymInt proj_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, SymInt[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -\u003e (Tensor, Tensor, Tensor, Tensor[])","  dispatch:","    CUDA: _cudnn_rnn_backward","  autogen: _cudnn_rnn_backward.out","","- func: _cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","  dispatch:","    CUDA: _cudnn_init_dropout_state","  autogen: _cudnn_init_dropout_state.out","  tags: nondeterministic_seeded","","- func: _debug_has_internal_overlap(Tensor self) -\u003e int","  variants: function","","- func: _fused_dropout(Tensor self, float p, Generator? generator=None) -\u003e (Tensor, Tensor)","  variants: function","  dispatch:","    CUDA: fused_dropout_cuda","  tags: nondeterministic_seeded","  autogen: _fused_dropout.out","","- func: _masked_scale(Tensor self, Tensor mask, float scale) -\u003e Tensor","  variants: function","  dispatch:","    CUDA: masked_scale_cuda","  autogen: _masked_scale.out","","- func: native_dropout(Tensor input, float p, bool? train) -\u003e (Tensor, Tensor)","  variants: function","  dispatch:","    CPU: native_dropout_cpu","    CUDA: native_dropout_cuda","    MPS: native_dropout_mps","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: native_dropout_nested","  tags: [nondeterministic_seeded, core]","  autogen: native_dropout.out","","- func: native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -\u003e Tensor","  dispatch:","    CPU, NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: native_dropout_backward","    CUDA: native_dropout_backward_cuda","    MPS: native_dropout_backward_mps","  autogen: native_dropout_backward.out","  tags: pointwise","","- func: _sobol_engine_draw(Tensor quasi, int n, Tensor sobolstate, int dimension, int num_generated, ScalarType? dtype) -\u003e (Tensor, Tensor)","","- func: _sobol_engine_ff_(Tensor(a!) self, int n, Tensor sobolstate, int dimension, int num_generated) -\u003e Tensor(a!)","","- func: _sobol_engine_scramble_(Tensor(a!) self, Tensor ltm, int dimension) -\u003e Tensor(a!)","","- func: _sobol_engine_initialize_state_(Tensor(a!) self, int dimension) -\u003e Tensor(a!)","","- func: _reshape_from_tensor(Tensor self, Tensor shape) -\u003e Tensor","","- func: _shape_as_tensor(Tensor self) -\u003e Tensor","","- func: dropout(Tensor input, float p, bool train) -\u003e Tensor","  tags: [nondeterministic_seeded, maybe_aliasing_or_mutating]","","- func: dropout_(Tensor(a!) self, float p, bool train) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","","- func: feature_dropout(Tensor input, float p, bool train) -\u003e Tensor","  tags: [nondeterministic_seeded, maybe_aliasing_or_mutating]","","- func: feature_dropout_(Tensor(a!) self, float p, bool train) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","","- func: alpha_dropout(Tensor input, float p, bool train) -\u003e Tensor","  tags: [nondeterministic_seeded, maybe_aliasing_or_mutating]","","- func: alpha_dropout_(Tensor(a!) self, float p, bool train) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","","- func: feature_alpha_dropout(Tensor input, float p, bool train) -\u003e Tensor","  tags: [nondeterministic_seeded, maybe_aliasing_or_mutating]","","- func: feature_alpha_dropout_(Tensor(a!) self, float p, bool train) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","","- func: abs(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: abs","    SparseCPU, SparseCUDA, SparseMPS: abs_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: abs_sparse_csr","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_abs","  tags: [core, pointwise]","","- func: abs_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: abs_","    SparseCPU, SparseCUDA, SparseMPS: abs_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: abs_sparse_csr_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_abs_","","- func: abs.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MPS, MTIA: abs_out","    SparseCPU, SparseCUDA, SparseMPS: abs_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: abs_sparse_csr_out","  tags: pointwise","","# Note [Adding an alias]","# To add an alias do the following:","#","# 1) Copy the original functions native_functions.yaml entry, but replace the","#      original function's name with their own and delete any dispatch","#      keys for the aliases. Specifying a dispatch key will prevent","#      autograd from recording the operations the alias performs, which","#      will stop it from \"inheriting\" the original operation's autograd behavior.","# 2) Implement the corresponding functions and have them redispatch to the","#      original function.","# 3) Add docstrings to the new function that reference the original function,","#      and document the method as usual (if it exists.)","#    (See torch/_torch_docs.py and docs/source/torch.rst if adding a function,","#     torch/_tensor_docs.py and docs/source/tensors.rst if adding a method,","#     or module-specific doc bindings (like torch/linalg/__init__.py) if","#     adding an alias in a namespace.)","# 4) Update torch/overrides.py consistent with the original function.","# 5) Update the alias_map in torch/csrc/jit/passes/normalize_ops.cpp.","# 6) Add aliases argument to existing OpInfo/UnaryUfuncInfo or create new OpInfo/UnaryUfuncInfo entry","# in op_db list in torch/testing/_internal/common_methods_invocations.py","#","# See torch.absolute, an alias for torch.abs, as an example.","# Absolute, alias for abs","","- func: absolute(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: absolute_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: absolute.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: angle(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CPU, CUDA: angle","    MPS: angle_mps","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: angle_sparse_csr","  tags: pointwise","","- func: angle.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: angle_out","    MPS: angle_out_mps","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: angle_sparse_csr_out","  tags: pointwise","","- func: view_as_real(Tensor(a) self) -\u003e Tensor(a)","  variants: function","  dispatch:","    CPU, CUDA, MPS, Meta: view_as_real","","- func: view_as_complex(Tensor(a) self) -\u003e Tensor(a)","  variants: function","  dispatch:","    CPU, CUDA, MPS, Meta: view_as_complex","","- func: sgn(Tensor self) -\u003e Tensor","  variants: function, method","  structured_delegate: sgn.out","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: sgn_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sgn_sparse_csr","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_sgn","  tags: pointwise","","- func: sgn_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: method","  structured_delegate: sgn.out","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: sgn_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sgn_sparse_csr_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_sgn_","  tags: pointwise","","- func: sgn.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: sgn_out","    MPS: sgn_out_mps","    SparseCPU, SparseCUDA, SparseMPS: sgn_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sgn_sparse_csr_out","  tags: pointwise","","- func: chalf(Tensor self, *, MemoryFormat? memory_format=None) -\u003e Tensor","  variants: method","","- func: real(Tensor(a) self) -\u003e Tensor(a)","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: imag(Tensor(a) self) -\u003e Tensor(a)","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: _conj(Tensor(a) self) -\u003e Tensor(a)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: _conj","","- func: conj(Tensor(a) self) -\u003e Tensor(a)","  variants: function, method","  manual_cpp_binding: True","","- func: _conj_physical(Tensor self) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: _conj_physical","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: conj_physical_sparse_csr","  autogen: _conj_physical.out","","- func: conj_physical(Tensor self) -\u003e Tensor","  variants: function, method","  tags: [pointwise, maybe_aliasing_or_mutating]","","- func: conj_physical.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: conj_physical_out","    MPS: conj_physical_out_mps","    SparseCPU, SparseCUDA, SparseMPS: conj_physical_out_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: conj_physical_sparse_csr_out","  tags: pointwise","","- func: conj_physical_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: conj_physical_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: conj_physical_sparse_csr_","  tags: pointwise","","- func: resolve_conj(Tensor(a) self) -\u003e Tensor(a)","  variants: function, method","","- func: resolve_neg(Tensor(a) self) -\u003e Tensor(a)","  variants: function, method","","- func: _neg_view(Tensor(a) self) -\u003e Tensor(a)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: _neg_view","","- func: acos(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: acos.out","  tags: [core, pointwise]","","- func: acos_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: acos.out","  tags: pointwise","","- func: acos.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: acos_out","  tags: pointwise","","# arccos, alias of acos","- func: arccos(Tensor self) -\u003e Tensor","  variants: function, method","","- func: arccos_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","","- func: arccos.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -\u003e Tensor","  tags: core","  autogen: avg_pool1d.out","","- func: adaptive_avg_pool1d(Tensor self, int[1] output_size) -\u003e Tensor","  tags: core","  autogen: adaptive_avg_pool1d.out","","# Return: (Tensor output, Tensor indices)","- func: adaptive_max_pool1d(Tensor self, int[1] output_size) -\u003e (Tensor, Tensor)","","- func: add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: add.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: add_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: add_sparse_csr","    MkldnnCPU: mkldnn_add","    ZeroTensor: add_zerotensor","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_add_Tensor","  tags: [core, pointwise]","","- func: add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: add.out","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: add_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: add_sparse_csr_","    MkldnnCPU: mkldnn_add_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_add__Tensor","  tags: pointwise","","- func: add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  ufunc_inner_loop:","    Generic: add (AllAndComplex, BFloat16, Half, ComplexHalf)","    ScalarOnly: add (Bool)","  dispatch:","    SparseCPU, SparseMeta: add_out_sparse_cpu","    SparseCUDA: add_out_sparse_cuda","    SparseMPS: add_out_sparse_mps","    SparseCsrCPU, SparseCsrMeta: add_out_sparse_compressed_cpu","    SparseCsrCUDA: add_out_sparse_compressed_cuda","    MkldnnCPU: mkldnn_add_out","    MPS: add_out_mps","    MTIA: add_out_mtia","  tags: pointwise","","- func: _add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -\u003e Tensor","  variants: function","  dispatch:","    CPU: add_relu","","- func: _add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CPU: add_relu_","","- func: _add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CPU: add_relu_out","","- func: _add_relu.Scalar(Tensor self, Scalar other, Scalar alpha=1) -\u003e Tensor","  variants: function","  dispatch:","    CPU: add_relu","","- func: _add_relu_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CPU: add_relu_","  autogen: _add_relu.Scalar_out","","# For C++ only, until we have conversion from C++ numbers to Tensor","- func: add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: add","  tags: [core, pointwise]","","- func: add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: add_","  autogen: add.Scalar_out","  tags: pointwise","","- func: addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor","  structured_delegate: addmv.out","  variants: function, method","","- func: addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor(a!)","  structured_delegate: addmv.out","  variants: function, method","","- func: addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: addmv_out_cpu","    CUDA: addmv_out_cuda","    MPS: addmv_out_mps","    XPU: addmv_out_xpu","    SparseCsrCPU: addmv_out_sparse_compressed","    SparseCsrCUDA: addmv_out_sparse_compressed_cuda","","- func: addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU, CUDA: addr","    MPS: addr_mps","    CompositeExplicitAutograd: math_addr","","- func: addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor(a!)","  variants: method","  dispatch:","    CompositeExplicitAutograd: addr_","","- func: addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: addr_out","    MPS: addr_out_mps","    CompositeExplicitAutograd: math_addr_out","","- func: affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: affine_grid_generator","  autogen: affine_grid_generator.out","","- func: affine_grid_generator_backward(Tensor grad, SymInt[] size, bool align_corners) -\u003e Tensor","  variants: function","","- func: _is_all_true(Tensor self) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: _is_all_true","","- func: _is_any_true(Tensor self) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: _is_any_true","","# Note: this function is only for testing.","- func: _test_check_tensor(Tensor self) -\u003e Tensor","  variants: function","","# Note; this function is only for testing","- func: _test_functorch_fallback(Tensor self, Tensor other) -\u003e Tensor","  variants: function","  dispatch:","    CPU: _test_functorch_fallback","  autogen: _test_functorch_fallback.out","","- func: all.dim(Tensor self, int dim, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: all.out","  variants: function, method","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_all","","","- func: all.dims(Tensor self, int[]? dim=None, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: all.dims_out","  variants: function, method","  cpp_no_default_args: ['dim']","  dispatch:","    CompositeExplicitAutograd: all_dims_default","","- func: all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  dispatch:","    CPU, CUDA: all_out","    MPS: all_out_mps","    MTIA: all_out_mtia","","- func: all.dims_out(Tensor self, int[]? dim=None, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  dispatch:","    CPU, CUDA: all_dims_out","    CompositeExplicitAutograd: all_dims_out_default","  cpp_no_default_args: ['dim']","","- func: all.dimname(Tensor self, Dimname dim, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: all.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -\u003e bool","  variants: function, method","  tags: data_dependent_output","  dispatch:","    CompositeExplicitAutograd: allclose","","- func: any.dim(Tensor self, int dim, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: any.out","  variants: function, method","  tags: core","","- func: any.dims(Tensor self, int[]? dim=None, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: any.dims_out","  variants: function, method","  cpp_no_default_args: ['dim']","  tags: core","  dispatch:","    CompositeExplicitAutograd: any_dims_default","","- func: any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  dispatch:","    CPU, CUDA: any_out","    MPS: any_out_mps","","- func: any.dims_out(Tensor self, int[]? dim=None, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  dispatch:","    CPU, CUDA: any_dims_out","    CompositeExplicitAutograd: any_dims_out_default","  cpp_no_default_args: ['dim']","","- func: any.dimname(Tensor self, Dimname dim, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: any.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: arange","","- func: arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: arange","","# This operator should be named `arange.start_out` if following the naming convention. However that","# name is already taken. Disabled because of CI job failures.","# FIXME: enable this","#- func: arange.start_out_(Scalar start, Scalar end, *, Tensor(a!) out) -\u003e Tensor(a!)","#  dispatch:","#    CompositeExplicitAutograd: arange_start_out","","- func: arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: arange","  cpp_no_default_args: ['step']","  tags: core","","- func: arange.out(Scalar end, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: arange_out","","- func: arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, Meta: arange_out","    CUDA: arange_cuda_out","    MPS: arange_mps_out","    MTIA: arange_mtia_out","  cpp_no_default_args: ['step']","","# This function is a temporary hack to allow tracing of arange like constructs with dynamic","# bounds on arange.  Normal arange is not traceable because it does not take any tensor inputs;","# if the range you need is based on another tensor, calling this function directly will","# preserve tracing.  Get rid of this when arange can directly take tensors for bounds","# (so that it can be traced directly).","- func: _dim_arange(Tensor like, int dim) -\u003e Tensor","","- func: argmax(Tensor self, int? dim=None, bool keepdim=False) -\u003e Tensor","  structured_delegate: argmax.out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  tags: core","","- func: argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU, CUDA: argmax_out","    MPS: argmax_out_mps","","- func: argmin(Tensor self, int? dim=None, bool keepdim=False) -\u003e Tensor","  structured_delegate: argmin.out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  tags: core","","- func: argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU, CUDA: argmin_out","    MPS: argmin_out_mps","","- func: acosh(Tensor self) -\u003e Tensor","  variants: function, method","  structured_delegate: acosh.out","  tags: [core, pointwise]","","- func: acosh_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","  structured_delegate: acosh.out","  tags: pointwise","","- func: acosh.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: acosh_out","    MPS: acosh_out_mps","  tags: pointwise","# arccosh, alias for acosh","","- func: arccosh(Tensor self) -\u003e Tensor","  variants: function, method","","- func: arccosh_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","","- func: arccosh.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: asinh(Tensor self) -\u003e Tensor","  variants: function, method","  structured_delegate: asinh.out","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: asinh_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: asinh_sparse_csr","  tags: [core, pointwise]","","- func: asinh_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","  structured_delegate: asinh.out","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: asinh_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: asinh_sparse_csr_","  tags: pointwise","","- func: asinh.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: asinh_out","    MPS: asinh_out_mps","    SparseCPU, SparseCUDA, SparseMPS: asinh_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: asinh_sparse_csr_out","  tags: pointwise","","# arcsinh, alias for asinh","- func: arcsinh(Tensor self) -\u003e Tensor","  variants: function, method","","- func: arcsinh_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","","- func: arcsinh.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: atanh(Tensor self) -\u003e Tensor","  structured_delegate: atanh.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: atanh_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: atanh_sparse_csr","  tags: [core, pointwise]","","- func: atanh_(Tensor(a!) self) -\u003e Tensor(a!)","  structured_delegate: atanh.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: atanh_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: atanh_sparse_csr_","  tags: pointwise","","- func: atanh.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: atanh_out","    MPS: atanh_out_mps","    SparseCPU, SparseCUDA, SparseMPS: atanh_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: atanh_sparse_csr_out","  tags: pointwise","# arctanh, alias for atanh","","- func: arctanh(Tensor self) -\u003e Tensor","  variants: function, method","","- func: arctanh_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","","- func: arctanh.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -\u003e Tensor(a)","  variants: function, method","  dispatch:","    ZeroTensor, CPU, CUDA, MTIA, MPS: as_strided_tensorimpl","    Meta: as_strided_tensorimpl_meta_symint","    QuantizedCPU, QuantizedCUDA: as_strided_qtensorimpl","  device_check: NoCheck","  device_guard: False","  tags: core","","- func: as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -\u003e Tensor(a!)","  use_const_ref_for_mutable_tensors: True","  variants: function, method","  device_check: NoCheck","  device_guard: False","  tags: inplace_view","  dispatch:","    CompositeExplicitAutogradNonFunctional: as_strided__symint","","- func: asin(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: asin.out","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: asin_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: asin_sparse_csr","  tags: [core, pointwise]","","- func: asin_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: asin.out","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: asin_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: asin_sparse_csr_","  tags: pointwise","","- func: asin.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: asin_out","    SparseCPU, SparseCUDA, SparseMPS: asin_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: asin_sparse_csr_out","  tags: pointwise","","# arcsin, alias of asin","- func: arcsin(Tensor self) -\u003e Tensor","  variants: function, method","","- func: arcsin_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","","- func: arcsin.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: atan(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: atan.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: atan_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: atan_sparse_csr","  tags: [core, pointwise]","","- func: atan_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: atan.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: atan_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: atan_sparse_csr_","  tags: pointwise","","- func: atan.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: atan_out","    SparseCPU, SparseCUDA, SparseMPS: atan_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: atan_sparse_csr_out","  tags: pointwise","","# arctan, alias of atan","- func: arctan(Tensor self) -\u003e Tensor","  variants: function, method","","- func: arctan_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","","- func: arctan.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: atleast_1d(Tensor self) -\u003e Tensor","  variants: function","  tags: maybe_aliasing_or_mutating","","- func: atleast_1d.Sequence(Tensor[] tensors) -\u003e Tensor[]","","- func: atleast_2d(Tensor self) -\u003e Tensor","  variants: function","  tags: maybe_aliasing_or_mutating","","- func: atleast_2d.Sequence(Tensor[] tensors) -\u003e Tensor[]","  variants: function","","- func: atleast_3d(Tensor self) -\u003e Tensor","  variants: function","  tags: maybe_aliasing_or_mutating","","- func: atleast_3d.Sequence(Tensor[] tensors) -\u003e Tensor[]","  variants: function","","- func: baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor","  variants: function, method","  structured_delegate: baddbmm.out","","- func: baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor(a!)","  variants: method","  structured_delegate: baddbmm.out","","- func: baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  dispatch:","    CPU: baddbmm_out_cpu","    CUDA: baddbmm_out_cuda","    MPS: baddbmm_out_mps","    XPU: baddbmm_out_xpu","    MTIA: baddbmm_out_mtia","    SparseCsrCUDA: baddbmm_out_sparse_csr_cuda","","- func: baddbmm.dtype(Tensor self, Tensor batch1, Tensor batch2, ScalarType out_dtype, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor","  variants: function","  dispatch:","    CUDA: _baddbmm_dtype_cuda","","- func: baddbmm.dtype_out(Tensor self, Tensor batch1, Tensor batch2, ScalarType out_dtype, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CUDA: _baddbmm_out_dtype_cuda","","- func: bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: bartlett_window","  autogen: bartlett_window.out","","- func: bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: bartlett_window","  autogen: bartlett_window.periodic_out","","- func: batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -\u003e Tensor","  tags: maybe_aliasing_or_mutating","","- func: quantized_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor var, float eps, float output_scale, int output_zero_point) -\u003e Tensor","  dispatch:","    QuantizedCPU: quantized_batch_norm","  autogen: quantized_batch_norm.out","","- func: _batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -\u003e (Tensor, Tensor, Tensor, Tensor, int)","  tags: maybe_aliasing_or_mutating","","- func: _batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -\u003e (Tensor, Tensor, Tensor)","","# Sample bernoulli with values in `self` as probability.","- func: bernoulli(Tensor self, *, Generator? generator=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: bernoulli","  tags: nondeterministic_seeded","","- func: bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","  tags: nondeterministic_seeded","  dispatch:","    CPU, CUDA: bernoulli_out","    MPS: bernoulli_out_mps","","- func: bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  tags: nondeterministic_seeded","  dispatch:","    CPU, CUDA: bernoulli_","    MPS: bernoulli_mps_","  autogen: bernoulli.Tensor, bernoulli.Tensor_out","","- func: bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  tags: nondeterministic_seeded","  dispatch:","    CPU, CUDA: bernoulli_","    MPS: bernoulli_mps_","  autogen: bernoulli.float_out","","# Note [bernoulli.p schema]","# We should probably just fix the overload ambiguity by appending a _functional to the C++ API name (BC breaking)","# This out-of-place version isn't used explicitly, but needed by jit.","# There is no default valid on `p` here because it would introduce ambiguity","# with `bernoulli(Tensor self, *, Generator? generator=None)` declaration.","- func: bernoulli.p(Tensor self, float p, *, Generator? generator=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutogradNonFunctional: bernoulli","","- func: bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias=None) -\u003e Tensor","","- func: binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: nn","  variants: function","  dispatch:","    CPU: binary_cross_entropy_cpu","    CUDA: binary_cross_entropy_cuda","    MPS: binary_cross_entropy_mps","","- func: binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: nn","  variants: function","  dispatch:","    CPU: binary_cross_entropy_out_cpu","    CUDA: binary_cross_entropy_out_cuda","    MPS: binary_cross_entropy_out_mps","","- func: binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -\u003e Tensor","  python_module: nn","  variants: function","  dispatch:","    CPU: binary_cross_entropy_backward_cpu","    CUDA: binary_cross_entropy_backward_cuda","    MPS: binary_cross_entropy_backward_mps","","- func: binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  variants: function","  dispatch:","    CPU: binary_cross_entropy_backward_out_cpu","    CUDA: binary_cross_entropy_backward_out_cuda","    MPS: binary_cross_entropy_backward_out_mps","","- func: binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: binary_cross_entropy_with_logits","  autogen: binary_cross_entropy_with_logits.out","","- func: bincount(Tensor self, Tensor? weights=None, SymInt minlength=0) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU: _bincount_cpu","    CUDA: _bincount_cuda","    MPS: _bincount_mps","  tags: dynamic_output_shape","  autogen: bincount.out","","- func: bitwise_not(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: bitwise_not.out","  variants: function, method","  tags: [core, pointwise]","","- func: bitwise_not_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: bitwise_not.out","  variants: method","  tags: pointwise","","- func: bitwise_not.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: bitwise_not_out","  tags: pointwise","","- func: copysign.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: copysign_out","  tags: pointwise","","- func: copysign.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: copysign.out","  tags: pointwise","","- func: copysign_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: copysign.out","","- func: copysign.Scalar(Tensor self, Scalar other) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: copysign","  tags: pointwise","","- func: copysign_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  variants: method","  dispatch:","    CompositeExplicitAutograd: copysign_","","- func: copysign.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: copysign_out","  tags: pointwise","","- func: _lazy_clone(Tensor self) -\u003e Tensor","  # Like clone, but the copy takes place lazily, only if either the","  # input or the output are written.","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: _lazy_clone","","- func: logical_not(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: logical_not","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_logical_not","  tags: [core, pointwise]","","- func: logical_not_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: logical_not_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_logical_not_","  tags: pointwise","","- func: logical_not.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: logical_not_out","    MPS: logical_not_out_mps","  tags: pointwise","","- func: logical_xor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: logical_xor","  tags: [core, pointwise]","","- func: logical_xor_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: logical_xor_","  tags: pointwise","","- func: logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: logical_xor_out","    MPS: logical_xor_out_mps","  tags: pointwise","","- func: logical_and(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: logical_and","  tags: [core, pointwise]","","- func: logical_and_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: logical_and_","  tags: pointwise","","- func: logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: logical_and_out","    MPS: logical_and_out_mps","  tags: pointwise","","- func: logical_or(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: logical_or","  tags: [core, pointwise]","","- func: logical_or_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: logical_or_","  tags: pointwise","","- func: logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: logical_or_out","    MPS: logical_or_out_mps","  tags: pointwise","","- func: blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: blackman_window","  autogen: blackman_window.out","","- func: blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: blackman_window","  autogen: blackman_window.periodic_out","","- func: bmm(Tensor self, Tensor mat2) -\u003e Tensor","  structured_delegate: bmm.out","  variants: function, method","  dispatch:","    SparseCPU: bmm_sparse_cpu","    SparseCUDA: bmm_sparse_cuda","    NestedTensorCPU: bmm_nested","    NestedTensorCUDA: bmm_nested_cuda","  tags: core","","- func: bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  dispatch:","    CPU: bmm_out_cpu","    CUDA: bmm_out_cuda","    MPS: bmm_out_mps","    XPU: bmm_out_xpu","    MTIA: bmm_out_mtia","    SparseCPU: bmm_out_sparse_cpu","    SparseCUDA: bmm_out_sparse_cuda","    SparseCsrCUDA: bmm_out_sparse_csr_cuda","","- func: bmm.dtype(Tensor self, Tensor mat2, ScalarType out_dtype) -\u003e Tensor","  variants: function","  dispatch:","    CUDA: _bmm_dtype_cuda","","- func: bmm.dtype_out(Tensor self, Tensor mat2, ScalarType out_dtype, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CUDA: _bmm_out_dtype_cuda","","- func: broadcast_tensors(Tensor[] tensors) -\u003e Tensor[]","  device_check: NoCheck","  device_guard: False","","- func: broadcast_to(Tensor(a) self, SymInt[] size) -\u003e Tensor(a)","  variants: function, method","  dispatch:","    CompositeImplicitAutograd: broadcast_to_symint","","- func: _sparse_broadcast_to(Tensor(a) self, int[] size) -\u003e Tensor(a)","  variants: function","  dispatch:","    SparseCPU, SparseCUDA: sparse_broadcast_to","","- func: cat(Tensor[] tensors, int dim=0) -\u003e Tensor","  structured_delegate: cat.out","  dispatch:","    SparseCPU, SparseCUDA: cat_sparse","    QuantizedCPU: cat_quantized_cpu","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: cat_nested","  tags: core","","- func: cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  precomputed:","  - dim -\u003e int dim, int valid, bool all_contiguous, bool all_same_dtype, bool all_same_sizes_and_stride, MemoryFormat memory_format","  dispatch:","    CPU: cat_out_cpu","    CUDA: cat_out_cuda","    MPS: cat_out_mps","    QuantizedCPU: cat_out_quantized_cpu","","- func: cat.names(Tensor[] tensors, Dimname dim) -\u003e Tensor","","- func: cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -\u003e Tensor(a!)","","# alias for torch.cat","- func: concat(Tensor[] tensors, int dim=0) -\u003e Tensor","","- func: concat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: concat.names(Tensor[] tensors, Dimname dim) -\u003e Tensor","","- func: concat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -\u003e Tensor(a!)","","# alias for torch.cat","- func: concatenate(Tensor[] tensors, int dim=0) -\u003e Tensor","","- func: concatenate.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: concatenate.names(Tensor[] tensors, Dimname dim) -\u003e Tensor","","- func: concatenate.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: block_diag(Tensor[] tensors) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: block_diag","  autogen: block_diag.out","","- func: ceil(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: ceil.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: ceil_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: ceil_sparse_csr","  tags: [core, pointwise]","","- func: ceil_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: ceil.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: ceil_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: ceil_sparse_csr_","  tags: pointwise","","- func: ceil.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: ceil_out","    SparseCPU, SparseCUDA, SparseMPS: ceil_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: ceil_sparse_csr_out","  tags: pointwise","","# alias for torch.linalg.multi_dot","- func: chain_matmul(Tensor[] matrices) -\u003e Tensor","  variants: function","","# alias for torch.linalg.multi_dot","- func: chain_matmul.out(Tensor[] matrices, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: unsafe_chunk(Tensor self, int chunks, int dim=0) -\u003e Tensor[]","  variants: function, method","  device_check: NoCheck","  device_guard: False","  tags: maybe_aliasing_or_mutating","","- func: chunk(Tensor(a -\u003e *) self, int chunks, int dim=0) -\u003e Tensor(a)[]","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: chunk","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: chunk_nested_tensor","","- func: tensor_split.sections(Tensor(a -\u003e *) self, SymInt sections, int dim=0) -\u003e Tensor(a)[]","  variants: function, method","  dispatch:","    CompositeImplicitAutograd: tensor_split_sections_symint","","- func: tensor_split.indices(Tensor(a -\u003e *) self, SymInt[] indices, int dim=0) -\u003e Tensor(a)[]","  variants: function, method","  dispatch:","    CompositeImplicitAutograd: tensor_split_indices_symint","","- func: tensor_split.tensor_indices_or_sections(Tensor(a -\u003e *) self, Tensor tensor_indices_or_sections, int dim=0) -\u003e Tensor(a)[]","  variants: function, method","","- func: clamp(Tensor self, Scalar? min=None, Scalar? max=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  cpp_no_default_args: ['min']","  structured_delegate: clamp.out","  dispatch:","    QuantizedCPU: clamp_quantized_cpu","  tags: [core, pointwise]","","- func: clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -\u003e Tensor","  variants: function, method","  structured_delegate: clamp.Tensor_out","  tags: [core, pointwise]","","- func: clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  cpp_no_default_args: ['min']","  structured_delegate: clamp.out","  tags: pointwise","","- func: clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -\u003e Tensor(a!)","  variants: function, method","  structured_delegate: clamp.Tensor_out","  tags: pointwise","","- func: clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  cpp_no_default_args: ['min']","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MTIA: clamp_out","    MPS: clamp_out_mps","  tags: pointwise","","- func: clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: clamp_Tensor_out","    MPS: clamp_Tensor_out_mps","  tags: pointwise","","- func: clamp_max(Tensor self, Scalar max) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: clamp_max.out","  tags: pointwise","","- func: clamp_max.Tensor(Tensor self, Tensor max) -\u003e Tensor","  variants: function, method","  structured_delegate: clamp_max.Tensor_out","  tags: pointwise","","- func: clamp_max_(Tensor(a!) self, Scalar max) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: clamp_max.out","  tags: pointwise","","- func: clamp_max_.Tensor(Tensor(a!) self, Tensor max) -\u003e Tensor(a!)","  variants: function, method","  structured_delegate: clamp_max.Tensor_out","  tags: pointwise","","- func: clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MTIA: clamp_max_out","    MPS: clamp_max_out_mps","  tags: pointwise","","- func: clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: clamp_max_Tensor_out","    MPS: clamp_max_Tensor_out_mps","  tags: pointwise","","- func: clamp_min(Tensor self, Scalar min) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: clamp_min.out","  tags: pointwise","","- func: clamp_min.Tensor(Tensor self, Tensor min) -\u003e Tensor","  variants: function, method","  structured_delegate: clamp_min.Tensor_out","  tags: pointwise","","- func: clamp_min_(Tensor(a!) self, Scalar min) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: clamp_min.out","  tags: pointwise","","- func: clamp_min_.Tensor(Tensor(a!) self, Tensor min) -\u003e Tensor(a!)","  variants: function, method","  structured_delegate: clamp_min.Tensor_out","  tags: pointwise","","- func: clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MTIA: clamp_min_out","    MPS: clamp_min_out_mps","  tags: pointwise","","- func: clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: clamp_min_Tensor_out","    MPS: clamp_min_Tensor_out_mps","  tags: pointwise","","# clip is an alias for clamp","- func: clip(Tensor self, Scalar? min=None, Scalar? max=None) -\u003e Tensor","  cpp_no_default_args: ['min']","  variants: function, method","  tags: pointwise","","- func: clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -\u003e Tensor","  variants: function, method","  tags: pointwise","","- func: clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -\u003e Tensor(a!)","  cpp_no_default_args: ['min']","  variants: function, method","  tags: pointwise","","- func: clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -\u003e Tensor(a!)","  variants: function, method","  tags: pointwise","","- func: clip.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  cpp_no_default_args: ['min']","  tags: pointwise","","- func: clip.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: cudnn_is_acceptable(Tensor self) -\u003e bool","  device_check: NoCheck","  device_guard: False","","- func: complex(Tensor real, Tensor imag) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: complex","","- func: complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: complex_out","","- func: polar(Tensor abs, Tensor angle) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: polar","","- func: polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: polar_out","","- func: constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: constant_pad_nd","    MPS: constant_pad_nd_mps","  autogen: constant_pad_nd.out","  tags: core","","- func: contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -\u003e Tensor(a)","  variants: method","  manual_cpp_binding: True","","- func: convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: convolution","  autogen: convolution.out","  tags: core","","- func: convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CompositeExplicitAutograd, CUDA: convolution_backward","  autogen: convolution_backward.out","  tags: core","","- func: convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: convolution_overrideable","  autogen: convolution_overrideable.out","","- func: convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -\u003e (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)","  dispatch:","    CompositeExplicitAutograd: convolution_backward_overrideable","  autogen: convolution_backward_overrideable.out","","- func: _convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _convolution","  autogen: _convolution.out","","- func: _convolution.deprecated(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, int[] output_padding, SymInt groups, bool benchmark, bool deterministic, bool cudnn_enabled) -\u003e Tensor","","- func: _convolution_mode(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, str padding, SymInt[] dilation, SymInt groups) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: _convolution_mode_symint","","- func: _convolution_double_backward(Tensor? ggI, Tensor? ggW, Tensor? ggb, Tensor gO, Tensor weight, Tensor self, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -\u003e (Tensor, Tensor, Tensor)","","- func: conv1d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, SymInt[1] padding=0, SymInt[1] dilation=1, SymInt groups=1) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: conv1d_symint","","- func: conv2d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1, SymInt groups=1) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: conv2d_symint","","- func: conv3d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] dilation=1, SymInt groups=1) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: conv3d_symint","","- func: conv1d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, str padding=\"valid\", SymInt[1] dilation=1, SymInt groups=1) -\u003e Tensor","  cpp_no_default_args: ['bias', 'stride', 'padding']","  dispatch:","    CompositeImplicitAutograd: conv1d_padding_symint","","- func: conv2d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[2] stride=1, str padding=\"valid\", SymInt[2] dilation=1, SymInt groups=1) -\u003e Tensor","  cpp_no_default_args: ['bias', 'stride', 'padding']","  dispatch:","    CompositeImplicitAutograd: conv2d_padding_symint","","- func: conv3d.padding(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, str padding=\"valid\", SymInt[3] dilation=1, SymInt groups=1) -\u003e Tensor","  cpp_no_default_args: ['bias', 'stride', 'padding']","  dispatch:","    CompositeImplicitAutograd: conv3d_padding_symint","","- func: conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: conv_tbc","  autogen: conv_tbc.out","","- func: conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -\u003e (Tensor, Tensor, Tensor)","","# NB: we inherit the goofy argument order from PyTorch torch.nn.functional","- func: conv_transpose1d(Tensor input, Tensor weight, Tensor? bias=None, SymInt[1] stride=1, SymInt[1] padding=0, SymInt[1] output_padding=0, SymInt groups=1, SymInt[1] dilation=1) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: conv_transpose1d_symint","","- func: conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt groups=1, SymInt[2] dilation=1) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: conv_transpose2d_symint","","- func: conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, SymInt groups=1, SymInt[3] dilation=1) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: conv_transpose3d_symint","","- func: copy(Tensor self, Tensor src, bool non_blocking=False) -\u003e Tensor","  variants: function","  dispatch:","    Meta: copy_meta","    CompositeExplicitAutogradNonFunctional: copy","  tags: core","","- func: copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    MkldnnCPU: copy_mkldnn_","    SparseCPU, SparseCUDA: copy_sparse_wrapper_","    CompositeExplicitAutograd: copy_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: copy_sparse_compressed_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: copy_nested_","  autogen: copy.out","","- func: _copy_from(Tensor self, Tensor dst, bool non_blocking=False) -\u003e Tensor","  dispatch:","    MPS: _copy_from_mps","  autogen: _copy_from.out","","# We need this to be able to properly copy from a CPU to an XLA tensor with different sizes.","# See https://github.com/pytorch/xla/issues/2881","- func: _copy_from_and_resize(Tensor self, Tensor dst) -\u003e Tensor","  dispatch:","    MPS: _copy_from_and_resize_mps","  autogen: _copy_from_and_resize.out","","- func: cos(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: cos.out","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_cos","  tags: [core, pointwise]","","- func: cos_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: cos.out","  tags: pointwise","","- func: cos.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: cos_out","  tags: pointwise","","- func: cosh(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: cosh.out","  tags: [core, pointwise]","","- func: cosh_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: cosh.out","  tags: pointwise","","- func: cosh.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: cosh_out","  tags: pointwise","","- func: cosine_embedding_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -\u003e Tensor","","- func: count_nonzero.dim_IntList(Tensor self, int[] dim) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU: count_nonzero_cpu","    CUDA: count_nonzero_cuda","    MPS: count_nonzero_mps","  autogen: count_nonzero.dim_IntList_out","","- func: count_nonzero(Tensor self, int? dim=None) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: count_nonzero","  autogen: count_nonzero.out","","- func: cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -\u003e Tensor","  variants: function, method","","- func: corrcoef(Tensor self) -\u003e Tensor","  variants: function, method","","- func: cudnn_affine_grid_generator(Tensor theta, int N, int C, int H, int W) -\u003e Tensor grid","  dispatch:","    CUDA: cudnn_affine_grid_generator_forward","  autogen: cudnn_affine_grid_generator.out","","# TODO: Why do I have to call this grad?!","- func: cudnn_affine_grid_generator_backward(Tensor grad, int N, int C, int H, int W) -\u003e Tensor grad_theta","  dispatch:","    CUDA: cudnn_affine_grid_generator_backward","  autogen: cudnn_affine_grid_generator_backward.out","","- func: cudnn_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -\u003e (Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CUDA: cudnn_batch_norm","","- func: cudnn_batch_norm.out(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon, *, Tensor(a!) out0, Tensor(b!) out1, Tensor(c!) out2, Tensor(d!) out3) -\u003e (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))","  dispatch:","    CUDA: cudnn_batch_norm_out","","# NB: You can only use this if you used cudnn_batch_norm training=True","- func: cudnn_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon, Tensor reserveSpace) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CUDA: cudnn_batch_norm_backward","  autogen: cudnn_batch_norm_backward.out","","- func: cudnn_convolution(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) -\u003e Tensor","  dispatch:","    CUDA: cudnn_convolution","","- func: cudnn_convolution.out(Tensor self, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CUDA: cudnn_convolution_out","","- func: cudnn_convolution_transpose(Tensor self, Tensor weight, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic, bool allow_tf32) -\u003e Tensor","  dispatch:","    CUDA: cudnn_convolution_transpose","  autogen: cudnn_convolution_transpose.out","","- func: _mps_convolution_transpose(Tensor self, Tensor weight, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups) -\u003e Tensor","  dispatch:","    MPS: _mps_convolution_transpose","  autogen: _mps_convolution_transpose.out","","- func: mps_convolution_transpose_backward(Tensor self, Tensor grad_output, Tensor weight, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool[2] output_mask) -\u003e (Tensor, Tensor)","  dispatch:","    MPS: mps_convolution_transpose_backward","  autogen: mps_convolution_transpose_backward.out","","- func: cudnn_convolution_relu(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -\u003e Tensor","  dispatch:","    CUDA: cudnn_convolution_relu","  autogen: cudnn_convolution_relu.out","","- func: cudnn_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -\u003e Tensor","  dispatch:","    CUDA: cudnn_convolution_add_relu","  autogen: cudnn_convolution_add_relu.out","","# NB: input is special cased in a way I don't quite understand","- func: cudnn_grid_sampler(Tensor self, Tensor grid) -\u003e Tensor output","  dispatch:","    CUDA: cudnn_grid_sampler_forward","  autogen: cudnn_grid_sampler.out","","- func: cudnn_grid_sampler_backward(Tensor self, Tensor grid, Tensor grad_output) -\u003e (Tensor grad_self, Tensor grad_grid)","  dispatch:","    CUDA: cudnn_grid_sampler_backward","  autogen: cudnn_grid_sampler_backward.out","","- func: cummax(Tensor self, int dim) -\u003e (Tensor values, Tensor indices)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: cummax","","- func: cummax.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CompositeExplicitAutograd: cummax_out","","- func: cummax.dimname(Tensor self, Dimname dim) -\u003e (Tensor values, Tensor indices)","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: cummax.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  device_check: NoCheck   # TensorIterator","","- func: _cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -\u003e ()","  variants: function","  dispatch:","    CPU: cummax_helper_cpu","    CUDA: cummax_helper_cuda","    MPS: cummax_helper_mps","","- func: cummin(Tensor self, int dim) -\u003e (Tensor values, Tensor indices)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: cummin","","- func: cummin.out(Tensor self, int dim, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CompositeExplicitAutograd: cummin_out","","- func: cummin.dimname(Tensor self, Dimname dim) -\u003e (Tensor values, Tensor indices)","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: cummin.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  device_check: NoCheck   # TensorIterator","","- func: _cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -\u003e ()","  variants: function","  dispatch:","    CPU: cummin_helper_cpu","    CUDA: cummin_helper_cuda","    MPS: cummin_helper_mps","","- func: cummaxmin_backward(Tensor grad, Tensor input, Tensor indices, int dim) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","","- func: cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -\u003e Tensor","  structured_delegate: cumprod.out","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -\u003e Tensor(a!)","  structured_delegate: cumprod.out","  variants: method","","- func: cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: cumprod_out","    MPS: cumprod_out_mps","","- func: cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor(a!)","  variants: method","","- func: cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: cumprod_backward(Tensor grad, Tensor input, int dim, Tensor output) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","","- func: cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -\u003e Tensor","  structured_delegate: cumsum.out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  tags: core","","- func: cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -\u003e Tensor(a!)","  structured_delegate: cumsum.out","  variants: method","","- func: cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: cumsum_out","    MPS: cumsum_out_mps","","- func: cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor(a!)","  variants: method","","- func: cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: cumulative_trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -\u003e Tensor","","- func: cumulative_trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -\u003e Tensor","","- func: ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -\u003e Tensor","","# convenience function that converts to intlists for you","- func: ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -\u003e Tensor","","- func: _ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -\u003e (Tensor, Tensor)","  dispatch:","    CPU: ctc_loss_cpu","    CUDA: ctc_loss_gpu","    Meta: ctc_loss_meta","  autogen: _ctc_loss.out","  tags: dynamic_output_shape  # the shape of second output is data dependent","","- func: _ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, bool zero_infinity=False) -\u003e (Tensor, Tensor)","  dispatch:","    CPU, CUDA: ctc_loss_tensor","  autogen: _ctc_loss.Tensor_out","  tags: dynamic_output_shape  # the shape of second output is data dependent","","- func: _ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -\u003e Tensor","  dispatch:","    CPU: ctc_loss_backward_cpu","    CUDA: ctc_loss_backward_gpu","  autogen: _ctc_loss_backward.out","","- func: _ctc_loss_backward.Tensor(Tensor grad, Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -\u003e Tensor","  dispatch:","    CPU, CUDA: ctc_loss_backward_tensor","","- func: diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutogradNonFunctional: diag_embed","  autogen: diag_embed.out","","- func: diagflat(Tensor self, int offset=0) -\u003e Tensor","  variants: function, method","","- func: diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -\u003e Tensor(a)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: diagonal","  tags: core","","- func: linalg_diagonal(Tensor(a) A, *, int offset=0, int dim1=-2, int dim2=-1) -\u003e Tensor(a)","  python_module: linalg","  variants: function","","- func: diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -\u003e Tensor(a)","  variants: function, method","","- func: diagonal_backward(Tensor grad_output, SymInt[] input_sizes, int offset, int dim1, int dim2) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: diagonal_backward_symint","  autogen: diagonal_backward.out","","- func: fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -\u003e Tensor(a!)","  variants: method","","- func: diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -\u003e Tensor","  variants: function, method","","- func: diff.out(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","","- func: gradient.scalarint(Tensor self, *, Scalar? spacing=None, int? dim=None, int edge_order=1) -\u003e Tensor[]","  variants: function","","- func: gradient.scalararray(Tensor self, *, Scalar spacing, int[] dim, int edge_order=1) -\u003e Tensor[]","  variants: function","","- func: gradient.array(Tensor self, *, int[] dim, int edge_order=1) -\u003e Tensor[]","  variants: function","","- func: gradient.scalarrayint(Tensor self, *, Scalar[] spacing, int? dim=None, int edge_order=1) -\u003e Tensor[]","  variants: function","","- func: gradient.scalarrayarray(Tensor self, *, Scalar[] spacing, int[] dim, int edge_order=1) -\u003e Tensor[]","  variants: function","","- func: gradient.tensorarrayint(Tensor self, *, Tensor[] spacing, int? dim=None, int edge_order=1) -\u003e Tensor[]","  variants: function","","- func: gradient.tensorarray(Tensor self, *, Tensor[] spacing, int[] dim, int edge_order=1) -\u003e Tensor[]","  variants: function","","- func: div.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: div.out","  dispatch:","    SparseCPU, SparseCUDA: div_sparse","    ZeroTensor: div_zerotensor","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_div_Tensor","  tags: [core, pointwise]","","- func: div_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: div.out","  dispatch:","    SparseCPU, SparseCUDA: div_sparse_","  tags: pointwise","","- func: div.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: div_out","    SparseCPU, SparseCUDA: div_out_sparse_zerodim","  tags: pointwise","","- func: div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: div.out_mode","  dispatch:","    SparseCPU, SparseCUDA: div_sparse","  tags: [core, pointwise]","","- func: div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: div.out_mode","  dispatch:","    SparseCPU, SparseCUDA: div_sparse_","  tags: pointwise","","- func: div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: div_out_mode","    SparseCPU, SparseCUDA: div_out_sparse_zerodim","  tags: pointwise","","# For C++ only, until we have conversion from C++ numbers to Tensor","- func: div.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: div","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_div_Scalar","  tags: [core, pointwise]","","- func: div_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: div_","  autogen: div.Scalar_out","  tags: pointwise","","- func: div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: div","  tags: [core, pointwise]","","- func: div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -\u003e Tensor(a!)","  variants: method","  dispatch:","    CompositeExplicitAutograd: div_","  autogen: div.Scalar_mode_out","  tags: pointwise","","# divide, alias for div","- func: divide.Tensor(Tensor self, Tensor other) -\u003e Tensor","  variants: function, method","","- func: divide_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  variants: method","","- func: divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: divide.Scalar(Tensor self, Scalar other) -\u003e Tensor","  variants: function, method","","- func: divide_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  variants: method","","- func: divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -\u003e Tensor","  variants: function, method","","- func: divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -\u003e Tensor(a!)","  variants: method","","- func: divide.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -\u003e Tensor(a!)","","- func: divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -\u003e Tensor","  variants: function, method","","- func: divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -\u003e Tensor(a!)","  variants: method","","  # true_divide, an alias for div","- func: true_divide.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  tags: pointwise","","- func: true_divide_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: true_divide.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: true_divide_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: dot(Tensor self, Tensor tensor) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU: dot","    CUDA: dot_cuda","    MPS: dot_mps","","- func: dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: dot_out","","- func: vdot(Tensor self, Tensor other) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU: vdot","    CUDA: vdot_cuda","","- func: vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: vdot_out","","- func: einsum(str equation, Tensor[] tensors, *, int[]? path=None) -\u003e Tensor","","- func: embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: embedding_symint","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_embedding","  autogen: embedding.out","  tags: core","","- func: embedding_backward(Tensor grad, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq, bool sparse) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: embedding_backward_symint","","- func: embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -\u003e Tensor","  dispatch:","    CPU: embedding_dense_backward_cpu","    CUDA: embedding_dense_backward_cuda","    MPS: embedding_dense_backward_mps","  autogen: embedding_dense_backward.out","  tags: core","","- func: embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -\u003e Tensor(a!)","  dispatch:","    CPU: embedding_renorm_cpu_","    CUDA: embedding_renorm_cuda_","  autogen: embedding_renorm, embedding_renorm.out","","- func: embedding_sparse_backward(Tensor grad, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -\u003e Tensor","","# NOTE [ embedding_bag Native Functions ]","# The `_embedding_bag.*` variants assume that input tensors except for `weight`,","# e.g. `indices` and `offsets` (and `offset2bag`), are contiguous.","# We really only need to enforce this for `_embedding_bag` (the forward) because","# the backward inputs are the same as forward ones.","# The above `embedding_bag` wrapper is created to achieve this, e.g.,","# applying indices = indices.contiguous().","# The backward functions apply a check that these input tensors are contiguous.","","","- func: _embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -\u003e (Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CPU: _embedding_bag_forward_only_cpu","    CUDA: _embedding_bag_forward_only_cuda","  autogen: _embedding_bag_forward_only.out","","- func: _rowwise_prune(Tensor weight, Tensor mask, ScalarType compressed_indices_dtype) -\u003e (Tensor, Tensor)","","# row_stack is the alias of vstack","- func: row_stack(Tensor[] tensors) -\u003e Tensor","","- func: row_stack.out(Tensor[] tensors, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -\u003e (Tensor, Tensor, Tensor, Tensor)","","# To keep backward and forward compatibility, and to avoid ambiguity with the","# original signature above, scale_grad_by_freq, mode, sparse,","# per_sample_weights, and include_last_offset parameters do not have default","# values. Once the original signature is removed, default values can be added.","- func: embedding_bag.padding_idx(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int? padding_idx) -\u003e (Tensor, Tensor, Tensor, Tensor)","","- func: _embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -\u003e (Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CPU: _embedding_bag_cpu","    CUDA: _embedding_bag_cuda","  autogen: _embedding_bag.out","  tags: core","","- func: _embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights, int padding_idx=-1) -\u003e Tensor","  dispatch:","    CPU, CUDA: _embedding_bag_backward_symint","","- func: _embedding_bag_sparse_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: _embedding_bag_sparse_backward_symint","","- func: _embedding_bag_dense_backward(Tensor grad, Tensor indices, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, SymInt num_weights, bool scale_grad_by_freq, int mode, Tensor? per_sample_weights, int padding_idx=-1) -\u003e Tensor","  dispatch:","    CPU: _embedding_bag_dense_backward_cpu","    CUDA: _embedding_bag_dense_backward_cuda","  autogen: _embedding_bag_dense_backward.out","","- func: _embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -\u003e Tensor","  dispatch:","    CPU: _embedding_bag_per_sample_weights_backward_cpu","    CUDA: _embedding_bag_per_sample_weights_backward_cuda","  autogen: _embedding_bag_per_sample_weights_backward.out","","- func: empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: empty_names","  autogen: empty.names_out","","- func: empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  dispatch:","    CPU: empty_cpu","    CUDA: empty_cuda","    MPS: empty_mps","    Meta: empty_meta_symint","    MkldnnCPU: empty_mkldnn","    SparseCPU, SparseCUDA, SparseMPS: empty_sparse","    SparseMeta: empty_sparse_symint","    SparseCsrCPU, SparseCsrCUDA: empty_sparse_compressed","    SparseCsrMeta: empty_sparse_compressed_symint","    QuantizedCPU, QuantizedCUDA, QuantizedMeta: empty_unknown_quantized","  tags: core","","- func: empty_permuted(SymInt[] size, int[] physical_layout, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: empty_permuted_symint","  autogen: empty_permuted.out","","# We do not make new_empty a composite that calls into new_empty_strided, as the strided version","# is significantly more difficult to implement by different backends","- func: new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  variants: method","  dispatch:","    CompositeExplicitAutograd: new_empty_symint","  autogen: new_empty.out","","- func: new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  variants: method","  dispatch:","    CompositeExplicitAutogradNonFunctional: new_empty_strided_symint","  autogen: new_empty_strided.out","","- func: new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  variants: method","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd: new_full","  autogen: new_full.out","","- func: new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  variants: method","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd: new_zeros","  autogen: new_zeros.out","","- func: new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  variants: method","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd: new_ones","  autogen: new_ones.out","","# other overrides are to provide a more helpful error message that dtype is required","- func: _empty_affine_quantized(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -\u003e Tensor","  dispatch:","    CPU: empty_affine_quantized_other_backends_stub","    QuantizedCPU, QuantizedCUDA: empty_affine_quantized","  autogen: _empty_affine_quantized.out","","# it's a factory function receiving a tensor argument, thus overriding explicitly","# other overrides are to provide a more helpful error message that dtype is required","- func: _empty_per_channel_affine_quantized(SymInt[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -\u003e Tensor","  category_override: factory","  dispatch:","    CPU: empty_per_channel_affine_quantized_other_backends_stub","    QuantizedCPU, QuantizedCUDA: empty_per_channel_affine_quantized","  autogen: _empty_per_channel_affine_quantized.out","","- func: resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -\u003e Tensor(a!)","  use_const_ref_for_mutable_tensors: True","  variants: method","  device_check: NoCheck","  device_guard: False","  tags: [core, inplace_view]","  dispatch:","    Meta: resize__symint","    CPU: resize_","    CUDA: resize_cuda_","    MPS: resize_mps_","    QuantizedCPU: quantized_resize_cpu_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: resize_sparse_csr_","  autogen: resize, resize.out","","# This is a utility function to enable users to resize out tensor while registering kernels for out variants.","# Eventually, we can consider exposing `resize_output` as a public API to ship it with python op registration","# to make it easy to register out variants for ops.","- func: _resize_output_(Tensor(a!) self, SymInt[] size, Device device) -\u003e Tensor(a!)","  use_const_ref_for_mutable_tensors: True","  variants: function","  dispatch:","    Meta: _resize_output_","  autogen: _resize_output, _resize_output.out","","- func: empty_quantized(int[] size, Tensor qtensor, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  category_override: factory","  variants: function","  dispatch:","    QuantizedCPU, QuantizedCUDA: empty_quantized","  autogen: empty_quantized.out","","- func: empty.out(SymInt[] size, *, MemoryFormat? memory_format=None, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  device_guard: False","","- func: empty_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: empty_like","    QuantizedCPU, QuantizedCUDA: empty_like_quantized","    SparseCPU, SparseCUDA, SparseMeta: empty_like_sparse_coo","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: empty_like_sparse_csr","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: empty_like_nested","  autogen: empty_like.out","","- func: empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CPU: empty_strided_cpu","    CUDA: empty_strided_cuda","    MPS: empty_strided_mps","    Meta: empty_strided_meta_symint","    QuantizedCPU, QuantizedCUDA: empty_strided_unknown_quantized","  autogen: empty_strided.out","  tags: core","","- func: erf(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: erf.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: erf_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: erf_sparse_csr","  tags: [core, pointwise]","","- func: erf_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: erf.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: erf_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: erf_sparse_csr_","  tags: pointwise","","- func: erf.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: erf_out","    SparseCPU, SparseCUDA, SparseMPS: erf_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: erf_sparse_csr_out","  tags: pointwise","","- func: erfc(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: erfc.out","  variants: function, method","  tags: pointwise","","- func: erfc_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: erfc.out","  variants: function, method","  tags: pointwise","","- func: erfc.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: erfc_out","  tags: pointwise","","- func: exp(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: exp.out","  variants: function, method","  tags: [core, pointwise]","","- func: exp_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: exp.out","  variants: function, method","  tags: pointwise","","- func: exp.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: exp_out","  tags: pointwise","","- func: exp2(Tensor self) -\u003e Tensor","  structured_delegate: exp2.out","  variants: function, method","  tags: pointwise","","- func: exp2_(Tensor(a!) self) -\u003e Tensor(a!)","  structured_delegate: exp2.out","  variants: function, method","  tags: pointwise","","- func: exp2.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: exp2_out","  tags: pointwise","","- func: expm1(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: expm1.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: expm1_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: expm1_sparse_csr","  tags: [core, pointwise]","","- func: expm1_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: expm1.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: expm1_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: expm1_sparse_csr_","  tags: pointwise","","- func: expm1.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: expm1_out","    SparseCPU, SparseCUDA, SparseMPS: expm1_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: expm1_sparse_csr_out","  tags: pointwise","","- func: expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -\u003e Tensor(a)","  variants: method  # This is method-only to match the previous tensor API. In the future we could make this a function too.","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: expand","  tags: core","","- func: expand_as(Tensor(a) self, Tensor other) -\u003e Tensor(a)","  variants: method  # This is method-only to match the previous tensor API. In the future we could make this a function too.","  device_check: NoCheck","  device_guard: False","","# decomposes to eye.m","- func: eye(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: eye","","- func: eye.m(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: eye","","- func: eye.out(SymInt n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, Meta: eye_out_cpu","    CUDA: eye_out_cuda","    MPS: eye_out_mps","","- func: eye.m_out(SymInt n, SymInt m, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, Meta: eye_out_cpu","    CUDA: eye_out_cuda","    MPS: eye_out_mps","","- func: flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -\u003e Tensor(a)","  variants: function, method","","- func: flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -\u003e Tensor(a)","  variants: function, method","","- func: flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -\u003e Tensor(a)","  variants: function, method","","- func: flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -\u003e Tensor(a)","  variants: function, method","","- func: unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -\u003e Tensor(a)","  variants: function, method","  dispatch:","    CompositeImplicitAutograd: unflatten_symint","","- func: unflatten.Dimname(Tensor(a) self, Dimname dim, SymInt[] sizes, Dimname[] names) -\u003e Tensor(a)","  variants: function, method","  dispatch:","    CompositeImplicitAutograd: unflatten_dimname_symint","","- func: fill.Scalar(Tensor self, Scalar value) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: fill","  tags: core","","- func: fill.Tensor(Tensor self, Tensor value) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: fill","","- func: fill_.Scalar(Tensor(a!) self, Scalar value) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CPU, CUDA: fill_","    MPS: fill_scalar_mps","    QuantizedCPU, QuantizedCUDA: fill_quantized_","    Meta: fill_meta_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: fill_sparse_csr_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: fill_nested_","  autogen: fill.Scalar_out","","- func: fill_.Tensor(Tensor(a!) self, Tensor value) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CPU, CUDA: fill_","    MPS: fill_tensor_mps_","    QuantizedCPU, QuantizedCUDA: fill_quantized_","    Meta: fill_meta_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: fill_nested_","  autogen: fill.Tensor_out","","- func: floor(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: floor.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: floor_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: floor_sparse_csr","  tags: [core, pointwise]","","- func: floor_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: floor.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: floor_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: floor_sparse_csr_","  tags: pointwise","","- func: floor.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: floor_out","    SparseCPU, SparseCUDA, SparseMPS: floor_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: floor_sparse_csr_out","  tags: pointwise","","- func: floor_divide(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CPU, CUDA, MPS, MTIA: floor_divide","    SparseCPU, SparseCUDA: floor_divide_sparse","","- func: floor_divide_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CPU, CUDA, MPS: floor_divide_","    SparseCPU, SparseCUDA: floor_divide_sparse_","","- func: floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MPS: floor_divide_out","    SparseCPU, SparseCUDA: floor_divide_out_sparse_zerodim","","- func: floor_divide.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: floor_divide","","- func: floor_divide_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: floor_divide_","  autogen: floor_divide.Scalar_out","","- func: frac(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: frac.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: frac_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: frac_sparse_csr","  tags: pointwise","","- func: frac_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: frac.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: frac_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: frac_sparse_csr_","  tags: pointwise","","- func: frac.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: frac_out","    MPS: frac_out_mps","    SparseCPU, SparseCUDA, SparseMPS: frac_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: frac_sparse_csr_out","  tags: pointwise","","- func: full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: full","  autogen: full.names_out","","- func: full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: full","  tags: core","","- func: full.out(SymInt[] size, Scalar fill_value, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: full_out","","- func: full_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd: full_like","  autogen: full_like.out","  tags: core","","- func: from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CPU: from_file","  autogen: from_file.out","","- func: gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: gcd_out","  tags: pointwise","","- func: gcd(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: gcd.out","  variants: function, method","  tags: pointwise","","- func: gcd_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: gcd.out","  variants: function, method","","- func: lcm.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: lcm_out","  tags: pointwise","","- func: lcm(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: lcm.out","  variants: function, method","  tags: pointwise","","- func: lcm_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: lcm.out","  variants: function, method","","# NOTE [ grid_sampler Native Functions ]","# `grid_sampler` is _supposed to_ do all the shape checking and then dispatch to","# one of `cudnn_grid_sampler`, `grid_sampler_2d`, or `grid_sampler_3d`, each of","# which has the corresponding backward defined as native functions as well.","# However, we do shape checking everywhere for now since each of the mentioned","# functions can be called directly, which will lead to crashes otherwise.","# See https://github.com/pytorch/pytorch/issues/73187 for more information.","#","# There is also _grid_sampler_2d_backward_cpu_fallback which is an","# implementation detail of grid_sampler_2d and is only exposed here for testing","# purposes.","#","# Additionally, arguments `padding_mode` and `interpolation_mode` are cast to","# enums defined in `native/GridSampler.h`. `cudnn_grid_sampler` doesn't take in","# `interpolation_mode` because it only supports Bilinear interpolation mode.","# Nor does it take in `align_corners` because it only supports the mode","# `align_corners = True`.","- func: grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -\u003e Tensor","","- func: grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -\u003e Tensor","  dispatch:","    CPU, QuantizedCPU: grid_sampler_2d_cpu","    CUDA: grid_sampler_2d_cuda","    MPS: grid_sampler_2d_mps","  autogen: grid_sampler_2d.out","  tags: core","","# `grid_sampler_2d_backward` takes in `output_mask` to optimize performance for","# the case where `input` doesn't require gradient. Gradient for `grid` is always","# computed (only `output_mask[0]` is checked by the implementations).","- func: grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -\u003e (Tensor, Tensor)","  dispatch:","    CPU: grid_sampler_2d_backward_cpu","    CUDA: grid_sampler_2d_backward_cuda","  autogen: grid_sampler_2d_backward.out","","# See NOTE [ grid_sample CPU fallback ]","- func: _grid_sampler_2d_cpu_fallback(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _grid_sampler_2d_cpu_fallback","  autogen: _grid_sampler_2d_cpu_fallback.out","","- func: _grid_sampler_2d_cpu_fallback_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -\u003e (Tensor, Tensor)","","- func: grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -\u003e Tensor","  dispatch:","    CPU: grid_sampler_3d_cpu","    CUDA: grid_sampler_3d_cuda","    MPS: grid_sampler_3d_mps","  autogen: grid_sampler_3d.out","","# `grid_sampler_3d_backward` takes in `output_mask` to optimize performance for","# the case where `input` doesn't require gradient. Gradient for `grid` is always","# computed (only `output_mask[0]` is checked by the implementations).","- func: grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -\u003e (Tensor, Tensor)","  dispatch:","    CPU: grid_sampler_3d_backward_cpu","    CUDA: grid_sampler_3d_backward_cuda","  autogen: grid_sampler_3d_backward.out","","- func: hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: hann_window","  autogen: hann_window.out","","- func: hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: hann_window","  autogen: hann_window.periodic_out","","- func: hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: hamming_window","  autogen: hamming_window.out","","- func: hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: hamming_window","  autogen: hamming_window.periodic_out","","- func: hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: hamming_window","  autogen: hamming_window.periodic_alpha_out","","- func: hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: hamming_window","  autogen: hamming_window.periodic_alpha_beta_out","","- func: kaiser_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: kaiser_window","  autogen: kaiser_window.out","","- func: kaiser_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: kaiser_window","  autogen: kaiser_window.periodic_out","","- func: kaiser_window.beta(int window_length, bool periodic, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: kaiser_window","  autogen: kaiser_window.beta_out","","- func: hinge_embedding_loss(Tensor self, Tensor target, float margin=1.0, int reduction=Mean) -\u003e Tensor","","- func: group_norm(Tensor input, int num_groups, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enabled=True) -\u003e Tensor","","- func: native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CPU, CUDA: native_group_norm","    CompositeExplicitAutograd: math_group_norm","  autogen: native_group_norm.out","  tags: core","","- func: native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CPU, CUDA: native_group_norm_backward","  autogen: native_group_norm_backward.out","  tags: core","","# Real to complex forward FFT","- func: _fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -\u003e Tensor","  variants: function","  dispatch:","    CPU: _fft_r2c_mkl","    CUDA: _fft_r2c_cufft","    MPS: _fft_r2c_mps","  tags: core","","- func: _fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CPU: _fft_r2c_mkl_out","    CUDA: _fft_r2c_cufft_out","    MPS: _fft_r2c_mps_out","","# Complex to real inverse FFT","- func: _fft_c2r(Tensor self, int[] dim, int normalization, SymInt last_dim_size) -\u003e Tensor","  variants: function","  dispatch:","    CPU: _fft_c2r_mkl","    CUDA: _fft_c2r_cufft","    MPS: _fft_c2r_mps","","- func: _fft_c2r.out(Tensor self, int[] dim, int normalization, SymInt last_dim_size, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CPU: _fft_c2r_mkl_out","    CUDA: _fft_c2r_cufft_out","    MPS: _fft_c2r_mps_out","","# Standard complex to complex FFT (forward or backward)","- func: _fft_c2c(Tensor self, SymInt[] dim, int normalization, bool forward) -\u003e Tensor","  variants: function","  dispatch:","    CPU: _fft_c2c_mkl","    CUDA: _fft_c2c_cufft","    MPS: _fft_c2c_mps","","- func: _fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CPU: _fft_c2c_mkl_out","    CUDA: _fft_c2c_cufft_out","    MPS: _fft_c2c_mps_out","","- func: _validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -\u003e ()","  device_check: NoCheck","  variants: function","  dispatch:","    CPU: _validate_compressed_sparse_indices_cpu","    CUDA: _validate_compressed_sparse_indices_cuda","","- func: _cufft_get_plan_cache_size(DeviceIndex device_index) -\u003e int","","- func: _cufft_get_plan_cache_max_size(DeviceIndex device_index) -\u003e int","","- func: _cufft_set_plan_cache_max_size(DeviceIndex device_index, int max_size) -\u003e ()","","- func: _cufft_clear_plan_cache(DeviceIndex device_index) -\u003e ()","","- func: index.Tensor(Tensor self, Tensor?[] indices) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: index.Tensor_out","  variants: function, method","  dispatch:","    QuantizedCPU: quantized_index","  tags: [core, dynamic_output_shape]","  # NB: This function is special-cased in tools/autograd/gen_variable_type.py","  # NB: The following functions are declared in aten/src/ATen/templates/TensorBody.h and defined in aten/src/ATen/TensorIndexing.cpp:","  # - Tensor Tensor::index(ArrayRef\u003cTensorIndex\u003e indices)","  # - Tensor Tensor::index(std::initializer_list\u003cTensorIndex\u003e indices)","","- func: index.Tensor_out(Tensor self, Tensor?[] indices, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  structured: True","  structured_inherits: TensorIteratorBase","  precomputed:","  - indices -\u003e DimVector sizes, DimVector strides","  dispatch:","    CPU, CUDA, MPS: index_out","","# Used by inductor to signal indexing without bounds checks","# Note that we don't support boolean indexing, to avoid dynamic output shapes","- func: _unsafe_index.Tensor(Tensor self, Tensor?[] indices) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: _unsafe_index","","# Used by inductor to generate masked loads","# Note that we don't support boolean indexing, to avoid dynamic output shapes","- func: _unsafe_masked_index(Tensor self, Tensor mask, Tensor?[] indices, Scalar fill) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: _unsafe_masked_index","","- func: _unsafe_masked_index_put_accumulate(Tensor self, Tensor mask, Tensor?[] indices, Tensor values) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: _unsafe_masked_index_put_accumulate","","- func: index_copy.out(Tensor self, int dim, Tensor index, Tensor source, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  precomputed:","  - dim -\u003e int dim","  dispatch:","    CPU, CUDA: index_copy_out","    MPS: index_copy_out_mps","","- func: index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -\u003e Tensor(a!)","  variants: method","  structured_delegate: index_copy.out","","- func: index_copy(Tensor self, int dim, Tensor index, Tensor source) -\u003e Tensor","  variants: function, method","  structured_delegate: index_copy.out","","- func: index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -\u003e Tensor(a!)","  variants: method","","- func: index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -\u003e Tensor","  variants: function, method","","- func: index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -\u003e Tensor(a!)","  device_check: NoCheck   # delegate to _index_put_impl_, which leverages TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: index_put_","  autogen: index_put.out","  # NB: The following functions are declared in aten/src/ATen/templates/TensorBody.h and defined in aten/src/ATen/TensorIndexing.cpp:","  # - Tensor \u0026 Tensor::index_put_(ArrayRef\u003cTensorIndex\u003e indices, Tensor const \u0026 rhs)","  # - Tensor \u0026 Tensor::index_put_(ArrayRef\u003cTensorIndex\u003e indices, Scalar v)","  # - Tensor \u0026 Tensor::index_put_(std::initializer_list\u003cTensorIndex\u003e indices, Tensor const \u0026 rhs)","  # - Tensor \u0026 Tensor::index_put_(std::initializer_list\u003cTensorIndex\u003e indices, Scalar v)","","- func: index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -\u003e Tensor","  device_check: NoCheck   # delegate to _index_put_impl_ after clone, which leverages TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: index_put","  tags: core","","- func: _unsafe_index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -\u003e Tensor","  device_check: NoCheck   # delegate to _index_put_impl_ after clone, which leverages TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: _unsafe_index_put","","- func: _index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CPU, CUDA, MPS: _index_put_impl_","    QuantizedCPU: _index_put_impl_quantized_cpu_","    QuantizedCUDA: _index_put_impl_quantized_cuda_","  autogen: _index_put_impl, _index_put_impl.out","","- func: instance_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool use_input_stats, float momentum, float eps, bool cudnn_enabled) -\u003e Tensor","  variants: function","","- func: isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -\u003e Tensor","  variants: function, method","","- func: isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  structured: True","  dispatch:","    CPU, CUDA: isin_Tensor_Tensor_out","    MPS: isin_Tensor_Tensor_out_mps","","- func: isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -\u003e Tensor","  variants: function","  structured_delegate: isin.Tensor_Tensor_out","","- func: isin.Tensor_Scalar_out(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  structured: True","  dispatch:","    CPU, CUDA, MPS: isin_Tensor_Scalar_out","","- func: isin.Tensor_Scalar(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False) -\u003e Tensor","  variants: function","  structured_delegate: isin.Tensor_Scalar_out","","- func: isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  structured: True","  dispatch:","    CPU, CUDA: isin_Scalar_Tensor_out","    MPS: isin_Scalar_Tensor_out_mps","","- func: isin.Scalar_Tensor(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -\u003e Tensor","  variants: function","  structured_delegate: isin.Scalar_Tensor_out","","- func: isnan(Tensor self) -\u003e Tensor","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CPU, CUDA, MPS, MTIA: isnan","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_isnan","    SparseCPU, SparseCUDA, SparseMPS: isnan_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: isnan_sparse_csr","  autogen: isnan.out","  tags: [core, pointwise]","","- func: is_distributed(Tensor self) -\u003e bool","  variants: function, method","  device_check: NoCheck","  device_guard: False","","- func: is_floating_point(Tensor self) -\u003e bool","  variants: function, method","  device_check: NoCheck","  device_guard: False","  manual_cpp_binding: True","","- func: is_complex(Tensor self) -\u003e bool","  variants: function, method","  device_check: NoCheck","  device_guard: False","  manual_cpp_binding: True","","- func: is_conj(Tensor self) -\u003e bool","  variants: function, method","  device_guard: False","  manual_cpp_binding: True","","- func: _is_zerotensor(Tensor self) -\u003e bool","  variants: function, method","  device_guard: False","  manual_cpp_binding: True","","- func: is_neg(Tensor self) -\u003e bool","  variants: function, method","  device_guard: False","  manual_cpp_binding: True","","- func: isreal(Tensor self) -\u003e Tensor","  variants: function, method","","- func: is_nonzero(Tensor self) -\u003e bool","  variants: function, method","  device_check: NoCheck","  device_guard: False","","- func: is_same_size(Tensor self, Tensor other) -\u003e bool","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: nested_is_same_size","    CompositeExplicitAutograd: is_same_size","","- func: is_signed(Tensor self) -\u003e bool","  variants: function, method","  device_check: NoCheck","  device_guard: False","  manual_cpp_binding: True","","- func: is_inference(Tensor self) -\u003e bool","  variants: function, method","  device_check: NoCheck","  device_guard: False","  manual_cpp_binding: True","","- func: kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -\u003e Tensor","","- func: kron(Tensor self, Tensor other) -\u003e Tensor","  variants: function, method","","- func: kron.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: kthvalue(Tensor self, SymInt k, int dim=-1, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: kthvalue","","- func: kthvalue.values(Tensor self, SymInt k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  dispatch:","    CPU: kthvalue_out_cpu","    CUDA: kthvalue_out_cuda","    MPS: kthvalue_out_mps","","- func: kthvalue.dimname(Tensor self, SymInt k, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  variants: function, method","","- func: kthvalue.dimname_out(Tensor self, SymInt k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","","- func: layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05, bool cudnn_enable=True) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: layer_norm_symint","","- func: native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CPU: layer_norm_cpu","    CUDA: layer_norm_cuda","    MPS: layer_norm_mps","    CompositeExplicitAutograd: math_native_layer_norm","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: nested_layer_norm","  autogen: native_layer_norm.out","  tags: core","","- func: native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CPU: layer_norm_backward_cpu","    CUDA: layer_norm_backward_cuda","    MPS: layer_norm_backward_mps","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: layer_norm_backward_nested","  autogen: native_layer_norm_backward.out","  tags: core","","- func: rms_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight=None, float? eps=None) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: rms_norm_symint","","- func: _fused_rms_norm(Tensor input, int[] normalized_shape, Tensor? weight, float? eps) -\u003e (Tensor, Tensor)","  dispatch:","    CUDA: _fused_rms_norm_cuda","    MPS: _fused_rms_norm_mps","    CompositeImplicitAutograd: rms_norm_composite","","- func: _fused_rms_norm_backward(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor rstd, Tensor? weight, bool[2] output_mask) -\u003e (Tensor, Tensor)","  dispatch:","    CUDA: _fused_rms_norm_backward_cuda","","- func: nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: nan_to_num","    SparseCPU, SparseCUDA, SparseMPS: nan_to_num_sparse","  tags: pointwise","","- func: nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -\u003e Tensor(a!)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: nan_to_num_","    SparseCPU, SparseCUDA, SparseMPS: nan_to_num_sparse_","  tags: pointwise","","- func: nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MTIA: nan_to_num_out","    MPS: nan_to_num_out_mps","    SparseCPU, SparseCUDA, SparseMPS: nan_to_num_sparse_out","  tags: pointwise","","- func: linear(Tensor input, Tensor weight, Tensor? bias=None) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: linear","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: nested_linear","    MPS: _mps_linear","","- func: linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: nested_linear_backward","    MPS: mps_linear_backward","  autogen: linear_backward.out","","- func: linear.out(Tensor input, Tensor weight, Tensor? bias=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CompositeExplicitAutograd: linear_out","","- func: mkldnn_linear(Tensor self, Tensor weight, Tensor? bias=None) -\u003e Tensor","  python_module: nn","  dispatch:","    MkldnnCPU: mkldnn_linear","  autogen: mkldnn_linear.out","","- func: mkldnn_linear_backward_input(int[] input_size, Tensor grad_output, Tensor weight) -\u003e Tensor","  dispatch:","    MkldnnCPU: mkldnn_linear_backward_input","  autogen: mkldnn_linear_backward_input.out","","- func: mkldnn_linear_backward_weights(Tensor grad_output, Tensor input, Tensor weight, bool bias_defined) -\u003e (Tensor, Tensor)","  dispatch:","    MkldnnCPU: mkldnn_linear_backward_weights","  autogen: mkldnn_linear_backward_weights.out","","- func: mkldnn_linear_backward(Tensor self, Tensor grad_output, Tensor weight, bool[3] output_mask) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    MkldnnCPU: mkldnn_linear_backward","  autogen: mkldnn_linear_backward.out","","- func: _cslt_compress(Tensor input) -\u003e Tensor","  dispatch:","    CUDA: _cslt_compress","","- func: _cslt_sparse_mm(Tensor compressed_A, Tensor dense_B, Tensor? bias=None, Tensor? alpha=None, ScalarType? out_dtype=None, bool transpose_result=False, int alg_id=0, int split_k=1, int split_k_mode=-1) -\u003e Tensor","  dispatch:","    CUDA: _cslt_sparse_mm","  tags: needs_fixed_stride_order","","- func: _cslt_sparse_mm_search(Tensor compressed_A, Tensor dense_B, Tensor? bias=None, Tensor? alpha=None, ScalarType? out_dtype=None, bool transpose_result=False) -\u003e int","  dispatch:","    CUDA: _cslt_sparse_mm_search","","- func: _sparse_semi_structured_tile(Tensor input, str algorithm=\"\", bool use_cutlass=True) -\u003e (Tensor, Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CUDA: _sparse_semi_structured_tile","","- func: _sparse_semi_structured_apply(Tensor input, Tensor thread_masks) -\u003e (Tensor, Tensor)","  dispatch:","    CUDA: _sparse_semi_structured_apply","","- func: _sparse_semi_structured_apply_dense(Tensor input, Tensor thread_masks) -\u003e Tensor","  dispatch:","    CUDA: _sparse_semi_structured_apply_dense","","# DEPRECATED: Use torch.__sparse_semi_structured_mm/torch._sparse_semi_structured_addmm instead","- func: _sparse_semi_structured_linear(Tensor input, Tensor weight, Tensor meta, *, Tensor? bias=None, str? activation=None, ScalarType? out_dtype=None) -\u003e Tensor","  dispatch:","    CUDA: _sparse_semi_structured_linear","","- func: _sparse_semi_structured_mm(Tensor mat1, Tensor mat1_meta, Tensor mat2, *, ScalarType? out_dtype=None) -\u003e Tensor","  dispatch:","    CUDA: _sparse_semi_structured_mm","","- func: _sparse_semi_structured_addmm(Tensor input, Tensor mat1, Tensor mat1_meta, Tensor mat2, *, Scalar alpha=1, Scalar beta=1, ScalarType? out_dtype=None) -\u003e Tensor","  dispatch:","    CUDA: _sparse_semi_structured_addmm","","- func: _mixed_dtypes_linear(Tensor input, Tensor weight, Tensor scale, *, Tensor? bias=None, str? activation=None) -\u003e Tensor","  dispatch:","    CUDA: _mixed_dtypes_linear","","- func: fbgemm_linear_int8_weight_fp32_activation(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -\u003e Tensor","","- func: fbgemm_linear_int8_weight(Tensor input, Tensor weight, Tensor packed, Tensor col_offsets, Scalar weight_scale, Scalar weight_zero_point, Tensor bias) -\u003e Tensor","","- func: fbgemm_linear_quantize_weight(Tensor input) -\u003e (Tensor, Tensor, float, int)","","- func: fbgemm_pack_gemm_matrix_fp16(Tensor input) -\u003e Tensor","","- func: _wrapped_linear_prepack(Tensor weight, Tensor weight_scale, Tensor weight_zero_point, Tensor bias) -\u003e Tensor","","- func: _wrapped_quantized_linear_prepacked(Tensor input, Tensor input_scale, Tensor input_zero_point, Tensor packed_weight, Tensor output_scale, Tensor output_zero_point, int out_channel) -\u003e Tensor","","- func: fbgemm_linear_fp16_weight_fp32_activation(Tensor input, Tensor packed_weight, Tensor? bias) -\u003e Tensor","","- func: fbgemm_linear_fp16_weight_fp32_activation.out(Tensor input, Tensor packed_weight, Tensor? bias, Tensor(a!) output) -\u003e Tensor","","- func: fbgemm_linear_fp16_weight(Tensor input, Tensor packed_weight, Tensor bias) -\u003e Tensor","","- func: fbgemm_linear_fp16_weight.out(Tensor input, Tensor packed_weight, Tensor bias, Tensor(a!) output) -\u003e Tensor","","- func: fbgemm_pack_quantized_matrix(Tensor input) -\u003e Tensor","","- func: fbgemm_pack_quantized_matrix.KN(Tensor input, int K, int N) -\u003e Tensor","","- func: ldexp.Tensor(Tensor self, Tensor other) -\u003e Tensor","  variants: function, method","","- func: ldexp_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  variants: function, method","  tags: pointwise","","- func: ldexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  tags: pointwise","","- func: linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: linspace","","- func: linspace.Tensor_Tensor(Tensor start, Tensor end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: linspace","","- func: linspace.Tensor_Scalar(Tensor start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: linspace","","- func: linspace.Scalar_Tensor(Scalar start, Tensor end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: linspace","","- func: linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, Meta: linspace_out","    CUDA: linspace_cuda_out","    MPS: linspace_out_mps","","- func: linspace.Tensor_Tensor_out(Tensor start, Tensor end, int steps, *, Tensor(a!) out) -\u003e Tensor(a!)","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: linspace_out","","- func: linspace.Tensor_Scalar_out(Tensor start, Scalar end, int steps, *, Tensor(a!) out) -\u003e Tensor(a!)","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: linspace_out","","- func: linspace.Scalar_Tensor_out(Scalar start, Tensor end, int steps, *, Tensor(a!) out) -\u003e Tensor(a!)","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: linspace_out","","- func: log(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: log.out","  variants: function, method","  tags: [core, pointwise]","","- func: log_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: log.out","  variants: function, method","  tags: pointwise","","- func: log.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: log_out","  tags: pointwise","","- func: log10(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: log10.out","  variants: function, method","  tags: [core, pointwise]","","- func: log10_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: log10.out","  variants: function, method","  tags: pointwise","","- func: log10.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: log10_out","  tags: pointwise","","- func: log1p(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: log1p.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: log1p_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: log1p_sparse_csr","  tags: [core, pointwise]","","- func: log1p_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: log1p.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: log1p_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: log1p_sparse_csr_","  tags: pointwise","","- func: log1p.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: log1p_out","    SparseCPU, SparseCUDA, SparseMPS: log1p_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: log1p_sparse_csr_out","  tags: pointwise","","- func: log2(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: log2.out","  variants: function, method","  tags: [core, pointwise]","","- func: log2_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: log2.out","  variants: function, method","  tags: pointwise","","- func: log2.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: log2_out","  tags: pointwise","","- func: logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: logaddexp_out","    MPS: logaddexp_out_mps","  tags: pointwise","","- func: logaddexp(Tensor self, Tensor other) -\u003e Tensor","  variants: method, function","  structured_delegate: logaddexp.out","  tags: pointwise","","- func: logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: logaddexp2_out","    MPS: logaddexp2_out_mps","  tags: pointwise","","- func: logaddexp2(Tensor self, Tensor other) -\u003e Tensor","  variants: method, function","  structured_delegate: logaddexp2.out","  tags: pointwise","","- func: xlogy.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: xlogy.OutTensor","  variants: function, method","  tags: pointwise","","- func: xlogy.Scalar_Self(Scalar self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: xlogy","  tags: pointwise","","- func: xlogy.Scalar_Other(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: xlogy","  tags: pointwise","","# xlogy: inplace variant","- func: xlogy_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: xlogy.OutTensor","  tags: pointwise","","- func: xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: xlogy_","","# xlogy: out variant","- func: xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  variants: function","  dispatch:","    CPU, CUDA: xlogy_out","    MPS: xlogy_out_mps","  tags: pointwise","","- func: xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: xlogy_out","  tags: pointwise","","- func: xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: xlogy_out","  tags: pointwise","","- func: logspace(Scalar start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: logspace","","- func: logspace.Tensor_Tensor(Tensor start, Tensor end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: logspace","","- func: logspace.Tensor_Scalar(Tensor start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: logspace","","- func: logspace.Scalar_Tensor(Scalar start, Tensor end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: logspace","","- func: logspace.out(Scalar start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, Meta: logspace_out","    CUDA: logspace_cuda_out","","- func: logspace.Tensor_Tensor_out(Tensor start, Tensor end, int steps, float base=10.0, *, Tensor(a!) out) -\u003e Tensor(a!)","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: logspace_out","","- func: logspace.Tensor_Scalar_out(Tensor start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -\u003e Tensor(a!)","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: logspace_out","","- func: logspace.Scalar_Tensor_out(Scalar start, Tensor end, int steps, float base=10.0, *, Tensor(a!) out) -\u003e Tensor(a!)","  category_override: factory","  dispatch:","    CompositeExplicitAutograd: logspace_out","","# log_softmax allows positional dtype, unlike most operators, because kwonly is BC-breaking when loading jit models.","- func: log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -\u003e Tensor","  variants: function, method","","- func: log_softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CompositeExplicitAutograd: log_softmax_out","","- func: log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor","  variants: function, method","","- func: _log_softmax(Tensor self, int dim, bool half_to_float) -\u003e Tensor","  structured_delegate: _log_softmax.out","  tags: core","","- func: _log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: log_softmax_cpu_out","    CUDA: log_softmax_cuda_out","    MTIA: log_softmax_mtia_out","    MPS: log_softmax_mps_out","","- func: _log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -\u003e Tensor","  structured_delegate: _log_softmax_backward_data.out","","- func: _log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: log_softmax_backward_cpu_out","    CUDA: log_softmax_backward_cuda_out","    MTIA: log_softmax_backward_mtia_out","    MPS: log_softmax_backward_mps_out","","- func: _logcumsumexp(Tensor self, int dim) -\u003e Tensor","  dispatch:","    CPU: _logcumsumexp_cpu","    CUDA: _logcumsumexp_cuda","    MPS: _logcumsumexp_mps","","- func: _logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU: _logcumsumexp_out_cpu","    CUDA: _logcumsumexp_out_cuda","    MPS: _logcumsumexp_out_mps","","- func: logcumsumexp(Tensor self, int dim) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: logcumsumexp","","- func: logcumsumexp.out(Tensor self, int dim, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: logcumsumexp_out","","- func: logcumsumexp.dimname(Tensor self, Dimname dim) -\u003e Tensor","  variants: function, method","","- func: logcumsumexp.dimname_out(Tensor self, Dimname dim, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: logsumexp(Tensor self, int[1] dim, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: logsumexp","","- func: logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    # calls squeeze","    CompositeExplicitAutogradNonFunctional: logsumexp_out","","- func: logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: margin_ranking_loss(Tensor input1, Tensor input2, Tensor target, float margin=0.0, int reduction=Mean) -\u003e Tensor","","- func: matmul(Tensor self, Tensor other) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeImplicitAutograd: matmul","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: matmul_nested","","- func: matmul_backward(Tensor grad, Tensor self, Tensor other, bool[2] mask) -\u003e (Tensor, Tensor)","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: matmul_backward_nested","  autogen: matmul_backward.out","","- func: matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeImplicitAutograd: matmul_out","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: matmul_out_nested","","# Alias to linalg.matrix_power","- func: matrix_power(Tensor self, int n) -\u003e Tensor","  variants: function, method","","# Alias to linalg.matrix_power","- func: matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -\u003e Tensor(a!)","","# Alias to linalg.matrix_exp","- func: matrix_exp(Tensor self) -\u003e Tensor","  variants: function, method","","# This function should be deprecated in favor of differential_analytic_matrix_function in FunctionsManual.cpp","- func: matrix_exp_backward(Tensor self, Tensor grad) -\u003e Tensor","","# DEPRECATED: Use torch.aminmax instead","- func: _aminmax(Tensor self) -\u003e (Tensor, Tensor)","  dispatch:","    CPU, CUDA: _aminmax_all","  autogen: _aminmax.out","","# DEPRECATED: Use torch.aminmax instead","- func: _aminmax.dim(Tensor self, int dim, bool keepdim=False) -\u003e (Tensor, Tensor)","  dispatch:","    CPU, CUDA: _aminmax","  autogen: _aminmax.dim_out","","- func: aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -\u003e (Tensor min, Tensor max)","  device_check: NoCheck   # TensorIterator","  structured_delegate: aminmax.out","  variants: function, method","","- func: aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -\u003e (Tensor(a!) min, Tensor(b!) max)","  device_check: NoCheck   # TensorIterator","  structured: True","  dispatch:","    CPU, CUDA: aminmax_out","    MPS: aminmax_out_mps","","- func: _compute_linear_combination(Tensor input, Tensor coefficients) -\u003e Tensor","  dispatch:","    CPU, CUDA: _compute_linear_combination","","- func: _compute_linear_combination.out(Tensor input, Tensor coefficients, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: _compute_linear_combination_out","","- func: max.dim(Tensor self, int dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  device_check: NoCheck   # TensorIterator","  structured_delegate: max.dim_max","  variants: function, method","  dispatch:","    QuantizedCPU, QuantizedCUDA: qmax","  tags: core","","- func: max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  device_check: NoCheck   # TensorIterator","  structured: True","  precomputed:","  - dim -\u003e int dim","  dispatch:","    CPU, CUDA, MTIA: max_out","    MPS: max_out_mps","","- func: max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  device_check: NoCheck   # TensorIterator","","- func: value_selecting_reduction_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: value_selecting_reduction_backward_symint","    NestedTensorCPU, NestedTensorCUDA: value_selecting_reduction_backward_nested_symint","","- func: amax(Tensor self, int[1] dim=[], bool keepdim=False) -\u003e Tensor","  variants: function, method","  structured_delegate: amax.out","  tags: core","","- func: amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU, CUDA: amax_out","    MPS: amax_out_mps","","# Return: (Tensor output, Tensor indices)","- func: max_pool1d_with_indices(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -\u003e (Tensor, Tensor)","","- func: max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -\u003e Tensor","","- func: max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: max_pool2d","    MPS: mps_max_pool2d","","- func: max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -\u003e Tensor","  dispatch:","    MPS: mps_max_pool2d_backward","  autogen: max_pool2d_backward.out","","- func: mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -\u003e Tensor","  dispatch:","    MkldnnCPU: mkldnn_max_pool2d","  autogen: mkldnn_max_pool2d.out","","- func: mkldnn_max_pool2d_backward(Tensor grad_output, Tensor output, Tensor input, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -\u003e Tensor","  dispatch:","    MkldnnCPU: mkldnn_max_pool2d_backward","  autogen: mkldnn_max_pool2d_backward.out","","- func: mkldnn_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -\u003e Tensor","  dispatch:","    MkldnnCPU: mkldnn_max_pool3d","  autogen: mkldnn_max_pool3d.out","","- func: mkldnn_max_pool3d_backward(Tensor grad_output, Tensor output, Tensor input, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -\u003e Tensor","  dispatch:","    MkldnnCPU: mkldnn_max_pool3d_backward","  autogen: mkldnn_max_pool3d_backward.out","","- func: quantized_max_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, int[1] dilation=1, bool ceil_mode=False) -\u003e Tensor","  dispatch:","    QuantizedCPU: quantized_max_pool1d","  autogen: quantized_max_pool1d.out","","- func: quantized_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -\u003e Tensor","  dispatch:","    QuantizedCPU: quantized_max_pool2d","    QuantizedCUDA: quantized_max_pool2d_cudnn","  autogen: quantized_max_pool2d.out","","- func: quantized_max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -\u003e Tensor","  dispatch:","    QuantizedCPU: quantized_max_pool3d","  autogen: quantized_max_pool3d.out","","- func: max_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -\u003e Tensor","","# The CPU and GPU dispatch variants are named weirdly here because otherwise there","# are namespacing issues in C++","- func: mean(Tensor self, *, ScalarType? dtype=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: mean","  tags: core","","# For normal naming convention this should be `mean.out`. However since we already have `mean.out` we have to rename this.","- func: mean.dtype_out(Tensor self, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CompositeExplicitAutograd: mean_dtype_out","","- func: mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  structured_delegate: mean.out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    QuantizedCPU: mean_quantized_cpu","  tags: core","","- func: mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: mean_out","    MPS: mean_out_mps","    QuantizedCPU: mean_out_quantized_cpu","","- func: mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  device_check: NoCheck   # Composite","  variants: function, method","","- func: nanmean.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # Composite","","- func: median(Tensor self) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU: median_cpu","    CUDA: median_cuda","    MPS: median_mps","  autogen: median.out","","- func: median.dim(Tensor self, int dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: median","","- func: median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  dispatch:","    CPU: median_out_cpu","    CUDA: median_out_cuda","    MPS: median_out_mps","","- func: median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  variants: function, method","","- func: median.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","","- func: nanmedian(Tensor self) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU: nanmedian_cpu","    CUDA: nanmedian_cuda","    MPS: nanmedian_mps","  autogen: nanmedian.out","","- func: nanmedian.dim(Tensor self, int dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: nanmedian","","- func: nanmedian.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  dispatch:","    CPU: nanmedian_out_cpu","    CUDA: nanmedian_out_cuda","    MPS: nanmedian_out_mps","","- func: nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  variants: function, method","","- func: nanmedian.names_dim_values(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","","- func: min.dim(Tensor self, int dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  device_check: NoCheck   # TensorIterator","  structured_delegate: min.dim_min","  variants: function, method","  dispatch:","    QuantizedCPU, QuantizedCUDA: qmin","  tags: core","","- func: min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  device_check: NoCheck   # TensorIterator","  structured: True","  precomputed:","  - dim -\u003e int dim","  dispatch:","    CPU, CUDA, MTIA: min_out","    MPS: min_out_mps","","- func: min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  device_check: NoCheck   # TensorIterator","","- func: amin(Tensor self, int[1] dim=[], bool keepdim=False) -\u003e Tensor","  variants: function, method","  structured_delegate: amin.out","  tags: core","","- func: amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU, CUDA: amin_out","    MPS: amin_out_mps","","# TODO: Add this function to MPS dispatch key so that we avoid declaring it in","# native_functions.yaml","# https://github.com/pytorch/pytorch/issues/77394","- func: _mps_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups) -\u003e Tensor","  dispatch:","    MPS: _mps_convolution","  autogen: _mps_convolution.out","","- func: mps_convolution_backward(Tensor self, Tensor grad_output, Tensor weight, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool[3] output_mask) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    MPS: mps_convolution_backward","  autogen: mps_convolution_backward.out","","- func: mkldnn_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: mkldnn_convolution","  autogen: mkldnn_convolution.out","","- func: mkldnn_rnn_layer(Tensor input, Tensor weight0, Tensor weight1, Tensor weight2, Tensor weight3, Tensor hx_, Tensor cx_, bool reverse, int[] batch_sizes, int mode, int hidden_size, int num_layers, bool has_biases, bool bidirectional, bool batch_first, bool train) -\u003e (Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CPU: mkldnn_rnn_layer","    MkldnnCPU: mkldnn_rnn_layer","  autogen: mkldnn_rnn_layer.out","","- func: mkldnn_rnn_layer_backward(Tensor input, Tensor weight1, Tensor weight2, Tensor weight3, Tensor weight4, Tensor hx_, Tensor cx_tmp, Tensor output, Tensor hy_, Tensor cy_, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, bool reverse, int mode, int hidden_size, int num_layers, bool has_biases, bool train, bool bidirectional, int[] batch_sizes, bool batch_first, Tensor workspace) -\u003e (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CPU: mkldnn_rnn_layer_backward","  autogen: mkldnn_rnn_layer_backward.out","","- func: miopen_batch_norm(Tensor input, Tensor weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float exponential_average_factor, float epsilon) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CUDA: miopen_batch_norm","  autogen: miopen_batch_norm.out","","- func: miopen_batch_norm_backward(Tensor input, Tensor grad_output, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, float epsilon) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CUDA: miopen_batch_norm_backward","  autogen: miopen_batch_norm_backward.out","","- func: miopen_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic) -\u003e Tensor","  dispatch:","    CUDA: miopen_convolution","  autogen: miopen_convolution.out","","- func: miopen_convolution_transpose(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] output_padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic) -\u003e Tensor","  dispatch:","    CUDA: miopen_convolution_transpose","  autogen: miopen_convolution_transpose.out","","- func: miopen_depthwise_convolution(Tensor self, Tensor weight, Tensor? bias, SymInt[] padding, SymInt[] stride, SymInt[] dilation, SymInt groups, bool benchmark, bool deterministic) -\u003e Tensor","  dispatch:","    CUDA: miopen_depthwise_convolution","  autogen: miopen_depthwise_convolution.out","","- func: miopen_convolution_relu(Tensor self, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -\u003e Tensor","  dispatch:","    CUDA: miopen_convolution_relu","","- func: miopen_convolution_add_relu(Tensor self, Tensor weight, Tensor z, Scalar? alpha, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, SymInt groups) -\u003e Tensor","  dispatch:","    CUDA: miopen_convolution_add_relu","","- func: miopen_rnn(Tensor input, Tensor[] weight, int weight_stride0, Tensor hx, Tensor? cx, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state) -\u003e (Tensor, Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CUDA: miopen_rnn","  autogen: miopen_rnn.out","  tags: nondeterministic_seeded","","","- func: miopen_rnn_backward(Tensor input, Tensor[] weight, int weight_stride0, Tensor weight_buf, Tensor hx, Tensor? cx, Tensor output, Tensor? grad_output, Tensor? grad_hy, Tensor? grad_cy, int mode, int hidden_size, int num_layers, bool batch_first, float dropout, bool train, bool bidirectional, int[] batch_sizes, Tensor? dropout_state, Tensor reserve, bool[4] output_mask) -\u003e (Tensor, Tensor, Tensor, Tensor[])","  dispatch:","    CUDA: miopen_rnn_backward","  autogen: miopen_rnn_backward.out","","- func: mm(Tensor self, Tensor mat2) -\u003e Tensor","  structured_delegate: mm.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA: _sparse_mm","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: _sparse_csr_mm","  tags: core","","- func: mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: mm_out_cpu","    CUDA: mm_out_cuda","    MTIA: mm_out_mtia","    MPS: mm_out_mps","    XPU: mm_out_xpu","    SparseCPU, SparseCUDA: _sparse_mm_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: _sparse_csr_mm_out","","- func: mm.dtype(Tensor self, Tensor mat2, ScalarType out_dtype) -\u003e Tensor","  dispatch:","    CUDA: _mm_dtype_cuda","","- func: mm.dtype_out(Tensor self, Tensor mat2, ScalarType out_dtype, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CUDA: _mm_dtype_out_cuda","","- func: _int_mm(Tensor self, Tensor mat2) -\u003e Tensor","  dispatch:","    CPU: _int_mm_cpu","    CUDA: _int_mm_cuda","    XPU: _int_mm_xpu","","- func: _int_mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU: _int_mm_out_cpu","    CUDA: _int_mm_out_cuda","    XPU: _int_mm_out_xpu","","- func: _convert_weight_to_int4pack(Tensor self, int innerKTiles) -\u003e Tensor","  dispatch:","    CUDA: _convert_weight_to_int4pack_cuda","    MPS: _convert_weight_to_int4pack_mps","","- func: _weight_int4pack_mm(Tensor self, Tensor mat2, int qGroupSize, Tensor qScaleAndZeros) -\u003e Tensor","  dispatch:","    MPS: _weight_int4pack_mm_mps","    CUDA: _weight_int4pack_mm_cuda","","- func: _weight_int4pack_mm_with_scales_and_zeros(Tensor self, Tensor mat2, int qGroupSize, Tensor qScale, Tensor qZeros) -\u003e Tensor","  dispatch:","    XPU: _weight_int4pack_mm_xpu","","# Split int4 pack weight between cpu and other devices due to","# https://github.com/pytorch/ao/issues/1117#issuecomment-2451252756.","- func: _convert_weight_to_int4pack_for_cpu(Tensor self, int innerKTiles) -\u003e Tensor","  dispatch:","    CPU: _convert_weight_to_int4pack_cpu","","- func: _weight_int4pack_mm_for_cpu(Tensor self, Tensor mat2, int qGroupSize, Tensor qScaleAndZeros) -\u003e Tensor","  dispatch:","    CPU: _weight_int4pack_mm_cpu","","- func: _dyn_quant_pack_4bit_weight(Tensor weights, Tensor scales_zeros, Tensor? bias, int block_size, int in_features, int out_features) -\u003e Tensor","  dispatch:","    CPU: _dyn_quant_pack_4bit_weight_cpu","","- func: _dyn_quant_matmul_4bit(Tensor inp, Tensor packed_weights, int block_size, int in_features, int out_features) -\u003e Tensor","  dispatch:","    CPU: _dyn_quant_matmul_4bit_cpu","","- func: _weight_int8pack_mm(Tensor self, Tensor mat2, Tensor scales) -\u003e Tensor","  dispatch:","    CPU: _weight_int8pack_mm_cpu","    CUDA: _weight_int8pack_mm_cuda","    MPS: _weight_int8pack_mm_mps","","- func: _sparse_mm(Tensor sparse, Tensor dense) -\u003e Tensor","  python_module: sparse","","- func: _sparse_mm.reduce(Tensor sparse, Tensor dense, str reduce) -\u003e Tensor","  python_module: sparse","","- func: _sparse_sparse_matmul(Tensor self, Tensor other) -\u003e Tensor","  dispatch:","    SparseCPU: sparse_sparse_matmul_cpu","    SparseCUDA: sparse_sparse_matmul_cuda","  autogen: _sparse_sparse_matmul.out","","- func: mode(Tensor self, int dim=-1, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  variants: function, method","  dispatch:","    CPU, CUDA: mode","","- func: mode.values(Tensor self, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  dispatch:","    CompositeExplicitAutograd: mode_out","","- func: mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices)","  variants: function, method","","- func: mode.dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","","- func: mul.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: mul.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA: mul_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: mul_sparse_csr","    MkldnnCPU: mkldnn_mul","    ZeroTensor: mul_zerotensor","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_mul_Tensor","  tags: [core, pointwise]","","- func: mul_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: mul.out","  variants: method","  dispatch:","    SparseCPU, SparseCUDA: mul_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: mul_sparse_csr_","    MkldnnCPU: mkldnn_mul_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_mul__Tensor","  tags: pointwise","","- func: mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: mul_out","    SparseCPU: mul_out_sparse_cpu","    SparseCUDA: mul_out_sparse_cuda","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: mul_out_sparse_csr","    MkldnnCPU: mkldnn_mul_out","  tags: pointwise","  # For C++ only, until we have conversion from C++ numbers to Tensor","","- func: mul.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: mul","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: mul_scalar_sparse_csr","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_mul_Scalar","  tags: [core, pointwise]","","- func: mul_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: mul_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: mul__scalar_sparse_csr","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_mul__Scalar","  autogen: mul.Scalar_out","  tags: pointwise","# multiply, alias for mul","","- func: multiply.Tensor(Tensor self, Tensor other) -\u003e Tensor","  variants: function, method","","- func: multiply_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  variants: method","","- func: multiply.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: multiply.Scalar(Tensor self, Scalar other) -\u003e Tensor","  variants: function, method","","- func: multiply_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  variants: method","","- func: mv(Tensor self, Tensor vec) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: mv","    SparseCPU, SparseCUDA: mv_sparse","","- func: mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: mv_out","","- func: mvlgamma.out(Tensor self, int p, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: mvlgamma_out","  tags: pointwise","","- func: mvlgamma(Tensor self, int p) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: mvlgamma","  tags: pointwise","","- func: mvlgamma_(Tensor(a!) self, int p) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: mvlgamma_","  tags: pointwise","","- func: narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU: narrow_copy_dense_cpu","    SparseCPU, SparseCUDA: narrow_copy_sparse","    CompositeExplicitAutogradNonFunctional: narrow_copy_dense_symint","  tags: view_copy","","- func: narrow_copy.out(Tensor self, int dim, SymInt start, SymInt length, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU: narrow_copy_dense_cpu_out","","- func: narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: narrow_symint","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: narrow_nested_symint","","- func: narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: narrow_tensor_symint","","- func: native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CPU: batch_norm_cpu","    CUDA: batch_norm_cuda","    MPS: batch_norm_mps","    MkldnnCPU: mkldnn_batch_norm","","- func: native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -\u003e (Tensor(a!), Tensor(b!), Tensor(c!))","  dispatch:","    CUDA: batch_norm_cuda_out","    MPS: batch_norm_mps_out","    CPU: batch_norm_cpu_out","","# TODO: In 2 weeks, we should make native_batch_norm composite implicit so that this correct schema percolates correctly through our dispatching","- func: _native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CPU: _batch_norm_legit_cpu","    CUDA: _batch_norm_legit_cuda","    MPS: _batch_norm_legit_mps","    MkldnnCPU: _mkldnn_batch_norm_legit","  autogen: _native_batch_norm_legit_functional","  tags: core","","# HACK: identical to _native_batch_norm_legit, but training is known to be False,","# So we known that running stats will not be mutated.","# The real fix here is batch norm consolidation.","- func: _native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CompositeExplicitAutograd: _batch_norm_legit_no_training","  autogen: _native_batch_norm_legit_no_training.out","  tags: core","","- func: _native_batch_norm_legit.out(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps, *, Tensor(d!) out, Tensor(e!) save_mean, Tensor(f!) save_invstd) -\u003e (Tensor(d!), Tensor(e!), Tensor(f!))","  dispatch:","    CPU: _batch_norm_legit_cpu_out","    CUDA: _batch_norm_legit_cuda_out","    MPS: _batch_norm_legit_mps_out","","- func: _native_batch_norm_legit.no_stats(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CPU: _batch_norm_legit_no_stats_cpu","    CUDA: _batch_norm_legit_no_stats_cuda","    MPS: _batch_norm_legit_no_stats_mps","    MkldnnCPU: _mkldnn_batch_norm_legit_no_stats","  tags: core","","- func: _native_batch_norm_legit.no_stats_out(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -\u003e (Tensor(a!), Tensor(b!), Tensor(c!))","  dispatch:","    CPU: _batch_norm_legit_no_stats_cpu_out","    CUDA: _batch_norm_legit_no_stats_cuda_out","    MPS: _batch_norm_legit_no_stats_mps_out","","- func: batch_norm_stats(Tensor input, float eps) -\u003e (Tensor, Tensor)","  dispatch:","    CUDA: batch_norm_stats_cuda","  autogen: batch_norm_stats.out","","- func: batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -\u003e Tensor","  dispatch:","    CUDA: batch_norm_elemt_cuda","","- func: batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CUDA: batch_norm_elemt_cuda_out","","# for backward compatibility","- func: batch_norm_gather_stats(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, int count) -\u003e (Tensor, Tensor)","  dispatch:","    CUDA: batch_norm_gather_stats_cuda","  autogen: batch_norm_gather_stats.out","","- func: batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -\u003e (Tensor, Tensor)","  dispatch:","    CUDA: batch_norm_gather_stats_with_counts_cuda","  autogen: batch_norm_gather_stats_with_counts.out","","- func: native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CPU: batch_norm_backward_cpu","    CUDA: batch_norm_backward_cuda","    MPS: batch_norm_backward_mps","    MkldnnCPU: mkldnn_batch_norm_backward","  autogen: native_batch_norm_backward.out","","- func: batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -\u003e (Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CUDA: batch_norm_backward_reduce_cuda","  autogen: batch_norm_backward_reduce.out","","- func: batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor sum_dy, Tensor sum_dy_xmu, Tensor count) -\u003e Tensor","  dispatch:","    CUDA: batch_norm_backward_elemt_cuda","  autogen: batch_norm_backward_elemt.out","","- func: batch_norm_update_stats(Tensor input, Tensor? running_mean, Tensor? running_var, float momentum) -\u003e (Tensor, Tensor)","  dispatch:","    CPU: batch_norm_update_stats_cpu","    CUDA: batch_norm_update_stats_cuda","  autogen: batch_norm_update_stats.out","","- func: is_vulkan_available() -\u003e bool","","- func: _nnpack_available() -\u003e bool","","- func: _nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, SymInt[2] stride=1) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: _nnpack_spatial_convolution","  autogen: _nnpack_spatial_convolution.out","","- func: ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: ones","  autogen: ones.names_out","","- func: ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: ones","","- func: ones.out(SymInt[] size, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: ones_out","","- func: ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd: ones_like","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: ones_like","  autogen: ones_like.out","","- func: pairwise_distance(Tensor x1, Tensor x2, float p=2, float eps=1e-06, bool keepdim=False) -\u003e Tensor","","- func: cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -\u003e Tensor","","- func: _euclidean_dist(Tensor x1, Tensor x2) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _euclidean_dist","  autogen: _euclidean_dist.out","","- func: _cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -\u003e Tensor","  dispatch:","    CPU, CUDA: _cdist_forward","    MPS: _cdist_forward_mps","  autogen: _cdist_forward.out","  tags: core","","- func: _cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -\u003e Tensor","  dispatch:","    CPU, CUDA: _cdist_backward","  autogen: _cdist_backward.out","","- func: pdist(Tensor self, float p=2) -\u003e Tensor","","- func: _pdist_forward(Tensor self, float p=2) -\u003e Tensor","  dispatch:","    CPU, CUDA: _pdist_forward","  autogen: _pdist_forward.out","  tags: core","","- func: _pdist_backward(Tensor grad, Tensor self, float p, Tensor pdist) -\u003e Tensor","  dispatch:","    CPU, CUDA: _pdist_backward","  autogen: _pdist_backward.out","","- func: cosine_similarity(Tensor x1, Tensor x2, int dim=1, float eps=1e-08) -\u003e Tensor","  variants: function","","- func: permute(Tensor(a) self, int[] dims) -\u003e Tensor(a)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: permute","    MPS: permute_mps","    SparseCPU, SparseCUDA: permute_sparse_coo","  tags: core","","- func: movedim.intlist(Tensor(a) self, int[] source, int[] destination) -\u003e Tensor(a)","  variants: function, method","","- func: movedim.int(Tensor(a) self, int source, int destination) -\u003e Tensor(a)","  variants: function, method","","# moveaxis, alias for movedim","- func: moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -\u003e Tensor(a)","  variants: function, method","","- func: moveaxis.int(Tensor(a) self, int source, int destination) -\u003e Tensor(a)","  variants: function, method","","# Only exposed from C++ -- in Python,","# we expose it as an attribute `T`, not a function.","#","# I'd like to name this \"T\" in C++ too, but","# calling a native function \"T\" causes undefined","# behavior on Windows, for reasons I don't understand","# (maybe related to capital letter collation somehow...)","- func: numpy_T(Tensor(a) self) -\u003e Tensor(a)","  variants: method","","# Exposed on Python as an attribute 'H'","- func: matrix_H(Tensor(a) self) -\u003e Tensor(a)","  variants: method","","# Exposed on Python as an attribute 'mT'","- func: mT(Tensor(a) self) -\u003e Tensor(a)","  variants: method","","# Exposed on Python as an attribute 'mH'","- func: mH(Tensor(a) self) -\u003e Tensor(a)","  variants: method","","- func: adjoint(Tensor(a) self) -\u003e Tensor(a)","  variants: function, method","","- func: pixel_shuffle(Tensor self, int upscale_factor) -\u003e Tensor","  dispatch:","    CPU: pixel_shuffle_cpu","    MPS: pixel_shuffle_mps","    CompositeExplicitAutogradNonFunctional: math_pixel_shuffle","  autogen: pixel_shuffle.out","","- func: pixel_unshuffle(Tensor self, int downscale_factor) -\u003e Tensor","  dispatch:","    CPU: pixel_unshuffle_cpu","    MPS: pixel_unshuffle_mps","    CompositeExplicitAutogradNonFunctional: math_pixel_unshuffle","  autogen: pixel_unshuffle.out","","- func: channel_shuffle(Tensor self, SymInt groups) -\u003e Tensor","  dispatch:","    CPU, CUDA: channel_shuffle","    QuantizedCPU: channel_shuffle_quantized_cpu","  autogen: channel_shuffle.out","","- func: native_channel_shuffle(Tensor self, SymInt groups) -\u003e Tensor","  dispatch:","    CPU: channel_shuffle_cpu","    CompositeImplicitAutograd: math_channel_shuffle","","- func: is_pinned(Tensor self, Device? device=None) -\u003e bool","  variants: method","  dispatch:","    # the NestedTensor keys are necessary because NestedTensor has been removed","    # from the CompositeExplicitAutograd keyset see Note [NestedTensor Not Included in Backend Keys]","    CompositeExplicitAutograd, NestedTensorCPU: is_pinned","    SparseCsrCPU: is_pinned_sparse_compressed","    SparseCPU: is_pinned_sparse_coo","","# TODO: add a copy kwarg that guarantees that the tensor is put into fresh","# pinned memory","- func: pin_memory(Tensor(a) self, Device? device=None) -\u003e Tensor(a)","  variants: method","","# Unlike pin_memory, this is guaranteed to give a new non-aliasing tensor","- func: _pin_memory(Tensor self, Device? device=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _pin_memory","    NestedTensorCPU: _pin_memory_nested","    SparseCPU: _pin_memory_sparse_coo","    SparseCsrCPU: _pin_memory_sparse_compressed","  autogen: _pin_memory.out","","- func: pinverse(Tensor self, float rcond=1e-15) -\u003e Tensor","  variants: function, method","","- func: poisson_nll_loss(Tensor input, Tensor target, bool log_input, bool full, float eps, int reduction) -\u003e Tensor","  variants: function","","- func: rad2deg(Tensor self) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: rad2deg","    SparseCPU, SparseCUDA, SparseMPS: rad2deg_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: rad2deg_sparse_csr","  tags: pointwise","","- func: rad2deg_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: rad2deg_","    SparseCPU, SparseCUDA, SparseMPS: rad2deg_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: rad2deg_sparse_csr_","  tags: pointwise","","- func: rad2deg.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: rad2deg_out","    SparseCPU, SparseCUDA, SparseMPS: rad2deg_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: rad2deg_sparse_csr_out","  tags: pointwise","","- func: deg2rad(Tensor self) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: deg2rad","    SparseCPU, SparseCUDA, SparseMPS: deg2rad_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: deg2rad_sparse_csr","  tags: pointwise","","- func: deg2rad_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: deg2rad_","    SparseCPU, SparseCUDA, SparseMPS: deg2rad_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: deg2rad_sparse_csr_","  tags: pointwise","","- func: deg2rad.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: deg2rad_out","    SparseCPU, SparseCUDA, SparseMPS: deg2rad_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: deg2rad_sparse_csr_out","  tags: pointwise","","- func: scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: scalar_tensor","  autogen: scalar_tensor.out","  tags: core","","- func: rand.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: rand","  autogen: rand.names_out","  tags: nondeterministic_seeded","","- func: rand.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  device_check: NoCheck","  device_guard: False","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: rand","  autogen: rand.generator_with_names_out","","- func: rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: [core, nondeterministic_seeded]","  dispatch:","    CompositeExplicitAutograd: rand","","- func: rand.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: rand","","- func: rand.out(SymInt[] size, *, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: rand_out","","- func: rand.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","","- func: rand_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd: rand_like","  autogen: rand_like.out","","- func: randint(SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randint","","- func: randint.generator(SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randint","","- func: randint.low(SymInt low, SymInt high, SymInt[] size, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randint","","- func: randint.low_generator(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randint","","- func: randint.out(SymInt high, SymInt[] size, *, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randint_out","","- func: randint.generator_out(SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randint_out","","- func: randint.low_out(SymInt low, SymInt high, SymInt[] size, *, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randint_out","","- func: randint.low_generator_out(SymInt low, SymInt high, SymInt[] size, *, Generator? generator, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randint_out","","- func: randint_like(Tensor self, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd: randint_like","  autogen: randint_like.out","","- func: randint_like.Tensor(Tensor self, Tensor high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd: randint_like","  autogen: randint_like.Tensor_out","","- func: randint_like.low_dtype(Tensor self, SymInt low, SymInt high, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd: randint_like","  autogen: randint_like.low_dtype_out","","- func: randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: [core, nondeterministic_seeded]","  dispatch:","    CompositeExplicitAutograd: randn","","- func: randn.generator(SymInt[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randn","","- func: randn.names(SymInt[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: nondeterministic_seeded","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: randn","  autogen: randn.names_out","","- func: randn.generator_with_names(SymInt[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: nondeterministic_seeded","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: randn","  autogen: randn.generator_with_names_out","","- func: randn.out(SymInt[] size, *, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","","- func: randn.generator_out(SymInt[] size, *, Generator? generator, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","","- func: randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd, CompositeImplicitAutogradNestedTensor: randn_like","  autogen: randn_like.out","","- func: randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: [core, nondeterministic_seeded]","  dispatch:","    CompositeExplicitAutograd: randperm","","- func: randperm.generator(SymInt n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randperm","","- func: randperm.out(SymInt n, *, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: randperm_out","","- func: randperm.generator_out(SymInt n, *, Generator? generator, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","  dispatch:","    CPU: randperm_out_cpu","    CUDA: randperm_out_cuda","    MPS: randperm_out_mps","","- func: range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: range","","- func: range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: range","","- func: range.out_(Scalar start, Scalar end, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: range_out_no_step","","- func: range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, Meta: range_out","    CUDA: range_cuda_out","    MPS: range_mps_out","  cpp_no_default_args: ['step']","","- func: ravel(Tensor(a) self) -\u003e Tensor(a)","  variants: function, method","","- func: reciprocal(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: reciprocal.out","  variants: function, method","  tags: [core, pointwise]","","- func: reciprocal_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: reciprocal.out","  variants: function, method","  tags: pointwise","","- func: reciprocal.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MTIA: reciprocal_out","    MPS: reciprocal_out_mps","  tags: pointwise","","- func: neg(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: neg.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: neg_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: neg_sparse_csr","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_neg","  tags: [core, pointwise]","","- func: neg_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: neg.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: neg_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: neg_sparse_csr_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_neg_","  tags: pointwise","","- func: neg.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: neg_out","    SparseCPU, SparseCUDA, SparseMPS: neg_out_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: neg_sparse_csr_out","  tags: pointwise","# Alias for neg","","- func: negative(Tensor self) -\u003e Tensor","  variants: function, method","","- func: negative_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","","- func: negative.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: repeat(Tensor self, SymInt[] repeats) -\u003e Tensor","  variants: method  # This is method-only to match the previous tensor API. In the future we could make this a function too.","  dispatch:","    CompositeExplicitAutograd: repeat","    MPS: repeat_mps","  autogen: repeat.out","  tags: core","","- func: repeat_interleave.Tensor(Tensor repeats, *, SymInt? output_size=None) -\u003e Tensor","  variants: function","  dispatch:","    CPU: repeat_interleave_cpu","    CUDA: repeat_interleave_cuda","    MPS: repeat_interleave_mps","  tags: dynamic_output_shape","  autogen: repeat_interleave.Tensor_out","","- func: repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeImplicitAutograd: repeat_interleave_symint","","- func: repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeImplicitAutograd: repeat_interleave_symint","","- func: reshape(Tensor(a) self, SymInt[] shape) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: reshape_symint","    CompositeImplicitAutogradNestedTensor: reshape_nested_symint","","- func: _reshape_copy(Tensor self, SymInt[] size) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: _reshape_copy_symint","","# NOTE [ _reshape_alias ] is meant to be used in the implementation of reshape.","# They are not user-facing, hence the leading underscore. Please don't use it","# anywhere else.","- func: _reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, ZeroTensor, MPS, MTIA: _reshape_alias","    # We don't need to support mkldnn since this is handled explicitly by the reshape operator.","","- func: _mkldnn_reshape(Tensor self, int[] shape) -\u003e Tensor","  device_check: NoCheck","  device_guard: False","  dispatch:","    MkldnnCPU: mkldnn_reshape","  autogen: _mkldnn_reshape.out","","- func: reshape_as(Tensor(a) self, Tensor other) -\u003e Tensor(a)","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: reshape_as","    CompositeImplicitAutogradNestedTensor: reshape_as_nested","","- func: round(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: round.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: round_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: round_sparse_csr","  tags: [core, pointwise]","","- func: round_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: round.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: round_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: round_sparse_csr_","  tags: pointwise","","- func: round.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: round_out","    SparseCPU, SparseCUDA, SparseMPS: round_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: round_sparse_csr_out","  tags: pointwise","","- func: round.decimals(Tensor self, *, int decimals) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: round.decimals_out","  variants: function, method","  tags: pointwise","","- func: round_.decimals(Tensor(a!) self, *, int decimals) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: round.decimals_out","  variants: function, method","  tags: pointwise","","- func: round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: round_decimals_out","  tags: pointwise","","- func: rrelu(Tensor self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  tags: [pointwise, nondeterministic_seeded]","","- func: rrelu_(Tensor(a!) self, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","  device_check: NoCheck   # TensorIterator","","- func: relu(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CPU, CUDA: relu","    MPS: relu_mps","    MTIA: relu_mtia","    MkldnnCPU: mkldnn_relu","    QuantizedCPU: relu_quantized_cpu","    QuantizedCUDA: relu_quantized_cuda","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_relu","    SparseCPU, SparseCUDA, SparseMPS: relu_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: relu_sparse_csr","  tags: [core, pointwise]","","- func: relu_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CPU, CUDA: relu_","    MPS: relu_mps_","    MTIA: relu_mtia_","    MkldnnCPU: mkldnn_relu_","    QuantizedCPU: relu_quantized_cpu_","    QuantizedCUDA: relu_quantized_cuda_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_relu_","    SparseCPU, SparseCUDA, SparseMPS: relu_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: relu_sparse_csr_","  autogen: relu.out","  tags: pointwise","","- func: relu6(Tensor self) -\u003e Tensor","  python_module: nn","  tags: pointwise","","- func: relu6_(Tensor(a!) self) -\u003e Tensor(a!)","  python_module: nn","","- func: prelu(Tensor self, Tensor weight) -\u003e Tensor","  variants: function, method","  autogen: prelu.out","","- func: _prelu_kernel(Tensor self, Tensor weight) -\u003e Tensor","  dispatch:","    CPU, CUDA: _prelu_kernel","    QuantizedCPU: _prelu_kernel_quantized_cpu","    MkldnnCPU: mkldnn_prelu","    MPS: prelu_mps","","- func: _prelu_kernel_backward(Tensor grad_output, Tensor self, Tensor weight) -\u003e (Tensor, Tensor)","  dispatch:","    CPU, CUDA: _prelu_kernel_backward","    MkldnnCPU: mkldnn_prelu_backward","    MPS: prelu_backward_mps","","- func: gelu.out(Tensor self, *, str approximate='none', Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU: gelu_out_cpu","    CUDA: gelu_out_cuda","    MPS: gelu_out_mps","","- func: gelu_(Tensor(a!) self, *, str approximate='none') -\u003e Tensor(a!)","  structured_delegate: gelu.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    QuantizedCPU: gelu_quantized_cpu_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_gelu_","","- func: gelu(Tensor self, *, str approximate='none') -\u003e Tensor","  structured_delegate: gelu.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    MkldnnCPU: mkldnn_gelu","    QuantizedCPU: gelu_quantized_cpu","    QuantizedCUDA: gelu_quantized_cuda","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_gelu","  tags: [core, pointwise]","","- func: gelu_backward.grad_input(Tensor grad_output, Tensor self, *, str approximate='none', Tensor(a!) grad_input) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU: gelu_backward_out_cpu","    CUDA: gelu_backward_out_cuda","    MPS: gelu_backward_out_mps","","- func: gelu_backward(Tensor grad_output, Tensor self, *, str approximate='none') -\u003e Tensor","  structured_delegate: gelu_backward.grad_input","  python_module: nn","  dispatch:","    MkldnnCPU: mkldnn_gelu_backward","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: gelu_backwards_nested","  tags: pointwise","","- func: infinitely_differentiable_gelu_backward(Tensor grad, Tensor self) -\u003e Tensor","  variants: function","  python_module: nn","  device_check: NoCheck","  device_guard: False","","- func: hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MPS: hardshrink_out","","- func: hardshrink(Tensor self, Scalar lambd=0.5) -\u003e Tensor","  structured_delegate: hardshrink.out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  tags: pointwise","","- func: hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: hardshrink_backward_out","","- func: hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -\u003e Tensor","  structured_delegate: hardshrink_backward.grad_input","  variants: function, method","","- func: rsqrt(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: rsqrt.out","  variants: function, method","  tags: [core, pointwise]","","- func: rsqrt_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: rsqrt.out","  variants: function, method","  tags: pointwise","","- func: rsqrt.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: rsqrt_out","  tags: pointwise","","- func: select.Dimname(Tensor(a) self, Dimname dim, int index) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","","- func: select.int(Tensor(a) self, int dim, SymInt index) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: select_symint","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: select_sparse_csr","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: select_nested","  tags: core","","- func: select_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt index) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutogradNonFunctional: select_backward_symint","  autogen: select_backward.out","","- func: _nested_select_backward(Tensor grad_output, Tensor self, int dim, SymInt index) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: _nested_select_backward_symint","","- func: selu(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  tags: pointwise","","- func: selu_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: celu(Tensor self, Scalar alpha=1.0) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  dispatch:","    CompositeExplicitAutograd: celu","  tags: pointwise","","- func: celu_(Tensor(a!) self, Scalar alpha=1.0) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CompositeExplicitAutograd: celu_","  autogen: celu.out","","- func: silu(Tensor self) -\u003e Tensor","  structured_delegate: silu.out","  python_module: nn","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_silu","  tags: pointwise","","- func: silu_(Tensor(a!) self) -\u003e Tensor(a!)","  structured_delegate: silu.out","  python_module: nn","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_silu_","  tags: pointwise","","- func: silu.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA, MTIA: silu_out","    MPS: silu_out_mps","  tags: pointwise","","- func: silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA: silu_backward_out","    MPS: silu_backward_out_mps","  tags: pointwise","","- func: silu_backward(Tensor grad_output, Tensor self) -\u003e Tensor","  structured_delegate: silu_backward.grad_input","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: math_silu_backward","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: silu_backward_nested","  tags: pointwise","","- func: mish(Tensor self) -\u003e Tensor","  structured_delegate: mish.out","  python_module: nn","  tags: pointwise","","- func: mish_(Tensor(a!) self) -\u003e Tensor(a!)","  structured_delegate: mish.out","  python_module: nn","","- func: mish.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA: mish_out","    MPS: mish_out_mps","","- func: mish_backward(Tensor grad_output, Tensor self) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU, CUDA: mish_backward","    MPS: mish_backward_mps","    CompositeImplicitAutograd: math_mish_backward","","- func: sigmoid(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: sigmoid.out","  variants: function, method","  dispatch:","    QuantizedCPU: sigmoid_quantized_cpu","    MkldnnCPU: mkldnn_sigmoid","  tags: [core, pointwise]","","- func: sigmoid_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: sigmoid.out","  variants: function, method","  dispatch:","    MkldnnCPU: mkldnn_sigmoid_","  tags: pointwise","","- func: sigmoid.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: sigmoid_out","  tags: pointwise","","- func: logit(Tensor self, float? eps=None) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU, CUDA, MTIA: logit","    MPS: logit_mps","  tags: pointwise","","- func: logit_(Tensor(a!) self, float? eps=None) -\u003e Tensor(a!)","  variants: function, method","  dispatch:","    CPU, CUDA: logit_","  tags: pointwise","","- func: logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: logit_out","    MPS: logit_out_mps","  tags: pointwise","","- func: sin(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: sin.out","  variants: function, method","  dispatch:","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sin_sparse_csr","    SparseCPU, SparseCUDA, SparseMPS: sin_sparse","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_sin","  tags: [core, pointwise]","","- func: sin_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: sin.out","  variants: function, method","  dispatch:","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sin_sparse_csr_","    SparseCPU, SparseCUDA, SparseMPS: sin_sparse_","  tags: pointwise","","- func: sin.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: sin_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sin_sparse_csr_out","    SparseCPU, SparseCUDA, SparseMPS: sin_sparse_out","  tags: pointwise","","- func: sinc(Tensor self) -\u003e Tensor","  structured_delegate: sinc.out","  variants: function, method","  tags: pointwise","","- func: sinc_(Tensor(a!) self) -\u003e Tensor(a!)","  structured_delegate: sinc.out","  variants: function, method","  tags: pointwise","","- func: sinc.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: sinc_out","  tags: pointwise","","- func: sinh(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: sinh.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: sinh_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sinh_sparse_csr","  tags: [core, pointwise]","","- func: sinh_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: sinh.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: sinh_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sinh_sparse_csr_","  tags: pointwise","","- func: sinh.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: sinh_out","    SparseCPU, SparseCUDA, SparseMPS: sinh_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sinh_sparse_csr_out","","# Returns a copy of this `Variable` that is detached from its autograd graph.","# This method is OK to call if the `Variable` is a view.","#","# NOTE: Previously, if we change the tensor metadata (e.g. sizes / strides /","# storage / storage_offset) of a tensor created from `detach()`, those metadata","# in the original tensor will also be updated. However, the new behavior is that","# those metadata changes to the detached tensor will not update the original tensor","# anymore, and in the `detach()` function we need to set `allow_tensor_metadata_change_`","# to false to make such changes explicitly illegal, in order to prevent users from","# changing metadata of the detached tensor and expecting the original tensor to also","# be updated.","  tags: pointwise","- func: detach(Tensor(a) self) -\u003e Tensor(a)","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: detach","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: detach","","# Like `detach()`, but modifies this `Variable` in-place. This method may","# only be called on non-view `Variable`s. You can use `is_view()` to check","# this. If this `Variable` is a view, throws an `std::runtime_error()`.","- func: detach_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","  tags: inplace_view","  dispatch:","    CompositeExplicitAutograd: detach_","","- func: size.int(Tensor self, int dim) -\u003e int","  variants: function","  device_check: NoCheck","  device_guard: False","  manual_cpp_binding: True","","- func: size.Dimname(Tensor self, Dimname dim) -\u003e int","  variants: function, method","  device_check: NoCheck","  device_guard: False","","- func: sym_size.int(Tensor self, int dim) -\u003e SymInt","  variants: function","  device_check: NoCheck","  device_guard: False","  tags: core","  manual_cpp_binding: True","","- func: sym_is_contiguous(Tensor self, MemoryFormat memory_format=contiguous_format) -\u003e SymBool","  variants: function","  device_check: NoCheck","  device_guard: False","  tags: core","  manual_cpp_binding: True","","- func: sym_numel(Tensor self) -\u003e SymInt","  variants: function","  device_check: NoCheck","  device_guard: False","  tags: core","  manual_cpp_binding: True","","- func: sym_storage_offset(Tensor self) -\u003e SymInt","  variants: function","  device_check: NoCheck","  device_guard: False","  tags: core","  manual_cpp_binding: True","","- func: slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: slice","  tags: core","","# NOTE: The implementation of split_with_sizes bypasses the dispatcher to call this; undo","# that if adding specific implementations here!","","- func: slice_backward(Tensor grad_output, SymInt[] input_sizes, int dim, SymInt start, SymInt end, SymInt step) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: slice_backward","  autogen: slice_backward.out","","# NB: This op exists to back the implementation of reverse view_funcs for various views (chunk,","# slice.Tensor, split_with_sizes, et al.). Currently, these are only used during fake-ification","# of PT2 graph input subclass instances that are views. This means:","# * This op shouldn't really show up in eager mode (so e.g. XLA shouldn't have to implement it)","# * This op shouldn't show up in a PT2 graph (so a PT2 backend shouldn't have to implement it)","# * A subclass will have to implement this to work in PT2 if a subclass view is used as a graph","#   input AND the view utilizes this op in its inverse. The idea is that slice_inverse() is","#   easier to implement for a subclass than as_strided()","- func: slice_inverse(Tensor(a) self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: slice_inverse_symint","","- func: slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -\u003e Tensor","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutogradNonFunctional: slice_scatter","  autogen: slice_scatter.out","  tags: [core, view_copy]","","- func: select_scatter(Tensor self, Tensor src, int dim, SymInt index) -\u003e Tensor","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutogradNonFunctional: select_scatter_symint","  autogen: select_scatter.out","  tags: core","","- func: diagonal_scatter(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1) -\u003e Tensor","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutogradNonFunctional: diagonal_scatter","  autogen: diagonal_scatter.out","","- func: as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -\u003e Tensor","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutogradNonFunctional: as_strided_scatter_symint","  autogen: as_strided_scatter.out","","- func: smm(Tensor self, Tensor mat2) -\u003e Tensor","  variants: function, method","","# softmax allows positional dtype, unlike most operators, because kwonly is BC-breaking when loading jit models.","- func: softmax.int(Tensor self, int dim, ScalarType? dtype=None) -\u003e Tensor","  variants: function, method","","- func: softmax.int_out(Tensor self, int dim, ScalarType? dtype=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CompositeExplicitAutograd: softmax_out","","- func: softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor","  variants: function, method","","- func: _softmax(Tensor self, int dim, bool half_to_float) -\u003e Tensor","  structured_delegate: _softmax.out","  dispatch:","    MkldnnCPU: mkldnn_softmax","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: softmax_nested","  tags: core","","- func: _softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: softmax_cpu_out","    CUDA: softmax_cuda_out","    MPS: softmax_mps_out","","- func: _softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -\u003e Tensor","  structured_delegate: _softmax_backward_data.out","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: nested_softmax_backward","","- func: _softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: softmax_backward_cpu_out","    CUDA: softmax_backward_cuda_out","    MPS: softmax_backward_mps_out","","- func: unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -\u003e Tensor[]","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: unsafe_split","  autogen: unsafe_split.Tensor_out","","- func: split.Tensor(Tensor(a -\u003e *) self, SymInt split_size, int dim=0) -\u003e Tensor(a)[]","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: split","","- func: split.sizes(Tensor(a -\u003e *) self, SymInt[] split_size, int dim=0) -\u003e Tensor(a)[]","  variants: function, method","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: split_symint","","- func: unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -\u003e Tensor[]","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: unsafe_split_with_sizes","  autogen: unsafe_split_with_sizes.out","","- func: split_with_sizes(Tensor(a -\u003e *) self, SymInt[] split_sizes, int dim=0) -\u003e Tensor(a)[]","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: split_with_sizes","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: split_with_sizes_nested","  tags: core","","- func: hsplit.int(Tensor(a -\u003e *) self, int sections) -\u003e Tensor(a)[]","  variants: function, method","","- func: hsplit.array(Tensor(a -\u003e *) self, int[] indices) -\u003e Tensor(a)[]","  variants: function, method","","- func: vsplit.int(Tensor(a -\u003e *) self, int sections) -\u003e Tensor(a)[]","  variants: function, method","","- func: vsplit.array(Tensor(a -\u003e *) self, int[] indices) -\u003e Tensor(a)[]","  variants: function, method","","- func: dsplit.int(Tensor(a -\u003e *) self, int sections) -\u003e Tensor(a)[]","  variants: function, method","","- func: dsplit.array(Tensor(a -\u003e *) self, int[] indices) -\u003e Tensor(a)[]","  variants: function, method","","- func: squeeze(Tensor(a) self) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: squeeze","    QuantizedCPU, QuantizedCUDA: squeeze_quantized","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: squeeze_nested","","- func: squeeze.dim(Tensor(a) self, int dim) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: squeeze","    QuantizedCPU, QuantizedCUDA: squeeze_quantized","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: squeeze_dim_nested","  tags: core","","- func: squeeze.dimname(Tensor(a) self, Dimname dim) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","","","- func: squeeze.dims(Tensor(a) self, int[] dim) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: squeeze","    QuantizedCPU, QuantizedCUDA: squeeze_quantized","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: squeeze_dim_nested","  tags: core","","- func: squeeze_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  tags: inplace_view","  dispatch:","    CompositeExplicitAutograd: squeeze_","","- func: squeeze_.dim(Tensor(a!) self, int dim) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  tags: inplace_view","  dispatch:","    CompositeExplicitAutograd: squeeze_","","- func: squeeze_.dims(Tensor(a!) self, int[] dim) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  tags: inplace_view","  dispatch:","    CompositeExplicitAutograd: squeeze_","","- func: squeeze_.dimname(Tensor(a!) self, Dimname dim) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  tags: inplace_view","","- func: sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor","  variants: function, method","","- func: sspaddmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU: _sspaddmm_out_only_sparse","    CUDA: _sspaddmm_out_only_sparse_cuda","    SparseCPU: _sspaddmm_out_cpu","    SparseCUDA: _sspaddmm_out_cuda","","- func: _chunk_cat(Tensor[] tensors, int dim, int num_chunks) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _chunk_cat","    CUDA: _chunk_cat_cuda","","- func: _chunk_cat.out(Tensor[] tensors, int dim, int num_chunks, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: _chunk_cat_out","    CUDA: _chunk_cat_out_cuda","","- func: stack(Tensor[] tensors, int dim=0) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: stack","","- func: stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: stack_out","","- func: _stack(Tensor[] tensors, int dim=0) -\u003e Tensor","  dispatch: # match the backends supported by _cat","    CPU: _stack_cpu","    CompositeExplicitAutograd: _stack","","- func: _stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch: # match the backends supported by _cat_out","    CPU: _stack_out_cpu","    CompositeExplicitAutograd: _stack_out","","- func: hstack(Tensor[] tensors) -\u003e Tensor","","- func: hstack.out(Tensor[] tensors, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: vstack(Tensor[] tensors) -\u003e Tensor","","- func: vstack.out(Tensor[] tensors, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: dstack(Tensor[] tensors) -\u003e Tensor","","- func: dstack.out(Tensor[] tensors, *, Tensor(a!) out) -\u003e Tensor(a!)","","# Overload without center \u0026 pad mode, needed for forward-compatibility","- func: stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None, bool? align_to_window=None) -\u003e Tensor","  variants: function, method","  cpp_no_default_args: ['hop_length', 'win_length', 'window', 'normalized']","","- func: stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode=\"reflect\", bool normalized=False, bool? onesided=None, bool? return_complex=None, bool? align_to_window=None) -\u003e Tensor","  variants: function, method","","- func: istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -\u003e Tensor","  variants: function, method","","- func: stride.int(Tensor self, int dim) -\u003e int","  variants: function","  device_check: NoCheck","  device_guard: False","  manual_cpp_binding: True","","- func: stride.Dimname(Tensor self, Dimname dim) -\u003e int","  variants: function, method","  device_check: NoCheck","  device_guard: False","","- func: sym_stride.int(Tensor self, int dim) -\u003e SymInt","  variants: function","  device_check: NoCheck","  device_guard: False","  tags: core","  manual_cpp_binding: True","","- func: sum(Tensor self, *, ScalarType? dtype=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: sum","    SparseCPU, SparseCUDA, SparseMeta: sum_coo","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sum_csr","  autogen: sum.out","","- func: sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  # TODO: Align the signature of sum.dim_IntList and _sparse_csr_sum.dim_dtype","  structured_delegate: sum.IntList_out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    NestedTensorCPU: NestedTensor_sum_dim_CPU","    SparseCPU, SparseCUDA: sum_sparse_coo","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sum_sparse_compressed","  tags: core","","- func: sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: sum_out","    MPS: sum_out_mps","","- func: sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","# TODO: this function will be replaced once nested expand semantics have been settled on","- func: _nested_sum_backward(Tensor grad, Tensor self, int[1]? dim, bool keepdim=False) -\u003e Tensor","  dispatch:","    NestedTensorCPU: _nested_sum_backward_cpu","","- func: nansum(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU, CUDA: nansum","    MPS: nansum_mps","","- func: nansum.out(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: nansum_out","    MPS: nansum_out_mps","","- func: hash_tensor(Tensor self, int[1] dim=[], *, bool keepdim=False, int mode=0) -\u003e Tensor","  variants: function, method","  structured_delegate: hash_tensor.out","","- func: hash_tensor.out(Tensor self, int[1] dim=[], *, bool keepdim=False, int mode=0, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU, CUDA: hash_tensor_out","","- func: sum_to_size(Tensor self, SymInt[] size) -\u003e Tensor","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: sum_to_size_symint","","- func: sqrt(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: sqrt.out","  variants: function, method","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_sqrt","    SparseCPU, SparseCUDA, SparseMPS: sqrt_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sqrt_sparse_csr","  tags: [core, pointwise]","","- func: sqrt_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: sqrt.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: sqrt_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sqrt_sparse_csr_","  tags: pointwise","","- func: sqrt.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: sqrt_out","    SparseCPU, SparseCUDA, SparseMPS: sqrt_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sqrt_sparse_csr_out","  tags: pointwise","","- func: square(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  tags: pointwise","","- func: square_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function, method","  tags: pointwise","","- func: square.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  tags: pointwise","","- func: std(Tensor self, bool unbiased=True) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  cpp_no_default_args: [\"unbiased\"]","","- func: std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  cpp_no_default_args: [\"unbiased\"]","","- func: std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CPU, CUDA: std","    MPS: std_mps","    QuantizedCPU: std_quantized_cpu","","- func: std_mean(Tensor self, bool unbiased=True) -\u003e (Tensor, Tensor)","  device_check: NoCheck   # TensorIterator","  variants: function","  cpp_no_default_args: [\"unbiased\"]","","- func: std_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -\u003e (Tensor, Tensor)","  device_check: NoCheck   # TensorIterator","  variants: function","  cpp_no_default_args: [\"unbiased\"]","","- func: std_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -\u003e (Tensor, Tensor)","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CPU, CUDA: std_mean","    MPS: std_mean_mps","  autogen: std_mean.correction_out","","- func: std_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -\u003e (Tensor, Tensor)","  device_check: NoCheck   # TensorIterator","  variants: function","  cpp_no_default_args: [\"unbiased\"]","","- func: std_mean.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -\u003e (Tensor, Tensor)","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: std.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  cpp_no_default_args: [\"unbiased\"]","","- func: std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: std_out","    QuantizedCPU: std_out_quantized_cpu","","- func: std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  cpp_no_default_args: [\"unbiased\"]","","- func: std.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  cpp_no_default_args: [\"unbiased\"]","","- func: std.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: std.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: prod(Tensor self, *, ScalarType? dtype=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CPU, CUDA: prod","    MPS: prod_mps","  autogen: prod.out","  tags: core","","- func: prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  structured_delegate: prod.int_out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  tags: core","","- func: prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: prod_out","    MPS: prod_out_mps","","- func: prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: prod.Dimname_out(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: t(Tensor(a) self) -\u003e Tensor(a)","  device_check: NoCheck","  device_guard: False","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: t","","- func: t_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck","  device_guard: False","  variants: method","  tags: inplace_view","  dispatch:","    CompositeExplicitAutograd: t_","","- func: tan(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: tan.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: tan_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: tan_sparse_csr","  tags: [core, pointwise]","","- func: tan_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: tan.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: tan_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: tan_sparse_csr_","  tags: pointwise","","- func: tan.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: tan_out","    SparseCPU, SparseCUDA, SparseMPS: tan_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: tan_sparse_csr_out","  tags: pointwise","","- func: tanh(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: tanh.out","  variants: function, method","  dispatch:","    QuantizedCPU: tanh_quantized_cpu","    MkldnnCPU: mkldnn_tanh","    SparseCPU, SparseCUDA, SparseMPS: tanh_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: tanh_sparse_csr","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_tanh","  tags: [core, pointwise]","","- func: tanh_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: tanh.out","  variants: function, method","  dispatch:","    MkldnnCPU: mkldnn_tanh_","    SparseCPU, SparseCUDA, SparseMPS: tanh_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: tanh_sparse_csr_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_tanh_","  tags: pointwise","","- func: tanh.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: tanh_out","    SparseCPU, SparseCUDA, SparseMPS: tanh_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: tanh_sparse_csr_out","  tags: pointwise","","- func: tensordot(Tensor self, Tensor other, int[] dims_self, int[] dims_other) -\u003e Tensor","  variants: function","","- func: tensordot.out(Tensor self, Tensor other, int[] dims_self, int[] dims_other, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","","# TODO: namespace threshold in 'nn'","- func: threshold(Tensor self, Scalar threshold, Scalar value) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  structured_delegate: threshold.out","  dispatch:","    QuantizedCPU: threshold_quantized_cpu","  tags: pointwise","","- func: threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","  structured_delegate: threshold.out","","- func: threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: threshold_out","    MPS: threshold_out_mps","","- func: threshold_backward.grad_input(Tensor grad_output, Tensor self, Scalar threshold, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: threshold_backward_out","    MPS: threshold_backward_out_mps","    SparseCPU, SparseCUDA: threshold_backward_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: threshold_backward_sparse_compressed_out","","- func: threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -\u003e Tensor","  variants: function","  structured_delegate: threshold_backward.grad_input","  dispatch:","    MkldnnCPU: mkldnn_relu_backward","    SparseCPU, SparseCUDA: threshold_backward_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: threshold_backward_sparse_compressed","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: threshold_backwards_nested","  tags: pointwise","","- func: tile(Tensor self, SymInt[] dims) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeImplicitAutograd: tile_symint","","- func: transpose.int(Tensor(a) self, int dim0, int dim1) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: transpose","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: transpose_nested","","- func: transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","","- func: _mkldnn_transpose(Tensor self, int dim0, int dim1) -\u003e Tensor","  device_check: NoCheck","  device_guard: False","  dispatch:","    MkldnnCPU: mkldnn_transpose","","- func: transpose_(Tensor(a!) self, int dim0, int dim1) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  tags: inplace_view","  dispatch:","    CompositeExplicitAutograd: transpose_","","- func: _mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -\u003e Tensor(a!)","  device_check: NoCheck","  device_guard: False","  dispatch:","    MkldnnCPU: mkldnn_transpose_","  autogen: _mkldnn_transpose.out","","- func: one_hot(Tensor self, int num_classes=-1) -\u003e Tensor","  python_module: nn","  variants: function","  tags: dynamic_output_shape","","- func: flip(Tensor self, int[] dims) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU, QuantizedCPU, CUDA, QuantizedCUDA: flip","    MPS: flip_mps","  autogen: flip.out","  tags: core","","- func: fliplr(Tensor self) -\u003e Tensor","  variants: function, method","","- func: flipud(Tensor self) -\u003e Tensor","  variants: function, method","","- func: roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU, MPS: roll","    CUDA: roll_cuda","  autogen: roll.out","","# default int[] value [0,1] should not add space after comma, since codegen parser uses ', ' to split args","","- func: rot90(Tensor self, int k=1, int[] dims=[0,1]) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: rot90","  autogen: rot90.out","","- func: trapezoid.x(Tensor y, Tensor x, *, int dim=-1) -\u003e Tensor","","- func: trapezoid.dx(Tensor y, *, Scalar dx=1, int dim=-1) -\u003e Tensor","","- func: trapz.x(Tensor y, Tensor x, *, int dim=-1) -\u003e Tensor","","- func: trapz.dx(Tensor y, *, float dx=1, int dim=-1) -\u003e Tensor","","# Fused implementation detail for transformers. Adds in-projection bias to QKV and divides Q by sqrt(D/num_heads).","- func: _transform_bias_rescale_qkv(Tensor qkv, Tensor qkv_bias, int num_heads) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CPU, NestedTensorCPU: transform_bias_rescale_qkv_cpu","    CUDA, NestedTensorCUDA: transform_bias_rescale_qkv_cuda","  autogen: _transform_bias_rescale_qkv.out","","- func: _nested_tensor_from_mask(Tensor t, Tensor mask, bool mask_check=True) -\u003e Tensor","  dispatch:","    CPU, CUDA: NestedTensor_nested_tensor_from_mask","  autogen: _nested_tensor_from_mask.out","","- func: _nested_tensor_from_mask_left_aligned(Tensor t, Tensor mask) -\u003e bool","  dispatch:","    CPU, CUDA: NestedTensor_nested_tensor_from_mask_left_aligned","","- func: _nested_from_padded(Tensor padded, Tensor cpu_nested_shape_example, bool fuse_transform_0213=False) -\u003e Tensor","  device_check: NoCheck # cpu_nested_shape_example will always be on CPU","  dispatch:","    CPU: nested_from_padded_generic","    CUDA: nested_from_padded_cuda","  autogen: _nested_from_padded.out","","# These private functions are temporary. They will be updated/deleted when nested tensors switch to using SymInts for their metadata representation","- func: _nested_tensor_size(Tensor self) -\u003e Tensor","  variants: method","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: _nested_tensor_size","  autogen: _nested_tensor_size.out","","- func: _nested_tensor_strides(Tensor self) -\u003e Tensor","  variants: method","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: _nested_tensor_strides","  autogen: _nested_tensor_strides.out","","- func: _nested_tensor_storage_offsets(Tensor self) -\u003e Tensor","  variants: method","  dispatch:","    NestedTensorCPU, NestedTensorCUDA, NestedTensorMeta: _nested_tensor_storage_offsets","  autogen: _nested_tensor_storage_offsets.out","","# _nested_from_padded is not usable from Python, so","# _nested_from_padded_and_nested_example is available for testing.","- func: _nested_from_padded_and_nested_example(Tensor padded, Tensor nt_example) -\u003e Tensor","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_from_padded_and_nested_example","  autogen: _nested_from_padded_and_nested_example.out","","# The input arguments' types to this functions are temporary. When nested tensors switch to using SymInts for their metadata representation","# this will need to be updated","- func: _nested_view_from_buffer(Tensor(a) self, Tensor nested_size, Tensor nested_strides, Tensor offsets) -\u003e Tensor(a)","  variants: function","  device_check: NoCheck","  dispatch:","    CPU, CUDA: _nested_view_from_buffer","","- func: _nested_view_from_buffer_copy(Tensor self, Tensor nested_size, Tensor nested_strides, Tensor offsets) -\u003e Tensor","  variants: function","  device_check: NoCheck","  tags: view_copy","  dispatch:","    CompositeExplicitAutogradNonFunctional: _nested_view_from_buffer_copy","  autogen: _nested_view_from_buffer_copy.out","","- func: _nested_view_from_jagged(Tensor(a) self, Tensor offsets, Tensor dummy, Tensor? lengths=None, int ragged_idx=1, Tensor? min_seqlen=None, Tensor? max_seqlen=None) -\u003e Tensor(a)","  variants: function","  device_check: NoCheck","  dispatch: {}","","- func: _nested_view_from_jagged_copy(Tensor self, Tensor offsets, Tensor dummy, Tensor? lengths=None, int ragged_idx=1, Tensor? min_seqlen=None, Tensor? max_seqlen=None) -\u003e Tensor","  variants: function","  device_check: NoCheck","  tags: view_copy","  dispatch:","    CompositeExplicitAutogradNonFunctional: _nested_view_from_jagged_copy","  autogen: _nested_view_from_jagged_copy.out","","- func: _nested_get_values(Tensor(a) self) -\u003e Tensor(a)","  variants: function","  device_check: NoCheck","  dispatch: {}","","- func: _nested_get_values_copy(Tensor self) -\u003e Tensor","  variants: function","  device_check: NoCheck","  tags: view_copy","  dispatch:","    CompositeExplicitAutogradNonFunctional: _nested_get_values_copy","  autogen: _nested_get_values_copy.out","","- func: _nested_get_offsets(Tensor self) -\u003e Tensor","  variants: function","  device_check: NoCheck","  dispatch: {}","","# returns undefined Tensor if no lengths present","- func: _nested_get_lengths(Tensor self) -\u003e Tensor","  variants: function","  device_check: NoCheck","  dispatch: {}","","- func: _nested_get_ragged_idx(Tensor self) -\u003e int","  variants: function","  device_check: NoCheck","  dispatch: {}","","- func: _nested_get_min_seqlen(Tensor self) -\u003e Tensor","  variants: function","  device_check: NoCheck","  dispatch: {}","","- func: _nested_get_max_seqlen(Tensor self) -\u003e Tensor","  variants: function","  device_check: NoCheck","  dispatch: {}","","- func: _nested_get_jagged_dummy(Tensor any) -\u003e Tensor","  category_override: dummy","  dispatch: {}","","- func: _nested_compute_contiguous_strides_offsets(Tensor nested_size) -\u003e (Tensor, Tensor)","  variants: function","  device_check: NoCheck","  dispatch:","    CPU, CUDA: _nested_compute_contiguous_strides_offsets","","- func: _trilinear(Tensor i1, Tensor i2, Tensor i3, int[] expand1, int[] expand2, int[] expand3, int[] sumdim, int unroll_dim=1) -\u003e Tensor","  dispatch:","    # calls unsqueeze","    CompositeExplicitAutogradNonFunctional: _trilinear","  autogen: _trilinear.out","","- func: triplet_margin_loss(Tensor anchor, Tensor positive, Tensor negative, float margin=1.0, float p=2, float eps=1e-06, bool swap=False, int reduction=Mean) -\u003e Tensor","","- func: trunc(Tensor self) -\u003e Tensor","  structured_delegate: trunc.out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: trunc_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: trunc_sparse_csr","  tags: [core, pointwise]","","- func: trunc_(Tensor(a!) self) -\u003e Tensor(a!)","  structured_delegate: trunc.out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: trunc_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: trunc_sparse_csr_","  tags: pointwise","","- func: trunc.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MPS: trunc_out","    SparseCPU, SparseCUDA, SparseMPS: trunc_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: trunc_sparse_csr_out","  tags: pointwise","# Alias for trunc","","- func: fix(Tensor self) -\u003e Tensor","  variants: function, method","","- func: fix_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: function, method","","- func: fix.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: type_as(Tensor self, Tensor other) -\u003e Tensor","  variants: method","","- func: _has_compatible_shallow_copy_type(Tensor self, Tensor from) -\u003e bool","  variants: function","","- func: _unique(Tensor self, bool sorted=True, bool return_inverse=False) -\u003e (Tensor, Tensor)","  variants: function","  dispatch:","    CPU: _unique_cpu","    CUDA: _unique_cuda","  autogen: _unique.out","","- func: unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -\u003e (Tensor, Tensor, Tensor)","  variants: function","  dispatch:","    CPU: unique_dim_cpu","    CUDA: unique_dim_cuda","  tags: dynamic_output_shape","  autogen: unique_dim.out","","- func: unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -\u003e (Tensor, Tensor, Tensor)","  variants: function","  dispatch:","    CPU: unique_consecutive_cpu","    CUDA: unique_consecutive_cuda","    MPS: unique_consecutive_mps","  tags: dynamic_output_shape","  autogen: unique_consecutive.out","","- func: unique_dim_consecutive(Tensor self, int dim, bool return_inverse=False, bool return_counts=False) -\u003e (Tensor, Tensor, Tensor)","  variants: function","  dispatch:","    CPU: unique_dim_consecutive_cpu","    CUDA: unique_dim_consecutive_cuda","    MPS: unique_dim_consecutive_mps","  tags: dynamic_output_shape","  autogen: unique_dim_consecutive.out","","# _unique and _unique_dim are fragile and modifying them easily cause internal break","# the below operator is a temporary hack for adding return_counts support","# Please don't rely on these two operators, they will be removed soon","","- func: _unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -\u003e (Tensor, Tensor, Tensor)","  variants: function","  dispatch:","    CPU: _unique2_cpu","    CUDA: _unique2_cuda","    MPS: _unique2_mps","  tags: dynamic_output_shape","  autogen: _unique2.out","","- func: _unsafe_view(Tensor self, SymInt[] size) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _unsafe_view","  autogen: _unsafe_view.out","","- func: unsqueeze(Tensor(a) self, int dim) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: unsqueeze","    SparseCPU, SparseCUDA: unsqueeze_sparse","    QuantizedCPU, QuantizedCUDA: unsqueeze_quantized","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: unsqueeze_nested","  tags: core","","- func: unsqueeze_(Tensor(a!) self, int dim) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  tags: inplace_view","  dispatch:","    CompositeExplicitAutograd: unsqueeze_","","- func: vander(Tensor x, int? N=None, bool increasing=False) -\u003e Tensor","","- func: var(Tensor self, bool unbiased=True) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  cpp_no_default_args: [\"unbiased\"]","","- func: var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  tags: core","  cpp_no_default_args: [\"unbiased\"]","","- func: var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CPU, CUDA: var","    MPS: var_mps","  tags: core","","- func: var.out(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  cpp_no_default_args: [\"unbiased\"]","","- func: var.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: var_out","","- func: var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  cpp_no_default_args: [\"unbiased\"]","","- func: var.names_out(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  cpp_no_default_args: [\"unbiased\"]","","- func: var.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: var.correction_names_out(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: var_mean(Tensor self, bool unbiased=True) -\u003e (Tensor, Tensor)","  device_check: NoCheck   # TensorIterator","  variants: function","  cpp_no_default_args: [\"unbiased\"]","","- func: var_mean.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -\u003e (Tensor, Tensor)","  device_check: NoCheck   # TensorIterator","  variants: function","  cpp_no_default_args: [\"unbiased\"]","","- func: var_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -\u003e (Tensor, Tensor)","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CPU, CUDA: var_mean","    MPS: var_mean_mps","  autogen: var_mean.correction_out","","- func: var_mean.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -\u003e (Tensor, Tensor)","  device_check: NoCheck   # TensorIterator","  variants: function","  cpp_no_default_args: [\"unbiased\"]","","- func: var_mean.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -\u003e (Tensor, Tensor)","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: view_as(Tensor(a) self, Tensor other) -\u003e Tensor(a)","  variants: method","  device_check: NoCheck","  device_guard: False","","- func: where.self(Tensor condition, Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CPU, CUDA, MPS, MTIA: where","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_where","  tags: [core, pointwise]","","- func: where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MPS, MTIA: where_self_out","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_where_out","","- func: where.ScalarSelf(Tensor condition, Scalar self, Tensor other) -\u003e Tensor","  variants: function","","- func: where.ScalarOther(Tensor condition, Tensor self, Scalar other) -\u003e Tensor","  variants: function, method","","- func: where.Scalar(Tensor condition, Scalar self, Scalar other) -\u003e Tensor","  variants: function","","- func: where(Tensor condition) -\u003e Tensor[]","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: norm_except_dim(Tensor v, int pow=2, int dim=0) -\u003e Tensor","  variants: function","","# VariableType::_weight_norm does not want to be given a gap in the autograd graph,","# so we don't define \"dispatch\" variants for it.","- func: _weight_norm(Tensor v, Tensor g, int dim=0) -\u003e Tensor","  variants: function","","- func: _weight_norm_interface(Tensor v, Tensor g, int dim=0) -\u003e (Tensor, Tensor)","  variants: function","  dispatch:","    CPU: weight_norm_cpu","    CUDA: weight_norm_cuda","    MPS: weight_norm_mps","  autogen: _weight_norm_interface.out","","- func: _weight_norm_interface_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -\u003e (Tensor, Tensor)","  variants: function","  dispatch:","    CPU: weight_norm_backward_cpu","    CUDA: weight_norm_backward_cuda","    MPS: weight_norm_backward_mps","  autogen: _weight_norm_interface_backward.out","","- func: _weight_norm_differentiable_backward(Tensor grad_w, Tensor saved_v, Tensor saved_g, Tensor saved_norms, int dim) -\u003e (Tensor, Tensor)","  variants: function","","- func: zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: zeros","  autogen: zeros.names_out","","- func: _efficientzerotensor(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CPU: _efficientzerotensor","    CUDA: _efficientzerotensor_cuda","    MPS: _efficientzerotensor_mps","    Meta: _efficientzerotensor_meta_symint","  autogen: _efficientzerotensor.out","","- func: zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: zeros_symint","","- func: zeros.out(SymInt[] size, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: zeros_out","    SparseCPU, SparseCUDA, SparseMeta: zeros_sparse_out","","- func: zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -\u003e Tensor","  dispatch:","    # NB: Although this composite mutates on the inside, it is","    # non-differentiable so NonFunctional doesn't apply","    CompositeExplicitAutograd, CompositeImplicitAutogradNestedTensor: zeros_like","  autogen: zeros_like.out","","- func: _standard_gamma_grad(Tensor self, Tensor output) -\u003e Tensor","  variants: function","  dispatch:","    CPU: _standard_gamma_grad_cpu","    CUDA: _standard_gamma_grad_cuda","  autogen: _standard_gamma_grad.out","","- func: _standard_gamma(Tensor self, Generator? generator=None) -\u003e Tensor","  variants: function","  dispatch:","    CPU: _s_gamma_cpu","    CUDA: _s_gamma_cuda","  tags: nondeterministic_seeded","  autogen: _standard_gamma.out","","- func: _dirichlet_grad(Tensor x, Tensor alpha, Tensor total) -\u003e Tensor","  dispatch:","    CPU: _dirichlet_grad_cpu","    CUDA: _dirichlet_grad_cuda","  autogen: _dirichlet_grad.out","","- func: _sample_dirichlet(Tensor self, Generator? generator=None) -\u003e Tensor","  tags: nondeterministic_seeded","  variants: function","  dispatch:","    CPU: _s_dirichlet_cpu","    CUDA: _s_dirichlet_cuda","  autogen: _sample_dirichlet.out","","- func: poisson(Tensor self, Generator? generator=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU: _s_poisson_cpu","    CUDA: _s_poisson_cuda","  tags: nondeterministic_seeded","  autogen: poisson.out","","- func: binomial(Tensor count, Tensor prob, Generator? generator=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU: _s_binomial_cpu","    CUDA: _s_binomial_cuda","  tags: nondeterministic_seeded","  autogen: binomial.out","","# When more variants get ported to native, this dispatch will get more","# complicated","","- func: native_norm(Tensor self, Scalar p=2) -\u003e Tensor","  dispatch:","    SparseCPU, SparseCUDA: norm_sparse","  autogen: native_norm.out","","- func: native_norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, ScalarType? dtype) -\u003e Tensor","  dispatch:","    SparseCPU, SparseCUDA: norm_sparse","  autogen: native_norm.ScalarOpt_dim_dtype_out","","- func: _batch_norm_with_update(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, float momentum, float eps) -\u003e (Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CPU: _batch_norm_with_update_cpu","    CUDA: _batch_norm_with_update_cuda","    MPS: _batch_norm_with_update_mps","    MkldnnCPU: _batch_norm_with_update_mkldnn","  autogen: _batch_norm_with_update_functional","","- func: _batch_norm_with_update.out(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, float momentum, float eps, *, Tensor(d!) out, Tensor(e!) save_mean, Tensor(f!) save_invstd, Tensor(g!) reserve) -\u003e (Tensor(d!), Tensor(e!), Tensor(f!), Tensor(g!))","  dispatch:","    CPU: _batch_norm_with_update_cpu_out","    CUDA: _batch_norm_with_update_cuda_out","    MPS: _batch_norm_with_update_mps_out","","- func: _batch_norm_no_update(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, float momentum, float eps) -\u003e (Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CompositeExplicitAutograd: _batch_norm_no_update","  autogen: _batch_norm_no_update.out","","- func: batch_norm_backward(Tensor grad_out, Tensor input, Tensor weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var, bool update, float eps, bool[3] output_mask, Tensor reserve) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CPU: _new_batch_norm_backward_cpu","    CUDA: _new_batch_norm_backward_cuda","    MPS: _new_batch_norm_backward_mps","    MkldnnCPU: _new_batch_norm_backward_mkldnn","","# TODO: reduce signatures down to one when optional args is available","- func: _sparse_sum(Tensor self) -\u003e Tensor","","- func: _sparse_sum.dtype(Tensor self, *, ScalarType dtype) -\u003e Tensor","","- func: _sparse_sum.dim(Tensor self, int[1] dim) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _sparse_sum","  autogen: _sparse_sum.dim_out","","- func: _sparse_sum.dim_dtype(Tensor self, int[1] dim, *, ScalarType dtype) -\u003e Tensor","","- func: _sparse_sum_backward(Tensor grad, Tensor self, int[] dim) -\u003e Tensor","  dispatch:","    SparseCPU: _sparse_sum_backward_cpu","    SparseCUDA: _sparse_sum_backward_cuda","  autogen: _sparse_sum_backward.out","","- func: _sparse_csr_sum.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  dispatch:","    SparseCsrCPU: _sparse_csr_sum_cpu","    SparseCsrCUDA: _sparse_csr_sum_cuda","  autogen: _sparse_csr_sum.dim_dtype_out","","- func: _sparse_csr_prod.dim_dtype(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  dispatch:","    SparseCsrCPU: _sparse_csr_prod_cpu","    SparseCsrCUDA: _sparse_csr_prod_cuda","  autogen: _sparse_csr_prod.dim_dtype_out","","- func: _sparse_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -\u003e Tensor","  python_module: sparse","  variants: function","","- func: _sparse_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor","  python_module: sparse","  variants: function","","- func: _sparse_softmax(Tensor self, int dim, bool half_to_float) -\u003e Tensor","  python_module: sparse","  dispatch:","    SparseCPU: softmax_sparse_cpu","    SparseCUDA: softmax_sparse_cuda","  autogen: _sparse_softmax.out","","- func: _sparse_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -\u003e Tensor","  dispatch:","    SparseCPU: softmax_backward_sparse_cpu","    SparseCUDA: softmax_backward_sparse_cuda","  autogen: _sparse_softmax_backward_data.out","","- func: _sparse_log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -\u003e Tensor","  python_module: sparse","  variants: function","","- func: _sparse_log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor","  python_module: sparse","  variants: function","","- func: _sparse_log_softmax(Tensor self, int dim, bool half_to_float) -\u003e Tensor","  python_module: sparse","  dispatch:","    SparseCPU: log_softmax_sparse_cpu","    SparseCUDA: log_softmax_sparse_cuda","  autogen: _sparse_log_softmax.out","","- func: _sparse_log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, Tensor self) -\u003e Tensor","  dispatch:","    SparseCPU: log_softmax_backward_sparse_cpu","    SparseCUDA: log_softmax_backward_sparse_cuda","  autogen: _sparse_log_softmax_backward_data.out","","- func: _spdiags(Tensor diagonals, Tensor offsets, int[] shape, Layout? layout=None) -\u003e Tensor","  python_module: sparse","  dispatch:","    CPU: spdiags","  autogen: _spdiags.out","","- func: norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: norm","  autogen: norm.ScalarOpt_dtype_out","","- func: norm.Scalar(Tensor self, Scalar p=2) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: norm","  autogen: norm.Scalar_out","","- func: norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -\u003e Tensor","  structured_delegate: norm.dtype_out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA: sparse_dtype_norm","","- func: norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -\u003e Tensor","  structured_delegate: norm.out","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA: sparse_norm","","- func: norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: norm_dtype_out","    MPS: norm_dtype_out_mps","","- func: norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: norm_out","    MPS: norm_out_mps","","# These four redispatch in their implementation, so OK to be CompositeImplicitAutograd","- func: norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: norm.names_dtype_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: norm.names_out(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: frexp.Tensor(Tensor self) -\u003e (Tensor mantissa, Tensor exponent)","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: frexp","  tags: pointwise","","- func: frexp.Tensor_out(Tensor self, *, Tensor(a!) mantissa, Tensor(b!) exponent) -\u003e (Tensor(a!) mantissa, Tensor(b!) exponent)","  dispatch:","    CPU, CUDA: frexp_out","  tags: pointwise","","# Deprecated (v.1.12)","- func: frobenius_norm.dim(Tensor self, int[1] dim, bool keepdim=False) -\u003e Tensor","  variants: function","","# Deprecated (v.1.12)","- func: frobenius_norm.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","","# Deprecated (v.1.12)","- func: nuclear_norm(Tensor self, bool keepdim=False) -\u003e Tensor","  variants: function","","# Deprecated (v.1.12)","- func: nuclear_norm.out(Tensor self, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","","# Deprecated (v.1.12)","- func: nuclear_norm.dim(Tensor self, int[2] dim, bool keepdim=False) -\u003e Tensor","  variants: function","","# Deprecated (v.1.12)","- func: nuclear_norm.dim_out(Tensor self, int[2] dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","","- func: clone(Tensor self, *, MemoryFormat? memory_format=None) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: clone","    SparseCPU, SparseCUDA, SparseMPS: clone_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: clone_sparse_compressed","    MkldnnCPU: mkldnn_clone","    QuantizedCPU, QuantizedCUDA: quantized_clone","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: clone_nested","  autogen: clone.out","  tags: [core, pointwise]","","- func: positive(Tensor(a) self) -\u003e Tensor(a)","  variants: function, method","  tags: pointwise","","- func: resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -\u003e Tensor(a!)","  use_const_ref_for_mutable_tensors: True","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: resize_as_","  autogen: resize_as, resize_as.out","  tags: inplace_view","","- func: resize_as_sparse_(Tensor(a!) self, Tensor the_template) -\u003e Tensor(a!)","  use_const_ref_for_mutable_tensors: True","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA: resize_as_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: resize_as_sparse_compressed_","  autogen: resize_as_sparse, resize_as_sparse.out","","- func: zero_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CPU, CUDA: zero_","    MPS: zero_mps_","    Meta: zero_meta_","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: zero_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: zero_sparse_csr_","    MkldnnCPU: mkldnn_zero_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: zero_nested_","  autogen: zero, zero.out","","- func: sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: sub_out","    MPS: sub_out_mps","    MTIA: sub_out_mtia","    SparseCPU, SparseCUDA: sub_out_sparse","  tags: pointwise","","- func: sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: sub.out","  dispatch:","    SparseCPU, SparseCUDA: sub_sparse","    ZeroTensor: sub_zerotensor","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_sub_Tensor","  tags: [core, pointwise]","","- func: sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: sub.out","  dispatch:","    SparseCPU, SparseCUDA: sub_sparse_","  tags: pointwise","# For C++ only, until we have conversion from C++ numbers to Tensor","","- func: sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: sub","  tags: [core, pointwise]","","- func: sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: sub_","  autogen: sub.Scalar_out","  tags: pointwise","# subtract, alias for sub","","- func: subtract.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","","- func: subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -\u003e Tensor","  variants: function, method","","- func: subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -\u003e Tensor(a!)","  variants: method","","# For C++ only, until we have conversion from C++ numbers to Tensor","- func: subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -\u003e Tensor","  variants: function, method","","- func: subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -\u003e Tensor(a!)","  variants: method","","- func: rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CPU, CUDA, MPS, MTIA: rsub","  autogen: rsub.Tensor_out","","- func: heaviside.out(Tensor self, Tensor values, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: heaviside_out","  tags: pointwise","","- func: heaviside(Tensor self, Tensor values) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: heaviside.out","  tags: pointwise","","- func: heaviside_(Tensor(a!) self, Tensor values) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: heaviside.out","","# For C++ only, until we have conversion from C++ numbers to Tensor","- func: rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: rsub","  autogen: rsub.Scalar_out","","# Functionally the same as addmm, but we give it a different derivative formula","# that doesn't propagate gradients to non-present entries on sparse.","  tags: pointwise","- func: _sparse_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor","  python_module: sparse","  dispatch:","    CompositeExplicitAutograd: _sparse_addmm","  autogen: _sparse_addmm.out","","- func: sparse_sampled_addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: sparse","  dispatch:","    SparseCsrCUDA: sparse_sampled_addmm_out_sparse_csr_cuda","    SparseCsrCPU: sparse_sampled_addmm_out_sparse_csr_cpu","","- func: sparse_sampled_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor","  python_module: sparse","  dispatch:","    SparseCsrCUDA: sparse_sampled_addmm_sparse_csr_cuda","    SparseCsrCPU: sparse_sampled_addmm_sparse_csr_cpu","","- func: _sparse_mm_reduce_impl(Tensor self, Tensor other, str reduce) -\u003e (Tensor, Tensor)","  python_module: sparse","  dispatch:","    SparseCsrCPU: _sparse_mm_reduce_impl_sparse_csr_cpu","","- func: _sparse_mm_reduce_impl_backward(Tensor self, Tensor grad_out, Tensor weight, str reduce, Tensor arg_out, bool[2] output_mask) -\u003e (Tensor, Tensor)","  python_module: sparse","  dispatch:","    SparseCsrCPU: _sparse_mm_reduce_impl_backward_sparse_csr_cpu","","- func: addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: addmm_out_cpu","    CUDA: addmm_out_cuda","    MPS: addmm_out_mps","    XPU: addmm_out_xpu","    MTIA: addmm_out_mtia","    SparseCPU: addmm_out_sparse_dense_cpu","    SparseCUDA: addmm_out_sparse_dense_cuda","    SparseCsrCPU: addmm_out_sparse_compressed_cpu","    SparseCsrCUDA: addmm_out_sparse_compressed_cuda","","- func: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor","  structured_delegate: addmm.out","  variants: function, method","  dispatch:","    SparseCPU: addmm_sparse_dense_cpu","    SparseCUDA: addmm_sparse_dense_cuda","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: addmm_sparse_compressed_dense","  tags: core","","- func: addmm.dtype(Tensor self, Tensor mat1, Tensor mat2, ScalarType out_dtype, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor","  dispatch:","    CUDA: _addmm_dtype_cuda","","- func: addmm.dtype_out(Tensor self, Tensor mat1, Tensor mat2, ScalarType out_dtype, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CUDA: _addmm_dtype_out_cuda","","- func: addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor(a!)","  structured_delegate: addmm.out","  variants: method","  dispatch:","    # Warning!  For whatever reason, the inplace sparse addmm is NON","    # broadcasting","    SparseCPU: s_addmm_sparse_dense_cpu_","    SparseCUDA: s_addmm_sparse_dense_cuda_","","- func: _addmm_activation.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: addmm_activation_out_cpu","    CUDA: addmm_activation_out_cuda","    XPU: addmm_activation_out_xpu","","- func: _addmm_activation(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False) -\u003e Tensor","  structured_delegate: _addmm_activation.out","  variants: function, method","","- func: _scaled_mm(Tensor self, Tensor mat2, Tensor scale_a, Tensor scale_b, Tensor? bias=None, Tensor? scale_result=None, ScalarType? out_dtype=None, bool use_fast_accum=False) -\u003e Tensor","  variants: function","  dispatch:","    CPU: _scaled_mm_cpu","    CUDA: _scaled_mm_cuda","  tags: needs_exact_strides","","- func: _scaled_mm.out(Tensor self, Tensor mat2, Tensor scale_a, Tensor scale_b, Tensor? bias=None, Tensor? scale_result=None, ScalarType? out_dtype=None, bool use_fast_accum=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CPU: _scaled_mm_out_cpu","    CUDA: _scaled_mm_out_cuda","  tags: needs_exact_strides","","","- func: _scaled_grouped_mm(Tensor self, Tensor mat2, Tensor scale_a, Tensor scale_b, Tensor? offs=None, Tensor? bias=None, Tensor? scale_result=None, ScalarType? out_dtype=None, bool use_fast_accum=False) -\u003e Tensor","  variants: function","  dispatch:","    CUDA: _scaled_grouped_mm_cuda","  tags: needs_exact_strides","","- func: _grouped_mm(Tensor self, Tensor mat2, Tensor? offs=None, Tensor? bias=None, ScalarType? out_dtype=None) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: _grouped_mm","    CUDA: _grouped_mm_cuda","","# NOTE [ Sparse: autograd and API ]","#","#","# Sparse Tensor Constructors","# ~~~~~~~~~~~~~~~~~~~~~~~~~~","#","# The API entry points to sparse tensor construction should be","# `sparse_coo tensor` and `_sparse_coo_tensor_unsafe`. Depending on whether the","# indices and values tensors are given, they eventually dispatch to either","# `sparse_coo_tensor_with_dims` or `sparse_coo_tensor_with_dims_and_tensors`.","#","# The autograd support for ctor is implement on `sparse_coo_tensor_with_dims_and_tensors`.","#","# The API methods `sparse_coo tensor` and `_sparse_coo_tensor_unsafe`","# **must not** have specific type dispatches because otherwise codegen will","# consider them as abstract methods (see Note [Abstract ATen methods]), dispatch","# using **Tensor** type, and thus lose autograd tracking on the actual method","# they dispatch to, e.g., `sparse_coo_tensor_with_dims_and_tensors`.","#","#","# Sparse Methods API Design","# ~~~~~~~~~~~~~~~~~~~~~~~~~","#","# Goals: 1. Flexible API for users to write custom sparse ops","#        2. ctor and member accessor with autograd support","#","# To achieve 1, we need to provide a set of *dangerous* APIs (dangerous in the","# sense that misusing them will break sparse tensor invariant and may out in","# unexpected behavior, e.g., crash). These methods are all prefixed with","# underscore \"_\" to indicate that they should be used with care. We provide:","#","#   + `_indices()`: returns the *raw* indices within the sparse tensor (not just","#                   sharing storage). Any inplace operation will change the","#                   actual indices, including t_, set_, as_strided_, resize_,","#                   etc.","#   + `_values()`: returns the *raw* values within the sparse tensor. Similar","#                  semantics as `_indices()`","#   + `_nnz()`: returns the number of non-zero entries. This will always be","#               determined by the shapes of indices and values.","#   + `_coalesced_(bool)`: inplace sets whether the tensor is coalesced, and","#                          returns itself.","#","# These methods are very useful in writing new operations, e.g., a custom","# autograd Function.","#","# We also provide other public *safe* APIs:","#   + `indices()`: returns a **view** of the indices tensor if the sparse tensor","#                  is **coalesced**.","#   + `values()`: returns a **view** of the values tensor if the containing","#                 sparse tensor is **coalesced**.","#   + `sparse_dim()`: number of sparse dimensions","#   + `dense_dim()`: number of dense dimensions","#   + `is_coalesced()`: whether the sparse tensor is coalesced","#","# `_indices()` and `_values()` should returns the raw indices and values dense","# tensors within a sparse tensor. They can be quite unsafe with inplace","# operations like `t_()`, and exposes uncoalesced indices and values. The public","# recommended API is `indices()` and `values()`, both of which first check that","# the tensor is coalesced and return views on those tensors.","#","#","# Autograd Support","# ~~~~~~~~~~~~~~~~","#","# Autograd is supported on `values()` and sparse tensor ctor with indices and","# values tensors. E.g., `torch.sparse_coo_tensor(i, v).values().sum()` is","# differentiable w.r.t. `v`.","#","# NB: The `values()` and `_values()` operators are special in that they are","# layout-aware, i.e., the output depends not just on the data it represents, but","# also on the input layout details (in this case, the `indices` tensor). See","# NOTE [ as_strided Backward and layout-aware/agnostic autograd ] in Functions.cpp","# for discussion on layout-aware vs layout-agnostic autograd. Since PyTorch ops","# operate in the layout-agnostic mode, similar to `as_strided`, backward of","# these two operators need to consider them in a layout-agnostic way:","#   + `values()`:","#     Input is coalesced.","#     We just pretend having `input.indices()` as an additional argument","#     `input_indices`, then forward is similar to","#     `input.to(kStrided).index_select(input_indices)` regardless of the layout.","#     Note that `values()` normally is layout-aware even if we constrain","#     ourselves on sparse inputs since it may include all zeros values entries","#     as \"present\" entries.","#   + `_values()`:","#     Input may be uncoalesced.","#     It is not straightforward to construct a layout-agnostic version because","#     duplicate indices entries may exist and additional parameterization is","#     needed to distribute the value into different values entries. Furthermore,","#     this op is intended to provide ways to write custom sparse ops, rather","#     than being used in autograd graph, so it is marked as *non-differentiable*","#     in derivatives.yaml.","#","# Before reading the following, see NOTE [ Autograd Variable Views ] in","# variable.h for details on views that are tracked by autograd, and views that","# are not.","#","# Moreover, these methods return tensors that share storage with inputs, so we","# mark these methods as view ops to support autograd history tracking.","# The sparse tensor ctor output should technically be view of both input indices","# and values tensors, but currently we only support setting as view of a single","# Variable, so it is only view of the values tensor.","# TODO: clone indices in sparse tensor ctor.","#","# For other methods that return outputs that share storage with inputs, i.e.,","# `indices()` and `_indices()`. We mark their outputs as non-differentiable, so","# the view relation is not tracked by autograd, but the version counter is still","# shared. In other words, their outputs are non-differentiable views of the","# sparse tensor.","# FIXME: would be nicer if TensorOptions was optional based; not adding default arguments for options given","# the default would never make sense.","","- func: _sparse_compressed_tensor_with_dims(int nnz, int dense_dim, int[] size, int[] blocksize, ScalarType index_dtype, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: sparse_compressed_tensor_with_dims","","- func: sparse_compressed_tensor.comp_plain_value_size(Tensor compressed_indices, Tensor plain_indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: sparse_compressed_tensor","","- func: sparse_csr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","- func: sparse_csc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","- func: sparse_bsr_tensor.crow_col_value_size(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","- func: sparse_bsc_tensor.ccol_row_value_size(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","","- func: sparse_compressed_tensor.comp_plain_value(Tensor compressed_indices, Tensor plain_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: sparse_compressed_tensor","- func: sparse_csr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","- func: sparse_csc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","- func: sparse_bsr_tensor.crow_col_value(Tensor crow_indices, Tensor col_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","- func: sparse_bsc_tensor.ccol_row_value(Tensor ccol_indices, Tensor row_indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","","- func: _sparse_compressed_tensor_unsafe(Tensor compressed_indices, Tensor plain_indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: _sparse_compressed_tensor_unsafe_symint","","- func: _sparse_csr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","- func: _sparse_csc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","- func: _sparse_bsr_tensor_unsafe(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","- func: _sparse_bsc_tensor_unsafe(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","","- func: sparse_coo_tensor.size(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: sparse_coo_tensor","  autogen: sparse_coo_tensor.size_out","","- func: sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -\u003e Tensor","","- func: sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -\u003e Tensor","","- func: _sparse_coo_tensor_unsafe(Tensor indices, Tensor values, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool? is_coalesced=None) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: _sparse_coo_tensor_unsafe_symint","","- func: _validate_sparse_coo_tensor_args(Tensor indices, Tensor values, int[] size, bool? is_coalesced=None, bool? check_pinning=None) -\u003e ()","","- func: _validate_sparse_compressed_tensor_args(Tensor compressed_indices, Tensor plain_indices, Tensor values, int[] size, Layout layout, bool? check_pinning=None) -\u003e ()","- func: _validate_sparse_csr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, bool? check_pinning=None) -\u003e ()","- func: _validate_sparse_csc_tensor_args(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, bool? check_pinning=None) -\u003e ()","- func: _validate_sparse_bsr_tensor_args(Tensor crow_indices, Tensor col_indices, Tensor values, int[] size, bool? check_pinning=None) -\u003e ()","- func: _validate_sparse_bsc_tensor_args(Tensor ccol_indices, Tensor row_indices, Tensor values, int[] size, bool? check_pinning=None) -\u003e ()","","- func: _sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False) -\u003e Tensor","  dispatch:","    SparseCPU, SparseCUDA, SparseMeta, SparseMPS, Meta: new_with_dims_sparse","  autogen: _sparse_coo_tensor_with_dims.out","","- func: _sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, SymInt[] size, Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? is_coalesced=None) -\u003e Tensor","  dispatch:","    SparseCPU, SparseCUDA, SparseMeta, SparseMPS, Meta: new_with_dims_and_tensor_sparse_symint","  autogen: _sparse_coo_tensor_with_dims_and_tensors.out","","- func: sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -\u003e Tensor(a!)","  use_const_ref_for_mutable_tensors: True","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: sparse_resize_","  autogen: sparse_resize, sparse_resize.out","","- func: sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -\u003e Tensor(a!)","  use_const_ref_for_mutable_tensors: True","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: sparse_resize_and_clear_","  autogen: sparse_resize_and_clear, sparse_resize_and_clear.out","","- func: sparse_mask(Tensor self, Tensor mask) -\u003e Tensor","  variants: method","  dispatch:","    SparseCPU, SparseCUDA: sparse_mask","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sparse_mask_sparse_compressed","  autogen: sparse_mask.out","","- func: _sparse_mask_projection(Tensor self, Tensor mask, bool accumulate_matches=False) -\u003e Tensor","  variants: method","  dispatch:","    SparseCPU, SparseCUDA: sparse_mask_projection","  autogen: _sparse_mask_projection.out","","- func: _to_cpu(Tensor[] tensors) -\u003e Tensor[]","  variants: function","","- func: to_dense(Tensor self, ScalarType? dtype=None, *, bool? masked_grad=None) -\u003e Tensor","  variants: method","","# Special case of to_dense with custom derivative","- func: _to_dense(Tensor self, ScalarType? dtype=None, bool? masked_grad=None) -\u003e Tensor","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: sparse_to_dense","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: sparse_compressed_to_dense","    MkldnnCPU: mkldnn_to_dense","  autogen: _to_dense.out","","- func: to_dense_backward(Tensor grad, Tensor input, bool? masked_grad=None) -\u003e Tensor","","- func: sparse_dim(Tensor self) -\u003e int","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: sparse_dim_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: sparse_dim_sparse_csr","    CompositeExplicitAutograd: sparse_dim_default","  device_check: NoCheck","  device_guard: False","","# legacy method","- func: _dimI(Tensor self) -\u003e int","  variants: method","  dispatch:","    SparseCPU, SparseCUDA: sparse_dim_sparse","  device_check: NoCheck","  device_guard: False","","- func: dense_dim(Tensor self) -\u003e int","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: dense_dim_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: dense_dim_sparse_csr","    CompositeExplicitAutograd: dense_dim_default","  device_check: NoCheck","  device_guard: False","","# legacy method","- func: _dimV(Tensor self) -\u003e int","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMeta: dense_dim_sparse","  device_check: NoCheck","  device_guard: False","","- func: _nnz(Tensor self) -\u003e int","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: _nnz_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMPS, SparseCsrMeta: _nnz_sparse_csr","  device_check: NoCheck","  device_guard: False","","# NOTE: [ coalesce autograd ]","# coalesce returns self directly for already coalesced sparse tensors.","# This means coalesce cannot have a derivative registered, otherwise it creates","# circular references in the autograd graph (see gh-52874).","# Instead, the derivative is registered on the slow-path \"_coalesce\"","- func: coalesce(Tensor(a) self) -\u003e Tensor(a)","  variants: method","","- func: _coalesce(Tensor self) -\u003e Tensor","  dispatch:","    SparseCPU: _coalesce_sparse_cpu","    SparseCUDA: _coalesce_sparse_cuda","    SparseMPS: _coalesce_sparse_mps","  autogen: _coalesce.out","","- func: is_coalesced(Tensor self) -\u003e bool","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: is_coalesced_sparse","    CompositeExplicitAutograd: is_coalesced_default","  device_check: NoCheck","  device_guard: False","","- func: _indices(Tensor(a) self) -\u003e Tensor(a)","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: _indices_sparse","  device_check: NoCheck","  device_guard: False","","- func: _values(Tensor(a) self) -\u003e Tensor(a)","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: _values_sparse","  device_check: NoCheck","  device_guard: False","","# This method doesn't do any check but only directly sets the flag. So it can be","# a bit unsafe. Similar to _indices and _values, this is useful for implementing","# custom sparse operations in Python/C++ extension.","- func: _coalesced_(Tensor(a!) self, bool coalesced) -\u003e Tensor(a!)","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: _coalesced_sparse_","  device_check: NoCheck","  device_guard: False","  autogen: _coalesced, _coalesced.out","","- func: indices(Tensor(a) self) -\u003e Tensor(a)","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: indices_sparse","    CompositeExplicitAutograd: indices_default","  device_check: NoCheck","  device_guard: False","","- func: values(Tensor(a) self) -\u003e Tensor(a)","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: values_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: values_sparse_csr","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: values_nested","    CompositeExplicitAutograd: values_default","  device_check: NoCheck","  device_guard: False","","- func: crow_indices(Tensor(a) self) -\u003e Tensor(a)","  variants: method","  dispatch:","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: crow_indices_sparse_csr","    CompositeExplicitAutograd: crow_indices_default","  device_check: NoCheck","  device_guard: False","","- func: col_indices(Tensor(a) self) -\u003e Tensor(a)","  variants: method","  dispatch:","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: col_indices_sparse_csr","    CompositeExplicitAutograd: col_indices_default","  device_check: NoCheck","  device_guard: False","","- func: ccol_indices(Tensor(a) self) -\u003e Tensor(a)","  variants: method","  dispatch:","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: ccol_indices_sparse_csr","    CompositeExplicitAutograd: ccol_indices_default","  device_check: NoCheck","  device_guard: False","","- func: row_indices(Tensor(a) self) -\u003e Tensor(a)","  variants: method","  dispatch:","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: row_indices_sparse_csr","    CompositeExplicitAutograd: row_indices_default","  device_check: NoCheck","  device_guard: False","","- func: hspmm.out(Tensor mat1, Tensor mat2, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    SparseCPU: hspmm_out_sparse_cpu","    SparseCUDA: hspmm_out_sparse_cuda","","- func: hspmm(Tensor mat1, Tensor mat2) -\u003e Tensor","  dispatch:","    SparseCPU: hspmm_sparse_cpu","    SparseCUDA: hspmm_sparse_cuda","","- func: copy_sparse_to_sparse_(Tensor(a!) self, Tensor src, bool non_blocking=False) -\u003e Tensor(a!)","  device_check: NoCheck  # Allows copy into different device","  variants: function","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS, SparseMeta: copy_sparse_","  autogen: copy_sparse_to_sparse, copy_sparse_to_sparse.out","","# By adding the AutogradNestedTensor this makes this function CompositeImplicit-like for nested tensors","- func: unbind.int(Tensor(a -\u003e *) self, int dim=0) -\u003e Tensor(a)[]","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: unbind","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_unbind","","- func: unbind.Dimname(Tensor(a -\u003e *) self, Dimname dim) -\u003e Tensor(a)[]","  variants: function, method","","- func: to_sparse.sparse_dim(Tensor self, int sparse_dim) -\u003e Tensor","  variants: method","","# Special case of to_sparse.sparse_dim with custom derivative","- func: _to_sparse.sparse_dim(Tensor self, int sparse_dim) -\u003e Tensor","  variants: method","  dispatch:","    CPU, CUDA, MPS: dense_to_sparse","    SparseCPU, SparseCUDA, SparseMPS: sparse_coo_to_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta, SparseCsrMPS: sparse_compressed_to_sparse","  autogen: _to_sparse.sparse_dim_out","","- func: to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -\u003e Tensor","  variants: method","","# Special case of to_sparse with custom derivative","- func: _to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -\u003e Tensor","  variants: method","  dispatch:","    CPU, CUDA, MPS: dense_to_sparse","    SparseCPU, SparseCUDA, SparseMPS: sparse_coo_to_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sparse_compressed_to_sparse","  autogen: _to_sparse.out","","- func: to_sparse_csr(Tensor self, int? dense_dim=None) -\u003e Tensor","  variants: method","","# Special case of to_sparse_csr with custom derivative","- func: _to_sparse_csr(Tensor self, int? dense_dim=None) -\u003e Tensor","  variants: method","  dispatch:","    CPU, CUDA: dense_to_sparse_csr","    SparseCPU, SparseCUDA: coo_to_sparse_csr","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sparse_compressed_to_sparse_csr","  autogen: _to_sparse_csr.out","","- func: to_sparse_csc(Tensor self, int? dense_dim=None) -\u003e Tensor","  variants: method","","# Special case of to_sparse_csc with custom derivative","- func: _to_sparse_csc(Tensor self, int? dense_dim=None) -\u003e Tensor","  variants: method","  dispatch:","    CPU, CUDA: dense_to_sparse_csc","    SparseCPU, SparseCUDA: coo_to_sparse_csc","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sparse_compressed_to_sparse_csc","  autogen: _to_sparse_csc.out","","- func: to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -\u003e Tensor","  variants: method","","# Special case of to_sparse_bsr with custom derivative","- func: _to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -\u003e Tensor","  variants: method","  dispatch:","    CPU, CUDA: dense_to_sparse_bsr","    SparseCPU, SparseCUDA: coo_to_sparse_bsr","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sparse_compressed_to_sparse_bsr","  autogen: _to_sparse_bsr.out","","- func: to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -\u003e Tensor","  variants: method","","# Special case of to_sparse_bsc with custom derivative","- func: _to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -\u003e Tensor","  variants: method","  dispatch:","    CPU, CUDA: dense_to_sparse_bsc","    SparseCPU, SparseCUDA: coo_to_sparse_bsc","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sparse_compressed_to_sparse_bsc","  autogen: _to_sparse_bsc.out","","- func: _to_sparse_semi_structured(Tensor dense) -\u003e (Tensor, Tensor)","  variants: function","  dispatch:","    CUDA: _to_sparse_semi_structured","","- func: to_mkldnn(Tensor self, ScalarType? dtype=None) -\u003e Tensor","  variants: method","  dispatch:","    CPU: dense_to_mkldnn","  autogen: to_mkldnn.out","","- func: mkldnn_reorder_conv2d_weight(Tensor self, SymInt[2] padding=0, SymInt[2] stride=1, SymInt[2] dilation=1, SymInt groups=1, SymInt[]? input_size=None) -\u003e Tensor","  variants: function","  python_module: nn","  dispatch:","    MkldnnCPU: mkldnn_reorder_conv2d_weight","  autogen: mkldnn_reorder_conv2d_weight.out","","- func: mkldnn_reorder_conv3d_weight(Tensor self, SymInt[3] padding=0, SymInt[3] stride=1, SymInt[3] dilation=1, SymInt groups=1, SymInt[]? input_size=None) -\u003e Tensor","  variants: function","  python_module: nn","  dispatch:","    MkldnnCPU: mkldnn_reorder_conv3d_weight","  autogen: mkldnn_reorder_conv3d_weight.out","","- func: to_mkldnn_backward(Tensor grad, Tensor input) -\u003e Tensor","","- func: quantize_per_tensor_dynamic(Tensor self, ScalarType dtype, bool reduce_range) -\u003e Tensor","  variants: function","  dispatch:","    CPU, CUDA: quantize_per_tensor_dynamic","  autogen: quantize_per_tensor_dynamic.out","","- func: quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -\u003e Tensor","  variants: function","  dispatch:","    CPU, CUDA: quantize_per_tensor","  autogen: quantize_per_tensor.out","","- func: quantize_per_tensor.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, ScalarType dtype) -\u003e Tensor","  variants: function","  dispatch:","    CPU, CUDA: quantize_per_tensor_tensor_qparams","  autogen: quantize_per_tensor.tensor_qparams_out","","- func: quantize_per_tensor.tensors(Tensor[] tensors, Tensor scales, Tensor zero_points, ScalarType dtype) -\u003e Tensor[]","  variants: function","  dispatch:","    CPU: quantize_per_tensor_list_cpu","  autogen: quantize_per_tensor.tensors_out","","- func: quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -\u003e Tensor","  variants: function","  dispatch:","    CPU, CUDA: quantize_per_channel","  autogen: quantize_per_channel.out","","- func: dequantize.self(Tensor self) -\u003e Tensor","  variants: function, method","  dispatch:","    CPU, CUDA: dequantize_cpu_or_cuda","    QuantizedCPU, QuantizedCUDA: dequantize_quantized","  autogen: dequantize.self_out","","- func: dequantize.tensors(Tensor[] tensors) -\u003e Tensor[]","  variants: function","  dispatch:","    QuantizedCPU: dequantize_tensors_quantized_cpu","  autogen: dequantize.tensors_out","","- func: q_scale(Tensor self) -\u003e float","  variants: function, method","  dispatch:","    QuantizedCPU, QuantizedCUDA: q_scale_quant","","- func: q_zero_point(Tensor self) -\u003e int","  variants: function, method","  dispatch:","    QuantizedCPU, QuantizedCUDA: q_zero_point_quant","","- func: q_per_channel_scales(Tensor self) -\u003e Tensor","  variants: function, method","  dispatch:","    QuantizedCPU, QuantizedCUDA: q_per_channel_scales","  autogen: q_per_channel_scales.out","","- func: q_per_channel_zero_points(Tensor self) -\u003e Tensor","  variants: function, method","  dispatch:","    QuantizedCPU, QuantizedCUDA: q_per_channel_zero_points","  autogen: q_per_channel_zero_points.out","","- func: q_per_channel_axis(Tensor self) -\u003e int","  variants: function, method","  dispatch:","    QuantizedCPU, QuantizedCUDA: q_per_channel_axis","","- func: int_repr(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    QuantizedCPU: int_repr_quantized_cpu","    QuantizedCUDA: int_repr_quantized_cuda","  autogen: int_repr.out","","- func: _make_per_tensor_quantized_tensor(Tensor self, float scale, int zero_point) -\u003e Tensor","  dispatch:","    CPU: make_per_tensor_quantized_tensor_cpu","    CUDA: make_per_tensor_quantized_tensor_cuda","  autogen: _make_per_tensor_quantized_tensor.out","","- func: _make_per_channel_quantized_tensor(Tensor self, Tensor scale, Tensor zero_point, int axis) -\u003e Tensor","  dispatch:","    CPU: make_per_channel_quantized_tensor_cpu","    CUDA: make_per_channel_quantized_tensor_cuda","  autogen: _make_per_channel_quantized_tensor.out","","- func: qscheme(Tensor self) -\u003e QScheme","  variants: method","  dispatch:","    QuantizedCPU, QuantizedCUDA: qscheme_quant","","- func: fake_quantize_per_tensor_affine(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: fake_quantize_per_tensor_affine.tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: fake_quantize_per_tensor_affine_cachemask(Tensor self, float scale, int zero_point, int quant_min, int quant_max) -\u003e (Tensor output, Tensor mask)","  variants: function","  dispatch:","    CPU, CUDA: fake_quantize_per_tensor_affine_cachemask","  autogen: fake_quantize_per_tensor_affine_cachemask.out","","- func: _fake_quantize_per_tensor_affine_cachemask_tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max) -\u003e (Tensor output, Tensor mask)","  variants: function","  dispatch:","    CPU, CUDA: _fake_quantize_per_tensor_affine_cachemask_tensor_qparams","  autogen: _fake_quantize_per_tensor_affine_cachemask_tensor_qparams.out","","- func: fake_quantize_per_tensor_affine_cachemask_backward(Tensor grad, Tensor mask) -\u003e Tensor","  variants: function","","- func: _fake_quantize_learnable_per_tensor_affine(Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -\u003e Tensor","  variants: function","  dispatch:","    CPU, CUDA: _fake_quantize_learnable_per_tensor_affine","  autogen: _fake_quantize_learnable_per_tensor_affine.out","","- func: _fake_quantize_learnable_per_tensor_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int quant_min, int quant_max, float grad_factor=1.0) -\u003e (Tensor, Tensor, Tensor)","  variants: function","  dispatch:","    CPU, CUDA: _fake_quantize_learnable_per_tensor_affine_backward","","- func: fake_quantize_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -\u003e (Tensor output, Tensor mask)","  variants: function","  dispatch:","    CPU, CUDA: fake_quantize_per_channel_affine_cachemask","  autogen: fake_quantize_per_channel_affine_cachemask.out","","- func: fake_quantize_per_channel_affine_cachemask_backward(Tensor grad, Tensor mask) -\u003e Tensor","  variants: function","","- func: _fake_quantize_learnable_per_channel_affine(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -\u003e Tensor","  variants: function","  dispatch:","    CPU, CUDA: _fake_quantize_learnable_per_channel_affine","  autogen: _fake_quantize_learnable_per_channel_affine.out","","- func: _fake_quantize_learnable_per_channel_affine_backward(Tensor grad, Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max, float grad_factor=1.0) -\u003e (Tensor, Tensor, Tensor)","  variants: function","  dispatch:","    CPU, CUDA: _fake_quantize_learnable_per_channel_affine_backward","","- func: fused_moving_avg_obs_fake_quant(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -\u003e Tensor","  variants: function","","- func: _fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -\u003e (Tensor output, Tensor mask)","  dispatch:","    CPU: fused_moving_avg_obs_fake_quant_cpu","    CUDA: fused_moving_avg_obs_fake_quant_cuda","  autogen: _fused_moving_avg_obs_fq_helper_functional, _fused_moving_avg_obs_fq_helper.out","","- func: _choose_qparams_per_tensor(Tensor self, bool reduce_range=False) -\u003e (float, int)","  variants: function","","- func: _saturate_weight_to_fp16(Tensor weight) -\u003e Tensor","  variants: function","","- func: choose_qparams_optimized(Tensor input, int numel, int n_bins, float ratio, int bit_width) -\u003e (Tensor, Tensor)","  variants: function","","- func: _autocast_to_reduced_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled, ScalarType cuda_dtype, ScalarType cpu_dtype) -\u003e Tensor(a)","  variants: method","  device_guard: False","","- func: _autocast_to_full_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled) -\u003e Tensor(a)","  variants: method","  device_guard: False","","- func: _to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -\u003e Tensor","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: _to_copy","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: _to_copy_nested","  autogen: _to_copy.out","  tags: core","","# to(Device) must not exist because all constructors of Device also works for","# TensorOptions. Otherwise, an ambiguity error is thrown.","# See NOTE [ TensorOptions Constructors ].","- func: to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -\u003e Tensor(a)","  variants: method","  device_check: NoCheck","  device_guard: False","","- func: to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -\u003e Tensor(a)","  variants: method","  device_check: NoCheck","  device_guard: False","","- func: to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -\u003e Tensor(a)","  variants: method","  device_check: NoCheck","  device_guard: False","","- func: to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -\u003e Tensor(a)","  variants: method","  device_check: NoCheck","  device_guard: False","","- func: meshgrid(Tensor[] tensors) -\u003e Tensor[]","","# TODO: Two weeks after this lands, combine these two overloads,","#       making \"indexing\" optional. These are temporarily distinct for","#       forward-compatibility reasons.","- func: meshgrid.indexing(Tensor[] tensors, *, str indexing) -\u003e Tensor[]","","- func: cartesian_prod(Tensor[] tensors) -\u003e Tensor","  variants: function","  tags: maybe_aliasing_or_mutating","","- func: combinations(Tensor self, int r=2, bool with_replacement=False) -\u003e Tensor","  variants: function","","- func: item(Tensor self) -\u003e Scalar","  tags: data_dependent_output","  variants: method","","- func: result_type.Tensor(Tensor tensor, Tensor other) -\u003e ScalarType","  variants: function","","- func: result_type.Scalar(Tensor tensor, Scalar other) -\u003e ScalarType","  variants: function","","- func: result_type.Scalar_Tensor(Scalar scalar, Tensor tensor) -\u003e ScalarType","  variants: function","","- func: result_type.Scalar_Scalar(Scalar scalar1, Scalar scalar2) -\u003e ScalarType","","- func: can_cast(ScalarType from_, ScalarType to) -\u003e bool","  variants: function","","- func: promote_types(ScalarType type1, ScalarType type2) -\u003e ScalarType","  variants: function","","# NB: Does NOT check precondition that numel == 1","- func: _local_scalar_dense(Tensor self) -\u003e Scalar","  tags: [core, data_dependent_output]","  dispatch:","    CPU: _local_scalar_dense_cpu","    CUDA: _local_scalar_dense_cuda","    MPS: _local_scalar_dense_mps","  variants: function","","# MPS LSTM implementation","","- func: _lstm_mps(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -\u003e (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)","  dispatch:","    MPS: _lstm_mps","  autogen: _lstm_mps.out","  tags: nondeterministic_seeded","","- func: lstm_mps_backward(Tensor? grad_y, Tensor? grad_hy, Tensor? grad_cy, Tensor z_state, Tensor cell_state_fwd, Tensor input, Tensor layersOutputs, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -\u003e (Tensor, Tensor[], Tensor[])","  dispatch:","    MPS: lstm_mps_backward","  autogen: lstm_mps_backward.out","","","# Fused RNN kernels","- func: _thnn_fused_lstm_cell(Tensor input_gates, Tensor hidden_gates, Tensor cx, Tensor? input_bias=None, Tensor? hidden_bias=None) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CUDA: _thnn_fused_lstm_cell_cuda","  autogen: _thnn_fused_lstm_cell.out","","# NB: The composite version of this function below is a simple wrapper that duplicates some of the outputs","#     It is necessary to avoid triggering TensorImpl use count checks in debug mode","# NB: this is function is NOT differentiable","- func: _thnn_fused_lstm_cell_backward_impl(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CUDA: _thnn_fused_lstm_cell_backward_impl_cuda","  autogen: _thnn_fused_lstm_cell_backward_impl.out","","- func: _thnn_fused_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor cx, Tensor cy, Tensor workspace, bool has_bias) -\u003e (Tensor, Tensor, Tensor, Tensor, Tensor)","","- func: _thnn_differentiable_lstm_cell_backward(Tensor? grad_hy, Tensor? grad_cy, Tensor input_gates, Tensor hidden_gates, Tensor? input_bias, Tensor? hidden_bias, Tensor cx, Tensor cy) -\u003e (Tensor, Tensor, Tensor, Tensor, Tensor)","","- func: _thnn_fused_gru_cell(Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias=None, Tensor? hidden_bias=None) -\u003e (Tensor, Tensor)","  dispatch:","    CUDA: _thnn_fused_gru_cell_cuda","  autogen: _thnn_fused_gru_cell.out","","- func: _thnn_fused_gru_cell_backward(Tensor grad_hy, Tensor workspace, bool has_bias) -\u003e (Tensor, Tensor, Tensor, Tensor, Tensor)","  dispatch:","    CUDA: _thnn_fused_gru_cell_backward_cuda","  autogen: _thnn_fused_gru_cell_backward.out","","- func: _thnn_differentiable_gru_cell_backward(Tensor grad_hy, Tensor input_gates, Tensor hidden_gates, Tensor hx, Tensor? input_bias, Tensor? hidden_bias) -\u003e (Tensor, Tensor, Tensor, Tensor, Tensor)","","# RNN cells and layers","- func: lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -\u003e (Tensor, Tensor, Tensor)","  tags: nondeterministic_seeded","","- func: lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -\u003e (Tensor, Tensor, Tensor)","  tags: nondeterministic_seeded","","- func: gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -\u003e (Tensor, Tensor)","  tags: nondeterministic_seeded","","- func: gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -\u003e (Tensor, Tensor)","  tags: nondeterministic_seeded","","- func: rnn_tanh.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -\u003e (Tensor, Tensor)","  tags: nondeterministic_seeded","","- func: rnn_tanh.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -\u003e (Tensor, Tensor)","  tags: nondeterministic_seeded","","- func: rnn_relu.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -\u003e (Tensor, Tensor)","  tags: nondeterministic_seeded","","- func: rnn_relu.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -\u003e (Tensor, Tensor)","  tags: nondeterministic_seeded","","- func: lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -\u003e (Tensor, Tensor)","","- func: gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -\u003e Tensor","","- func: rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -\u003e Tensor","","- func: rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -\u003e Tensor","","# Quantized RNN layer registration has been moved to C10 dispatch in `RNN.cpp`","","# Quantized RNN layers","# - func: quantized_lstm(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, *, ScalarType? dtype=None, bool use_dynamic=False) -\u003e (Tensor, Tensor, Tensor)","","","# - func: quantized_lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, *, ScalarType? dtype=None, bool use_dynamic=False) -\u003e (Tensor, Tensor, Tensor)","","","# Quantized GRU layers","","# - func: quantized_gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -\u003e (Tensor, Tensor)","#","","# - func: quantized_gru.data(Tensor data, Tensor batch_sizes, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -\u003e (Tensor, Tensor)","#","","# Quantized RNN cells","- func: quantized_lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -\u003e (Tensor, Tensor)","","- func: quantized_gru_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -\u003e Tensor","","- func: quantized_rnn_relu_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -\u003e Tensor","","- func: quantized_rnn_tanh_cell(Tensor input, Tensor hx, Tensor w_ih, Tensor w_hh, Tensor b_ih, Tensor b_hh, Tensor packed_ih, Tensor packed_hh, Tensor col_offsets_ih, Tensor col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) -\u003e Tensor","","# PackedSequence utilities","- func: _pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -\u003e (Tensor, Tensor)","  dispatch:","    CompositeExplicitAutograd: _pack_padded_sequence","  autogen: _pack_padded_sequence.out","","- func: _pack_padded_sequence_backward(Tensor grad, SymInt[] input_size, Tensor batch_sizes, bool batch_first) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd: _pack_padded_sequence_backward_symint","","- func: _pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -\u003e (Tensor, Tensor)","","# wrappers for legacy TH methods","","- func: set_.source_Storage(Tensor(a!) self, Storage source) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CPU, CUDA, Meta, MPS: set_","  autogen: set.source_Storage, set.source_Storage_out","  tags: inplace_view","","- func: set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CPU: set_storage_cpu_","    Meta: set_storage_meta__symint","    CUDA: set_storage_cuda_","    MPS: set_storage_mps_","    QuantizedCPU, QuantizedCUDA: set_storage_quantized_","  autogen: set.source_Storage_storage_offset, set.source_Storage_storage_offset_out","  tags: inplace_view","","- func: set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: set__symint","  tags: inplace_view","","- func: set_.source_Tensor(Tensor(a!) self, Tensor source) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CPU, CUDA, Meta, MPS: set_tensor_","  autogen: set.source_Tensor, set.source_Tensor_out","  tags: inplace_view","","- func: set_(Tensor(a!) self) -\u003e Tensor(a!)","  variants: method","  dispatch:","    CPU: set_cpu_","    CUDA: set_cuda_","    Meta: set_meta_","    MPS: set_mps_","  autogen: set, set.out","  tags: inplace_view","","# Not making it CompositeImplicitAutograd because lift","# should be a primitive w.r.t. functorch","","# TODO: this should have a view annotation","# TODO: shouldn't be a method","- func: lift(Tensor self) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: lift","  autogen: lift.out","","# lift_fresh is called with an argument that is guaranteed to be","# fresh (i.e., newly allocated).  This is ONLY called from a","# torch.tensor call; if you FX trace a lift_fresh, you are obligated","# to convert this into a lift_fresh_copy (because FX will violate the","# freshness invariant when tracing).","- func: lift_fresh(Tensor(a) self) -\u003e Tensor(a)","  dispatch:","    CompositeExplicitAutograd: lift_fresh","","# Like lift, but it clones the input.","- func: lift_fresh_copy(Tensor self) -\u003e Tensor","  tags: view_copy","  dispatch:","    CompositeExplicitAutogradNonFunctional: lift_fresh_copy","  autogen: lift_fresh_copy.out","","- func: is_set_to(Tensor self, Tensor tensor) -\u003e bool","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CPU, CUDA, MPS: is_set_to","","- func: masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CPU: masked_fill__cpu","    CUDA: masked_fill__cuda","    QuantizedCPU: masked_fill__quantized_cpu","    QuantizedCUDA: masked_fill__quantized_cuda","    MPS: masked_fill__mps","  autogen: masked_fill.Scalar_out","","- func: masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: masked_fill","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_masked_fill","  tags: pointwise","","- func: masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CPU: masked_fill__cpu","    CUDA: masked_fill__cuda","    QuantizedCPU: masked_fill__quantized_cpu","    QuantizedCUDA: masked_fill__quantized_cuda","    MPS: masked_fill__mps","  autogen: masked_fill.Tensor_out","","- func: masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: masked_fill","","- func: masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -\u003e Tensor(a!)","  variants: method","  dispatch:","    CPU: masked_scatter__cpu","    CUDA: masked_scatter__cuda","    MPS: masked_scatter__mps","  autogen: masked_scatter.out","","- func: masked_scatter(Tensor self, Tensor mask, Tensor source) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: masked_scatter","  tags: core","","- func: masked_scatter_backward(Tensor grad_output, Tensor mask, SymInt[] sizes) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: masked_scatter_backward_symint","","- func: _masked_softmax(Tensor self, Tensor mask, int? dim=None, int? mask_type=None) -\u003e Tensor","  dispatch:","    CUDA: masked_softmax_cuda","    CPU: masked_softmax_cpu","  autogen: _masked_softmax.out","","- func: _masked_softmax_backward(Tensor grad_output, Tensor output, Tensor mask, int? dim=None) -\u003e Tensor","  dispatch:","    CUDA: masked_softmax_backward_cuda","    CPU: masked_softmax_backward_cpu","  autogen: _masked_softmax_backward.out","","- func: view(Tensor(a) self, SymInt[] size) -\u003e Tensor(a)","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    ZeroTensor, Meta, CPU, CUDA, QuantizedCPU, QuantizedCUDA, MPS, MTIA: view","    MkldnnCPU: mkldnn_view","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: view_nested","  tags: core","","# Warning: If you want to change the name or overload name of this","# operator, you might also want to change the `isBlockListedSchema`","# function in `torch/csrc/jit/frontend/schema_catching.cpp`.","# The name and overload name of this operator is hardcoded in that","# function in order to workaround a bug:","# https://github.com/pytorch/pytorch/issues/47964","- func: view.dtype(Tensor(a) self, ScalarType dtype) -\u003e Tensor(a)","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: view_dtype","","- func: put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -\u003e Tensor(a!)","  variants: method","  dispatch:","    CPU, CUDA: put_","  autogen: put.out","","- func: put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -\u003e Tensor","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: put","","- func: index_add.out(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  precomputed:","  - dim -\u003e int dim","  dispatch:","    CPU: index_add_cpu_out","    CUDA: index_add_cuda_out","    MPS: index_add_mps_out","","- func: index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -\u003e Tensor(a!)","  structured_delegate: index_add.out","  variants: method","","- func: index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -\u003e Tensor","  structured_delegate: index_add.out","  variants: function, method","","- func: index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -\u003e Tensor","  variants: function, method","","- func: index_reduce.out(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  precomputed:","  - dim -\u003e int dim","  dispatch:","    CPU: index_reduce_cpu_out","    CUDA: index_reduce_cuda_out","","- func: index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -\u003e Tensor(a!)","  structured_delegate: index_reduce.out","  variants: method","","- func: index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -\u003e Tensor","  structured_delegate: index_reduce.out","  variants: function, method","","- func: index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CPU: index_fill_","    CUDA: index_fill_","    MPS: index_fill_mps_","  autogen: index_fill.int_Scalar_out","","- func: index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: index_fill","","- func: index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CPU, CUDA: index_fill_","    MPS: index_fill_mps_","  autogen: index_fill.int_Tensor_out","","- func: index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  dispatch:","    CompositeExplicitAutograd: index_fill","","- func: index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","","- func: scatter.src(Tensor self, int dim, Tensor index, Tensor src) -\u003e Tensor","  structured_delegate: scatter.src_out","  variants: function, method","  tags: core","","- func: scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -\u003e Tensor(a!)","  structured_delegate: scatter.src_out","  variants: method","","- func: scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  dispatch:","    CPU, CUDA: scatter_src_out","    MPS: scatter_src_out_mps","","- func: scatter.value(Tensor self, int dim, Tensor index, Scalar value) -\u003e Tensor","  structured_delegate: scatter.value_out","  variants: function, method","  tags: core","","- func: scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -\u003e Tensor(a!)","  structured_delegate: scatter.value_out","  variants: method","","- func: scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  dispatch:","    CPU, CUDA: scatter_value_out","    MPS: scatter_value_out_mps","","- func: scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -\u003e Tensor","  structured_delegate: scatter.reduce_out","  variants: function, method","","- func: scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -\u003e Tensor(a!)","  structured_delegate: scatter.reduce_out","  variants: method","","- func: scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  dispatch:","    CPU, CUDA: scatter_reduce_out","    MPS: scatter_reduce_out_mps","","- func: scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -\u003e Tensor","  structured_delegate: scatter.value_reduce_out","  variants: function, method","","- func: scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -\u003e Tensor(a!)","  structured_delegate: scatter.value_reduce_out","  variants: method","","- func: scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  dispatch:","    CPU, CUDA: scatter_value_reduce_out","    MPS: scatter_value_reduce_out_mps","","- func: scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -\u003e Tensor","  variants: function, method","","- func: scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -\u003e Tensor","  variants: function, method","","- func: scatter_add(Tensor self, int dim, Tensor index, Tensor src) -\u003e Tensor","  structured_delegate: scatter_add.out","  variants: function, method","  tags: core","","- func: scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -\u003e Tensor(a!)","  structured_delegate: scatter_add.out","  variants: method","","- func: scatter_add.out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  dispatch:","    CPU, CUDA: scatter_add","    MPS: scatter_add_mps_out","","- func: scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -\u003e Tensor","  variants: function, method","","- func: scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -\u003e Tensor","  structured_delegate: scatter_reduce.two_out","  variants: function, method","  tags: core","","- func: scatter_reduce_.two(Tensor(a!) self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -\u003e Tensor(a!)","  structured_delegate: scatter_reduce.two_out","  variants: method","","- func: scatter_reduce.two_out(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  variants: function","  dispatch:","    CPU, CUDA, MPS: scatter_reduce_two","","- func: eq_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  structured_delegate: eq.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: eq_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: eq.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  variants: function","  dispatch:","    CPU, CUDA, MTIA: bitwise_and_out","    MPS: bitwise_and_out_mps","  tags: pointwise","","- func: bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: bitwise_and_out","  tags: pointwise","","- func: bitwise_and.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: bitwise_and","  tags: [core, pointwise]","","- func: bitwise_and.Scalar_Tensor(Scalar self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: bitwise_and","  autogen: bitwise_and.Scalar_Tensor_out","  tags: pointwise","","- func: bitwise_and.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  structured_delegate: bitwise_and.Tensor_out","  tags: [core, pointwise]","","- func: bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: bitwise_and_","  tags: pointwise","","- func: bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: bitwise_and.Tensor_out","  tags: pointwise","","- func: __and__.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","","- func: __and__.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","","- func: __iand__.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: __iand__.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  variants: function","  dispatch:","    CPU, CUDA, MTIA: bitwise_or_out","    MPS: bitwise_or_out_mps","  tags: pointwise","","- func: bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: bitwise_or_out","  tags: pointwise","","- func: bitwise_or.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: bitwise_or","  tags: [core, pointwise]","","- func: bitwise_or.Scalar_Tensor(Scalar self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: bitwise_or","  autogen: bitwise_or.Scalar_Tensor_out","  tags: pointwise","","- func: bitwise_or.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  structured_delegate: bitwise_or.Tensor_out","  tags: [core, pointwise]","","- func: bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: bitwise_or_","  tags: pointwise","","- func: bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: bitwise_or.Tensor_out","  tags: pointwise","","- func: __or__.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","","- func: __or__.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","","- func: __ior__.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: __ior__.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  variants: function","  dispatch:","    CPU, CUDA: bitwise_xor_out","    MPS: bitwise_xor_out_mps","  tags: pointwise","","- func: bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: bitwise_xor_out","  tags: pointwise","","- func: bitwise_xor.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: bitwise_xor","  tags: [core, pointwise]","","- func: bitwise_xor.Scalar_Tensor(Scalar self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: bitwise_xor","  autogen: bitwise_xor.Scalar_Tensor_out","  tags: pointwise","","- func: bitwise_xor.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  structured_delegate: bitwise_xor.Tensor_out","  tags: [core, pointwise]","","- func: bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: bitwise_xor_","  tags: pointwise","","- func: bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: bitwise_xor.Tensor_out","  tags: pointwise","","- func: __xor__.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  tags: pointwise","","- func: __xor__.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  tags: pointwise","","- func: __ixor__.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  tags: pointwise","","- func: __ixor__.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  tags: pointwise","","- func: __lshift__.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CPU, CUDA, MPS: __lshift__","  tags: pointwise","","- func: __lshift__.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CPU, CUDA, MPS: __lshift__","  tags: pointwise","","- func: __ilshift__.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CPU, CUDA, MPS: __ilshift__","  autogen: __lshift__.Scalar_out","  tags: pointwise","","- func: __ilshift__.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CPU, CUDA, MPS: __ilshift__","  autogen: __lshift__.Tensor_out","  tags: pointwise","","- func: bitwise_left_shift.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: bitwise_left_shift.Tensor_out","  tags: pointwise","","- func: bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: bitwise_left_shift.Tensor_out","  tags: pointwise","","- func: bitwise_left_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: bitwise_left_shift_out","  tags: pointwise","","- func: bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: bitwise_left_shift","  tags: pointwise","","- func: bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: bitwise_left_shift_","  tags: pointwise","","- func: bitwise_left_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: bitwise_left_shift_out","  tags: pointwise","","- func: bitwise_left_shift.Scalar_Tensor(Scalar self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: bitwise_left_shift","  autogen: bitwise_left_shift.Scalar_Tensor_out","  tags: pointwise","","- func: __rshift__.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CPU, CUDA, MPS: __rshift__","  tags: pointwise","","- func: __rshift__.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CPU, CUDA, MPS: __rshift__","  tags: pointwise","","- func: __irshift__.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CPU, CUDA, MPS: __irshift__","  autogen: __rshift__.Scalar_out","","- func: __irshift__.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CPU, CUDA, MPS: __irshift__","  autogen: __rshift__.Tensor_out","","- func: bitwise_right_shift.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function, method","  structured_delegate: bitwise_right_shift.Tensor_out","  tags: pointwise","","- func: bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: bitwise_right_shift.Tensor_out","  tags: pointwise","","- func: bitwise_right_shift.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: bitwise_right_shift_out","  tags: pointwise","","- func: bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: bitwise_right_shift","  tags: pointwise","","- func: bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: bitwise_right_shift_","  tags: pointwise","","- func: bitwise_right_shift.Tensor_Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: bitwise_right_shift_out","  tags: pointwise","","- func: bitwise_right_shift.Scalar_Tensor(Scalar self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CompositeExplicitAutograd: bitwise_right_shift","  autogen: bitwise_right_shift.Scalar_Tensor_out","  tags: pointwise","","- func: tril_(Tensor(a!) self, int diagonal=0) -\u003e Tensor(a!)","  structured_delegate: tril.out","  variants: method","","- func: triu_(Tensor(a!) self, int diagonal=0) -\u003e Tensor(a!)","  structured_delegate: triu.out","  variants: method","","- func: digamma_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: digamma.out","  variants: method","  tags: pointwise","","- func: lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: lerp.Scalar_out","  tags: pointwise","","- func: lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: lerp.Tensor_out","  tags: pointwise","","- func: addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor(a!)","  variants: method","  dispatch:","    CPU, CUDA, XPU: addbmm_","    MPS: addbmm_mps_","","- func: addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, XPU: addbmm_out","    MPS: addbmm_out_mps","","- func: addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU, CUDA, XPU: addbmm","    MPS: addbmm_mps","","- func: random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  tags: nondeterministic_seeded","  dispatch:","    CPU, CUDA: random_","    Meta: random_meta_","    MPS: random_mps_","  autogen: random.from, random.from_out","","- func: random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  tags: nondeterministic_seeded","  variants: method","  dispatch:","    CPU, CUDA: random_","    Meta: random_meta_","    MPS: random_mps_","  autogen: random.to, random.to_out","","- func: random_(Tensor(a!) self, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  tags: nondeterministic_seeded","  variants: method","  dispatch:","    CPU, CUDA: random_","    MPS: random_mps_","    Meta: random_meta_","  autogen: random, random.out","","- func: uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  tags: nondeterministic_seeded","  variants: method","  dispatch:","    CPU, CUDA: uniform_","    MPS: uniform_mps_","    Meta: uniform_meta_","  autogen: uniform, uniform.out","","- func: cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  tags: nondeterministic_seeded","  dispatch:","    CPU, CUDA: cauchy_","  autogen: cauchy, cauchy.out","","- func: log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  tags: nondeterministic_seeded","  variants: method","  dispatch:","    CPU, CUDA: log_normal_","  autogen: log_normal, log_normal.out","","- func: exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  tags: nondeterministic_seeded","  variants: method","  dispatch:","    CPU, CUDA: exponential_","    MPS: exponential_mps_","  autogen: exponential, exponential.out","","- func: geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  tags: nondeterministic_seeded","  variants: method","  dispatch:","    CPU, CUDA: geometric_","","  # wrappers for TH functions","  autogen: geometric, geometric.out","","- func: diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: diag(Tensor self, int diagonal=0) -\u003e Tensor","  variants: method, function","","- func: cross.out(Tensor self, Tensor other, int? dim=None, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: cross(Tensor self, Tensor other, int? dim=None) -\u003e Tensor","  variants: method, function","","- func: triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: triu_cpu","    CUDA: triu_cuda","    MPS: triu_mps_out","","- func: triu(Tensor self, int diagonal=0) -\u003e Tensor","  structured_delegate: triu.out","  variants: method, function","","- func: tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: tril_cpu","    CUDA: tril_cuda","    MPS: tril_mps_out","","- func: tril(Tensor self, int diagonal=0) -\u003e Tensor","  structured_delegate: tril.out","  variants: method, function","","- func: tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CPU: tril_indices_cpu","    CUDA: tril_indices_cuda","    MPS: tril_indices_mps","  autogen: tril_indices.out","","- func: triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CPU: triu_indices_cpu","    CUDA: triu_indices_cuda","    MPS: triu_indices_mps","  autogen: triu_indices.out","","- func: trace(Tensor self) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU: trace_cpu","    CUDA: trace_cuda","    MPS: trace_mps","  autogen: trace.out","","- func: trace_backward(Tensor grad, SymInt[] sizes) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: trace_backward_symint","","- func: ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: ne_Scalar_out","    MPS: ne_scalar_out_mps","    QuantizedCPU: ne_out_quantized_cpu","  tags: pointwise","","- func: ne.Scalar(Tensor self, Scalar other) -\u003e Tensor","  structured_delegate: ne.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: ne_quantized_cpu","  tags: [core, pointwise]","","- func: ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: ne_Tensor_out","    MPS: ne_tensor_out_mps","    QuantizedCPU: ne_out_quantized_cpu","  tags: pointwise","","- func: ne.Tensor(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: ne.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: ne_quantized_cpu","  tags: [core, pointwise]","","- func: ne_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  structured_delegate: ne.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: ne_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: ne.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method","","# not_equal, alias for torch.ne","- func: not_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: not_equal.Scalar(Tensor self, Scalar other) -\u003e Tensor","  variants: method, function","","- func: not_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: not_equal.Tensor(Tensor self, Tensor other) -\u003e Tensor","  variants: method, function","","- func: not_equal_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  variants: method","","- func: not_equal_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  variants: method","","- func: eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: eq_Scalar_out","    MPS: eq_scalar_out_mps","    QuantizedCPU: eq_out_quantized_cpu","  tags: pointwise","","- func: eq.Scalar(Tensor self, Scalar other) -\u003e Tensor","  structured_delegate: eq.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: eq_quantized_cpu","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: eq_scalar_nested","  tags: [core, pointwise]","","- func: eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: eq_Tensor_out","    MPS: eq_tensor_out_mps","    QuantizedCPU: eq_out_quantized_cpu","  tags: pointwise","","- func: eq.Tensor(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: eq.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: eq_quantized_cpu","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: eq_tensor_nested","  tags: [core, pointwise]","","- func: ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: ge_Scalar_out","    MPS: ge_scalar_out_mps","    QuantizedCPU: ge_out_quantized_cpu","  tags: pointwise","","- func: ge.Scalar(Tensor self, Scalar other) -\u003e Tensor","  structured_delegate: ge.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: ge_quantized_cpu","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: ge_scalar_nested","  tags: [core, pointwise]","","- func: ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: ge_Tensor_out","    MPS: ge_tensor_out_mps","    QuantizedCPU: ge_out_quantized_cpu","  tags: pointwise","","- func: ge.Tensor(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: ge.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: ge_quantized_cpu","  tags: [core, pointwise]","","- func: ge_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  structured_delegate: ge.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: ge_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: ge.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method","","# greater_equal, alias for torch.ge","- func: greater_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: greater_equal.Scalar(Tensor self, Scalar other) -\u003e Tensor","  variants: method, function","","- func: greater_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: greater_equal.Tensor(Tensor self, Tensor other) -\u003e Tensor","  variants: method, function","","- func: greater_equal_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  variants: method","","- func: greater_equal_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  variants: method","","- func: le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: le_Scalar_out","    MPS: le_scalar_out_mps","    QuantizedCPU: le_out_quantized_cpu","  tags: pointwise","","- func: le.Scalar(Tensor self, Scalar other) -\u003e Tensor","  structured_delegate: le.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: le_quantized_cpu","  tags: [core, pointwise]","","- func: le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: le_Tensor_out","    MPS: le_tensor_out_mps","    QuantizedCPU: le_out_quantized_cpu","  tags: pointwise","","- func: le.Tensor(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: le.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: le_quantized_cpu","  tags: [core, pointwise]","","- func: le_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  structured_delegate: le.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: le_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: le.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method","","# less_equal, alias for torch.le","- func: less_equal.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: less_equal.Scalar(Tensor self, Scalar other) -\u003e Tensor","  variants: method, function","","- func: less_equal.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: less_equal.Tensor(Tensor self, Tensor other) -\u003e Tensor","  variants: method, function","","- func: less_equal_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  variants: method","","- func: less_equal_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  variants: method","","- func: gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA,MTIA: gt_Scalar_out","    MPS: gt_scalar_out_mps","    QuantizedCPU: gt_out_quantized_cpu","  tags: pointwise","","- func: gt.Scalar(Tensor self, Scalar other) -\u003e Tensor","  structured_delegate: gt.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: gt_quantized_cpu","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: gt_scalar_nested","  tags: [core, pointwise]","","- func: gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: gt_Tensor_out","    MPS: gt_tensor_out_mps","    QuantizedCPU: gt_out_quantized_cpu","  tags: pointwise","","- func: gt.Tensor(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: gt.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: gt_quantized_cpu","  tags: [core, pointwise]","","- func: gt_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  structured_delegate: gt.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: gt_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: gt.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method","","#  greater, alias for torch.gt","- func: greater.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: greater.Scalar(Tensor self, Scalar other) -\u003e Tensor","  variants: method, function","","- func: greater.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: greater.Tensor(Tensor self, Tensor other) -\u003e Tensor","  variants: method, function","","- func: greater_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  variants: method","","- func: greater_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  variants: method","","- func: lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: lt_Scalar_out","    MPS: lt_scalar_out_mps","    QuantizedCPU: lt_out_quantized_cpu","  tags: pointwise","","- func: lt.Scalar(Tensor self, Scalar other) -\u003e Tensor","  structured_delegate: lt.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: lt_quantized_cpu","  tags: [core, pointwise]","","- func: lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: lt_Tensor_out","    MPS: lt_tensor_out_mps","    QuantizedCPU: lt_out_quantized_cpu","  tags: pointwise","","- func: lt.Tensor(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: lt.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    QuantizedCPU: lt_quantized_cpu","  tags: [core, pointwise]","","- func: lt_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  structured_delegate: lt.Scalar_out","  device_check: NoCheck   # TensorIterator","  variants: method","","- func: lt_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: lt.Tensor_out","  device_check: NoCheck   # TensorIterator","  variants: method","","#  less, alias for torch.lt","- func: less.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: less.Scalar(Tensor self, Scalar other) -\u003e Tensor","  variants: method, function","","- func: less.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: less.Tensor(Tensor self, Tensor other) -\u003e Tensor","  variants: method, function","","- func: less_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  variants: method","","- func: less_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  variants: method","","- func: take.out(Tensor self, Tensor index, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: take_out","","- func: take(Tensor self, Tensor index) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU, CUDA: take","","- func: take_along_dim.out(Tensor self, Tensor indices, int? dim=None, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: take_along_dim(Tensor self, Tensor indices, int? dim=None) -\u003e Tensor","  variants: method, function","","- func: index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, QuantizedCPU: index_select_out_cpu_","    CUDA, QuantizedCUDA: index_select_out_cuda","    MPS: index_select_out_mps","","- func: index_select(Tensor self, int dim, Tensor index) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU: index_select_cpu_","    QuantizedCPU: index_select_quantized_cpu_","    CUDA: index_select_cuda","    QuantizedCUDA: index_select_quantized_cuda","    SparseCPU: index_select_sparse_cpu","    SparseCUDA: index_select_sparse_cuda","    MPS: index_select_mps","  tags: core","","- func: index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: index_select.dimname(Tensor self, Dimname dim, Tensor index) -\u003e Tensor","  variants: method, function","","- func: index_select_backward(Tensor grad, SymInt[] self_sizes, int dim, Tensor index) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeImplicitAutograd: index_select_backward_symint","","- func: masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU: masked_select_out_cpu","    CUDA: masked_select_out_cuda","    MPS: masked_select_out_mps","  tags: dynamic_output_shape","","- func: masked_select(Tensor self, Tensor mask) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU: masked_select_cpu","    CUDA: masked_select_cuda","    MPS: masked_select_mps","  tags: dynamic_output_shape","","- func: masked_select_backward(Tensor grad, Tensor input, Tensor mask) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","","- func: nonzero.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU: nonzero_out_cpu","    CUDA: nonzero_out_cuda","    MPS: nonzero_out_mps","  tags: dynamic_output_shape","","- func: nonzero(Tensor self) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU: nonzero_cpu","    CUDA: nonzero_cuda","    MPS: nonzero_mps","  tags: [dynamic_output_shape, core]","","- func: nonzero_static.out(Tensor self, *, SymInt size, int fill_value=-1, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU: nonzero_static_out_cpu","    CUDA: nonzero_static_out_cuda","","- func: nonzero_static(Tensor self, *, SymInt size, int fill_value=-1) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU: nonzero_static_cpu","    CUDA: nonzero_static_cuda","","- func: nonzero_numpy(Tensor self) -\u003e Tensor[]","  variants: method, function","","- func: argwhere(Tensor self) -\u003e Tensor","  variants: method, function","  tags: dynamic_output_shape","","- func: gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU, CUDA: gather_out","    MPS: gather_out_mps","","- func: gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -\u003e Tensor","  variants: method, function","  structured_delegate: gather.out","  tags: core","","- func: gather_backward(Tensor grad, Tensor self, int dim, Tensor index, bool sparse_grad) -\u003e Tensor","  variants: function","  device_check: NoCheck","  device_guard: False","","- func: gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -\u003e Tensor(a!)","","- func: gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -\u003e Tensor","  variants: method, function","","- func: _gather_sparse_backward(Tensor self, int dim, Tensor index, Tensor grad) -\u003e Tensor","","- func: addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: addcmul_out","    MPS: addcmul_out_mps","  tags: pointwise","","- func: addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -\u003e Tensor","  structured_delegate: addcmul.out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  tags: pointwise","","- func: addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -\u003e Tensor(a!)","  structured_delegate: addcmul.out","  device_check: NoCheck   # TensorIterator","  variants: method","  tags: pointwise","","- func: addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: addcdiv_out","    MPS: addcdiv_out_mps","  tags: pointwise","","- func: addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -\u003e Tensor","  structured_delegate: addcdiv.out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  tags: pointwise","","- func: addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -\u003e Tensor(a!)","  structured_delegate: addcdiv.out","  device_check: NoCheck   # TensorIterator","  variants: method","  tags: pointwise","","- func: cross_entropy_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, float label_smoothing=0.0) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: cross_entropy_loss_symint","","- func: triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -\u003e (Tensor(a!) solution, Tensor(b!) cloned_coefficient)","  structured: True","  dispatch:","    CPU, CUDA: triangular_solve_out","    MPS: triangular_solve_mps_out","    SparseCsrCPU: triangular_solve_out_sparse_csr_cpu","    SparseCsrCUDA: triangular_solve_out_sparse_csr_cuda","","- func: triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -\u003e (Tensor solution, Tensor cloned_coefficient)","  structured_delegate: triangular_solve.X","  variants: method, function","","- func: _linalg_check_errors(Tensor info, str api_name, *, bool is_matrix) -\u003e ()","  dispatch:","    CompositeExplicitAutograd: _linalg_check_errors","","- func: linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  dispatch:","    CPU, CUDA: linalg_solve_triangular_out","    MPS: linalg_solve_triangular_mps_out","","- func: linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -\u003e Tensor","  python_module: linalg","  variants: function","  dispatch:","    CPU, CUDA: linalg_solve_triangular","    MPS: linalg_solve_triangular_mps","","- func: linalg_vander(Tensor x, *, SymInt? N=None) -\u003e Tensor","  python_module: linalg","  dispatch:","    CompositeImplicitAutograd: linalg_vander_symint","","- func: svd.U(Tensor self, bool some=True, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) V) -\u003e (Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)","","- func: svd(Tensor self, bool some=True, bool compute_uv=True) -\u003e (Tensor U, Tensor S, Tensor V)","  variants: method, function","","# swapaxes, alias for transpose","- func: swapaxes(Tensor(a) self, int axis0, int axis1) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","","- func: swapaxes_(Tensor(a!) self, int axis0, int axis1) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  tags: inplace_view","","# swapdims, alias for transpose","- func: swapdims(Tensor(a) self, int dim0, int dim1) -\u003e Tensor(a)","  variants: function, method","  device_check: NoCheck","  device_guard: False","","- func: swapdims_(Tensor(a!) self, int dim0, int dim1) -\u003e Tensor(a!)","  variants: method","  device_check: NoCheck","  device_guard: False","  tags: inplace_view","","- func: cholesky.out(Tensor self, bool upper=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: cholesky_out","","- func: cholesky(Tensor self, bool upper=False) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU, CUDA, MPS: cholesky","","- func: cholesky_solve.out(Tensor self, Tensor input2, bool upper=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: cholesky_solve_out","","- func: cholesky_solve(Tensor self, Tensor input2, bool upper=False) -\u003e Tensor","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: cholesky_solve","","- func: _cholesky_solve_helper(Tensor self, Tensor A, bool upper) -\u003e Tensor","  variants: function","  dispatch:","    CPU: _cholesky_solve_helper_cpu","    CUDA: _cholesky_solve_helper_cuda","  autogen: _cholesky_solve_helper.out","","- func: cholesky_inverse(Tensor self, bool upper=False) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU, CUDA: cholesky_inverse","","- func: cholesky_inverse.out(Tensor self, bool upper=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: cholesky_inverse_out","","- func: qr.Q(Tensor self, bool some=True, *, Tensor(a!) Q, Tensor(b!) R) -\u003e (Tensor(a!) Q, Tensor(b!) R)","","- func: qr(Tensor self, bool some=True) -\u003e (Tensor Q, Tensor R)","  variants: method, function","","- func: geqrf.a(Tensor self, *, Tensor(a!) a, Tensor(b!) tau) -\u003e (Tensor(a!) a, Tensor(b!) tau)","  dispatch:","    CPU, CUDA: geqrf_out","","- func: geqrf(Tensor self) -\u003e (Tensor a, Tensor tau)","  variants: method, function","  dispatch:","    CPU, CUDA: geqrf","","# orgqr, alias for linalg_householder_product","- func: orgqr(Tensor self, Tensor input2) -\u003e Tensor","  variants: method, function","","- func: orgqr.out(Tensor self, Tensor input2, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: ormqr.out(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: ormqr_out","","- func: ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU, CUDA: ormqr","","- func: _lu_with_info(Tensor self, bool pivot=True, bool check_errors=True) -\u003e (Tensor LU, Tensor pivots, Tensor info)","  variants: function","","- func: lu_solve.out(Tensor self, Tensor LU_data, Tensor LU_pivots, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -\u003e Tensor","  variants: method, function","","# lu_unpack","- func: lu_unpack(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True) -\u003e (Tensor P, Tensor L, Tensor U)","  structured_delegate: lu_unpack.out","  variants: function","","- func: lu_unpack.out(Tensor LU_data, Tensor LU_pivots, bool unpack_data=True, bool unpack_pivots=True, *, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -\u003e (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)","  variants: function","  structured: True","  dispatch:","    CPU, CUDA: lu_unpack_out","    MPS: lu_unpack_out_mps","","# TODO: remove dispatch section when porting TH CUDA to ATen","- func: multinomial.out(Tensor self, SymInt num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","  dispatch:","    CPU, CUDA: multinomial_out","    MPS: multinomial_out_mps","","- func: multinomial(Tensor self, SymInt num_samples, bool replacement=False, *, Generator? generator=None) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU, CUDA: multinomial","    MPS: multinomial_mps","  tags: nondeterministic_seeded","","- func: lgamma.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: lgamma_out","    MPS: lgamma_out_mps","  tags: pointwise","","- func: lgamma_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: lgamma.out","  variants: method","  tags: pointwise","","- func: lgamma(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: lgamma.out","  variants: method, function","  tags: pointwise","","- func: digamma.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: digamma_out","    MPS: digamma_out_mps","  tags: pointwise","","- func: digamma(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: digamma.out","  variants: method, function","  tags: pointwise","","- func: polygamma.out(int n, Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: polygamma_out","    MPS: polygamma_out_mps","  tags: pointwise","","- func: polygamma(int n, Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: polygamma.out","  variants: method, function","  tags: pointwise","","- func: polygamma_(Tensor(a!) self, int n) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: polygamma_","  tags: pointwise","","- func: erfinv(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: erfinv.out","  variants: method, function","  dispatch:","    SparseCPU, SparseCUDA: erfinv_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: erfinv_sparse_csr","  tags: pointwise","","- func: erfinv_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: erfinv.out","  variants: method","  dispatch:","    SparseCPU, SparseCUDA: erfinv_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: erfinv_sparse_csr_","  tags: pointwise","","- func: erfinv.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: erfinv_out","    SparseCPU, SparseCUDA: erfinv_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: erfinv_sparse_csr_out","  tags: pointwise","","- func: i0(Tensor self) -\u003e Tensor","  structured_delegate: i0.out","  variants: function, method","  tags: pointwise","","- func: i0_(Tensor(a!) self) -\u003e Tensor(a!)","  structured_delegate: i0.out","  variants: function, method","  tags: pointwise","","- func: i0.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: i0_out","  tags: pointwise","","- func: sign(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: sign.out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: sign_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sign_sparse_csr","  tags: [core, pointwise]","","- func: sign_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: sign.out","  variants: method","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: sign_sparse_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sign_sparse_csr_","  tags: pointwise","","- func: sign.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: sign_out","    MPS: sign_out_mps","    SparseCPU, SparseCUDA, SparseMPS: sign_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: sign_sparse_csr_out","  tags: pointwise","","- func: signbit(Tensor self) -\u003e Tensor","  variants: function, method","  structured_delegate: signbit.out","  dispatch:","    SparseCPU, SparseCUDA, SparseMPS: signbit_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: signbit_sparse_csr","  tags: pointwise","","- func: signbit.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU: signbit_out","    CUDA: signbit_out","    MPS: signbit_out_mps","    SparseCPU, SparseCUDA, SparseMPS: signbit_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: signbit_sparse_csr_out","  tags: pointwise","","- func: dist(Tensor self, Tensor other, Scalar p=2) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: dist","  autogen: dist.out","","- func: atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: atan2_out","    MPS: atan2_out_mps","  tags: [core, pointwise]","","- func: atan2_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: atan2.out","  variants: method","  tags: pointwise","","- func: atan2(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: atan2.out","  variants: method, function","  tags: [core, pointwise]","# arctan2, alias of atan2","","- func: arctan2(Tensor self, Tensor other) -\u003e Tensor","  variants: method, function","","- func: arctan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","","- func: arctan2_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  variants: method","","- func: lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: lerp_Scalar","  tags: pointwise","","- func: lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: lerp_Tensor","    MPS: lerp_Tensor_mps","  tags: pointwise","","- func: lerp.Scalar(Tensor self, Tensor end, Scalar weight) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  structured_delegate: lerp.Scalar_out","  tags: pointwise","","- func: lerp.Tensor(Tensor self, Tensor end, Tensor weight) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  structured_delegate: lerp.Tensor_out","  tags: pointwise","","- func: histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, MPS: histogram_histc_out","    CUDA: _histc_out_cuda","","- func: histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -\u003e Tensor","  variants: method, function","  dispatch:","    CPU, MPS: histogram_histc","    CUDA: _histc_cuda","","- func: histogram.bins_tensor_out(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -\u003e (Tensor(a!) hist, Tensor(b!) bin_edges)","  dispatch:","    CPU, MPS: histogram_out","","- func: histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -\u003e (Tensor hist, Tensor bin_edges)","  variants: method, function","  dispatch:","    CPU, MPS: histogram","","- func: histogram.bin_ct_out(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False, Tensor(a!) hist, Tensor(b!) bin_edges) -\u003e (Tensor(a!) hist, Tensor(b!) bin_edges)","  dispatch:","    CPU, MPS: histogram_out","","- func: histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -\u003e (Tensor hist, Tensor bin_edges)","  variants: method, function","  dispatch:","    CPU, MPS: histogram","","- func: _histogramdd_bin_edges(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False) -\u003e Tensor[]","  dispatch:","    CPU, MPS: histogramdd_bin_edges","  autogen: _histogramdd_bin_edges.out","","- func: _histogramdd_from_bin_cts(Tensor self, int[] bins, *, float[]? range=None, Tensor? weight=None, bool density=False) -\u003e Tensor","  dispatch:","    CPU, MPS: _histogramdd","  autogen: _histogramdd_from_bin_cts.out","","- func: _histogramdd_from_bin_tensors(Tensor self, Tensor[] bins, *, Tensor? weight=None, bool density=False) -\u003e Tensor","  dispatch:","    CPU, MPS: _histogramdd","  autogen: _histogramdd_from_bin_tensors.out","","- func: histogramdd(Tensor self, int[] bins, float[]? range=None, Tensor? weight=None, bool density=False) -\u003e (Tensor hist, Tensor[] bin_edges)","","- func: histogramdd.int_bins(Tensor self, int bins, float[]? range=None, Tensor? weight=None, bool density=False) -\u003e (Tensor hist, Tensor[] bin_edges)","","- func: histogramdd.TensorList_bins(Tensor self, Tensor[] bins, float[]? range=None, Tensor? weight=None, bool density=False) -\u003e (Tensor hist, Tensor[] bin_edges)","","- func: fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CompositeExplicitAutograd: fmod_out","  tags: pointwise","","- func: fmod.Scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: fmod","  tags: [core, pointwise]","","- func: fmod_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  dispatch:","    CompositeExplicitAutograd: fmod_","  tags: pointwise","","- func: fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: fmod_out","  tags: pointwise","","- func: fmod.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: fmod.Tensor_out","  variants: method, function","  tags: [core, pointwise]","","- func: fmod_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: fmod.Tensor_out","  tags: pointwise","","- func: hypot.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: hypot_out","    MPS: hypot_out_mps","  tags: pointwise","","- func: hypot(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: hypot.out","  variants: method, function","  tags: pointwise","","- func: hypot_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: hypot.out","  variants: method","  tags: pointwise","","- func: igamma.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: igamma_out","  tags: pointwise","","- func: igamma(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: igamma.out","  variants: method, function","  tags: pointwise","","- func: igamma_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: igamma.out","  variants: method","  tags: pointwise","","- func: igammac.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: igammac_out","  tags: pointwise","","- func: igammac(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: igammac.out","  variants: method, function","  tags: pointwise","","- func: igammac_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: igammac.out","  variants: method","  tags: pointwise","","- func: nextafter.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: nextafter_out","  tags: pointwise","","- func: nextafter(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: nextafter.out","  variants: method, function","  tags: pointwise","","- func: nextafter_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  structured_delegate: nextafter.out","  variants: method","  tags: pointwise","","- func: remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: remainder_out","  tags: pointwise","","- func: remainder.Scalar(Tensor self, Scalar other) -\u003e Tensor","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: remainder","  tags: [core, pointwise]","","- func: remainder_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!)","  variants: method","  dispatch:","    CompositeExplicitAutograd: remainder_","  tags: pointwise","","- func: remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS, MTIA: remainder_out","  tags: pointwise","","- func: remainder.Tensor(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: remainder.Tensor_out","  variants: method, function","  tags: [core, pointwise]","","- func: remainder_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: remainder.Tensor_out","  variants: method","  tags: pointwise","","- func: remainder.Scalar_Tensor(Scalar self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: function","  dispatch:","    CPU, CUDA, MPS: remainder","  autogen: remainder.Scalar_Tensor_out","  tags: pointwise","","- func: min(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CPU, CUDA: min","    MPS: min_mps","    QuantizedCPU: min_quantized_cpu","","- func: min.unary_out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: min_unary_out","    QuantizedCPU: min_quantized_unary_out","","- func: fmin(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: fmin.out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  tags: pointwise","","- func: fmin.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MPS: fmin_out","  tags: pointwise","","- func: max(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CPU, CUDA: max","    MPS: max_mps","    QuantizedCPU: max_quantized_cpu","","- func: fmax(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: fmax.out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  tags: pointwise","","- func: fmax.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MPS: fmax_out","  tags: pointwise","","- func: maximum(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: maximum.out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  tags: [core, pointwise]","","- func: maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: maximum_out","    MPS: maximum_out_mps","  tags: pointwise","","# binary max, alias of maximum","# NOTE: max is not an alias for maximum, since there is also unary max","- func: max.other(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  tags: pointwise","","- func: max.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  tags: pointwise","","- func: max.unary_out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA: max_unary_out","    QuantizedCPU: max_quantized_unary_out","","- func: minimum(Tensor self, Tensor other) -\u003e Tensor","  structured_delegate: minimum.out","  device_check: NoCheck   # TensorIterator","  variants: method, function","  tags: [core, pointwise]","","- func: minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  dispatch:","    CPU, CUDA, MTIA: minimum_out","    MPS: minimum_out_mps","  tags: pointwise","","# binary min, alias for minimum","# NOTE: min is not an alias for minimum, since there is also unary min","- func: min.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  tags: pointwise","","- func: min.other(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  tags: pointwise","","- func: quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -\u003e Tensor","  variants: method, function","","- func: quantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -\u003e Tensor(a!)","","- func: quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -\u003e Tensor","  variants: method, function","","- func: quantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -\u003e Tensor(a!)","","- func: nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -\u003e Tensor","  variants: method, function","","- func: nanquantile.out(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -\u003e Tensor(a!)","","- func: nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear') -\u003e Tensor","  variants: method, function","","- func: nanquantile.scalar_out(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation='linear', Tensor(a!) out) -\u003e Tensor(a!)","","- func: sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  device_check: NoCheck   # TensorIterator","  dispatch:","    CompositeExplicitAutograd: sort_out","","- func: sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  structured: True","  dispatch:","    CPU, CUDA: sort_stable_out","    MPS: sort_stable_out_mps","","- func: sort(Tensor self, int dim=-1, bool descending=False) -\u003e (Tensor values, Tensor indices)","  device_check: NoCheck   # TensorIterator","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: sort","  tags: core","","- func: sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -\u003e (Tensor values, Tensor indices)","  structured_delegate: sort.values_stable","  variants: method, function","  dispatch:","    QuantizedCPU: sort_quantized_cpu_stable","","- func: sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","","- func: sort.dimname_values_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","","- func: sort.dimname(Tensor self, Dimname dim, bool descending=False) -\u003e (Tensor values, Tensor indices)","  variants: method, function","","- func: sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -\u003e (Tensor values, Tensor indices)","  variants: method, function","","- func: msort.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: msort(Tensor self) -\u003e Tensor","  variants: method, function","","- func: argsort(Tensor self, int dim=-1, bool descending=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","","- func: argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","","- func: argsort.stable_out(Tensor self, *, bool stable, int dim=-1, bool descending=False, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: function","","- func: argsort.dimname(Tensor self, Dimname dim, bool descending=False) -\u003e Tensor","  variants: method, function","","- func: topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -\u003e (Tensor(a!) values, Tensor(b!) indices)","  structured: True","  dispatch:","    CPU: topk_out_cpu","    CUDA: topk_out_cuda","    MPS: topk_out_mps","","- func: topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -\u003e (Tensor values, Tensor indices)","  variants: method, function","  structured_delegate: topk.values","  dispatch:","    QuantizedCPU: topk_quantized_cpu","  tags: core","","- func: all(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: all.all_out","  variants: method, function","","- func: all.all_out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  structured: True","  dispatch:","    CPU, CUDA: all_all_out","    MPS: all_all_out_mps","","- func: any(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: any.all_out","  variants: method, function","  dispatch:","    SparseCPU, SparseCUDA: any_sparse","  tags: core","","- func: any.all_out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  structured: True","  dispatch:","    CPU, CUDA: any_all_out","    MPS: any_all_out_mps","","- func: renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  dispatch:","    CPU, CUDA: renorm_out","    MPS: renorm_out_mps","","- func: renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  variants: method, function","  structured_delegate: renorm.out","","- func: renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  variants: method","  structured_delegate: renorm.out","","- func: unfold(Tensor(a) self, int dimension, int size, int step) -\u003e Tensor(a)","  variants: method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CPU, CUDA, Meta, MPS, MTIA: unfold","    QuantizedCPU, QuantizedCUDA: unfold","","- func: unfold_backward(Tensor grad_in, SymInt[] input_sizes, int dim, int size, int step) -\u003e Tensor","  variants: function","  dispatch:","    CPU, CUDA, MPS: unfold_backward","  autogen: unfold_backward.out","","- func: equal(Tensor self, Tensor other) -\u003e bool","  tags: [data_dependent_output, pointwise]","  variants: method, function","  dispatch:","    CPU: cpu_equal","    CUDA: cuda_equal","    MPS: mps_equal","    QuantizedCPU: equal_quantized_cpu","","- func: pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: pow_Tensor_Tensor_out","    MPS: pow_tensor_tensor_out_mps","  tags: pointwise","","- func: pow.Tensor_Tensor(Tensor self, Tensor exponent) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: pow.Tensor_Tensor_out","  variants: method, function","  tags: [core, pointwise]","","- func: pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  dispatch:","    CPU, CUDA: pow_Scalar_out","    MPS: pow_Scalar_out_mps","  tags: pointwise","","- func: pow.Scalar(Scalar self, Tensor exponent) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: pow.Scalar_out","  tags: [core, pointwise]","","- func: pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: pow_Tensor_Scalar_out","    SparseCPU, SparseCUDA: pow_out_sparse_scalar","    MPS: pow_tensor_scalar_out_mps","  tags: pointwise","","- func: pow.Tensor_Scalar(Tensor self, Scalar exponent) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: pow.Tensor_Scalar_out","  variants: function, method","  dispatch:","    SparseCPU, SparseCUDA: pow_sparse_scalar","  tags: [core, pointwise]","","- func: pow_.Scalar(Tensor(a!) self, Scalar exponent) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: pow.Tensor_Scalar_out","  variants: method","  tags: pointwise","","- func: pow_.Tensor(Tensor(a!) self, Tensor exponent) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured_delegate: pow.Tensor_Tensor_out","  variants: method","  tags: pointwise","","- func: float_power.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -\u003e Tensor(a!)","  tags: pointwise","","- func: float_power.Tensor_Tensor(Tensor self, Tensor exponent) -\u003e Tensor","  variants: function, method","  tags: pointwise","","- func: float_power.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -\u003e Tensor(a!)","  tags: pointwise","","- func: float_power.Scalar(Scalar self, Tensor exponent) -\u003e Tensor","  tags: pointwise","","- func: float_power.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -\u003e Tensor(a!)","  tags: pointwise","","- func: float_power.Tensor_Scalar(Tensor self, Scalar exponent) -\u003e Tensor","  variants: function, method","  tags: pointwise","","- func: float_power_.Scalar(Tensor(a!) self, Scalar exponent) -\u003e Tensor(a!)","  variants: method","  tags: pointwise","","- func: float_power_.Tensor(Tensor(a!) self, Tensor exponent) -\u003e Tensor(a!)","  variants: method","  tags: pointwise","","- func: normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  tags: nondeterministic_seeded","  variants: method","  dispatch:","    CPU, CUDA: normal_","    MPS: normal_mps_","    Meta: normal_meta_","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: normal_sparse_csr_","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: normal_nested_","  autogen: normal.out","","# Only used by the functionalization pass.","# Normally, the codegen would be able to generate a normal() NativeFunction,","# but we can't due to overload ambiguity with normal.Tensor_float.","- func: normal_functional(Tensor self, float mean=0, float std=1, *, Generator? generator=None) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  tags: nondeterministic_seeded","  dispatch:","    CompositeExplicitAutograd: normal_functional","","- func: normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -\u003e Tensor(a!)","  tags: nondeterministic_seeded","  dispatch:","    CPU, CUDA: normal_out","    MPS: normal_mps_out","    Meta: normal_out_meta","","- func: normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -\u003e Tensor","  dispatch:","    CPU, CUDA: normal","    MPS: normal_mps","    Meta: normal_meta","  tags: nondeterministic_seeded","","- func: normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: normal_out","    Meta: normal_out_meta","    MPS: normal_mps_out","  tags: nondeterministic_seeded","","- func: normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -\u003e Tensor","  dispatch:","    CPU, CUDA: normal","    MPS: normal_mps","    Meta: normal_meta","  tags: nondeterministic_seeded","","- func: normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: normal_out","    Meta: normal_out_meta","    MPS: normal_mps_out","  tags: nondeterministic_seeded","","- func: normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -\u003e Tensor","  dispatch:","    CPU, CUDA: normal","    MPS: normal_mps","    Meta: normal_meta","  tags: nondeterministic_seeded","","- func: normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: normal","  tags: nondeterministic_seeded","","- func: normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: normal_out","  tags: nondeterministic_seeded","","- func: alias(Tensor(a) self) -\u003e Tensor(a)","  variants: method, function","  dispatch:","    CompositeExplicitAutograd: alias","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: alias_nested","  tags: core","","- func: _amp_foreach_non_finite_check_and_unscale_(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -\u003e ()","  variants: function","  dispatch:","    CUDA: _amp_foreach_non_finite_check_and_unscale_cuda_","    CPU: _amp_foreach_non_finite_check_and_unscale_cpu_","    MPS: _amp_foreach_non_finite_check_and_unscale_mps_","  autogen: _amp_foreach_non_finite_check_and_unscale, _amp_foreach_non_finite_check_and_unscale.out","","- func: _amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float scale_growth_factor, float scale_backoff_factor, int growth_interval) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CUDA: _amp_update_scale_cuda_","    CPU: _amp_update_scale_cpu_","    MPS: _amp_update_scale_mps_","  autogen: _amp_update_scale, _amp_update_scale.out","","    #- func: _cat(Tensor[] tensors, int dim=0) -\u003e Tensor","    #dispatch:","    #CPU: _cat_cpu","    #CUDA: cat_cuda","    #MPS: cat_mps","    #QuantizedCPU: cat_quantized_cpu","","    #- func: _cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -\u003e Tensor(a!)","    #dispatch:","    #CPU: _cat_out_cpu","  #CUDA: cat_out_cuda","  #QuantizedCPU: cat_out_quantized_cpu","","- func: _foreach_add.Scalar(Tensor[] self, Scalar scalar) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_add_scalar_kernel_slow","    CUDA: foreach_tensor_add_scalar_kernel_cuda","","- func: _foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_add_scalar_kernel_slow_","    CUDA: foreach_tensor_add_scalar_kernel_cuda_","    MTIA: foreach_tensor_add_scalar_kernel_mtia_","  autogen: _foreach_add.Scalar_out","","- func: _foreach_add.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_add_list_kernel_slow","    CUDA: foreach_tensor_add_list_kernel_cuda","    MTIA: foreach_tensor_add_list_kernel_mtia","","- func: _foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_add_list_kernel_slow_","    CUDA: foreach_tensor_add_list_kernel_cuda_","    MTIA: foreach_tensor_add_list_kernel_mtia_","  autogen: _foreach_add.List_out","","- func: _foreach_add.ScalarList(Tensor[] self, Scalar[] scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_add_scalarlist_kernel_slow","    CUDA: foreach_tensor_add_scalarlist_kernel_cuda","","- func: _foreach_add_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_add_scalarlist_kernel_slow_","    CUDA: foreach_tensor_add_scalarlist_kernel_cuda_","  autogen: _foreach_add.ScalarList_out","","- func: _foreach_add.Tensor(Tensor[] self, Tensor other, *, Scalar alpha=1) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_add_tensor_kernel_slow","    CUDA: foreach_tensor_add_tensor_kernel_cuda","","- func: _foreach_add_.Tensor(Tensor(a!)[] self, Tensor other, *, Scalar alpha=1) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_add_tensor_kernel_slow_","    CUDA: foreach_tensor_add_tensor_kernel_cuda_","    MTIA: foreach_tensor_add_tensor_kernel_mtia_","  autogen: _foreach_add.Tensor_out","","- func: _foreach_sub.Scalar(Tensor[] self, Scalar scalar) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sub_scalar_kernel_slow","    CUDA: foreach_tensor_sub_scalar_kernel_cuda","","- func: _foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sub_scalar_kernel_slow_","    CUDA: foreach_tensor_sub_scalar_kernel_cuda_","  autogen: _foreach_sub.Scalar_out","","- func: _foreach_sub.List(Tensor[] self, Tensor[] other, *, Scalar alpha=1) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sub_list_kernel_slow","    CUDA: foreach_tensor_sub_list_kernel_cuda","","- func: _foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sub_list_kernel_slow_","    CUDA: foreach_tensor_sub_list_kernel_cuda_","  autogen: _foreach_sub.List_out","","- func: _foreach_sub.ScalarList(Tensor[] self, Scalar[] scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sub_scalarlist_kernel_slow","    CUDA: foreach_tensor_sub_scalarlist_kernel_cuda","","- func: _foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sub_scalarlist_kernel_slow_","    CUDA: foreach_tensor_sub_scalarlist_kernel_cuda_","  autogen: _foreach_sub.ScalarList_out","","- func: _foreach_mul.Scalar(Tensor[] self, Scalar scalar) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_mul_scalar_kernel_slow","    CUDA: foreach_tensor_mul_scalar_kernel_cuda","","- func: _foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_mul_scalar_kernel_slow_","    CUDA: foreach_tensor_mul_scalar_kernel_cuda_","    MTIA: foreach_tensor_mul_scalar_kernel_mtia_","  autogen: _foreach_mul.Scalar_out","","- func: _foreach_mul.List(Tensor[] self, Tensor[] other) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_mul_list_kernel_slow","    CUDA: foreach_tensor_mul_list_kernel_cuda","    MTIA: foreach_tensor_mul_list_kernel_mtia","","- func: _foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_mul_list_kernel_slow_","    CUDA: foreach_tensor_mul_list_kernel_cuda_","    MTIA: foreach_tensor_mul_list_kernel_mtia_","  autogen: _foreach_mul.List_out","","- func: _foreach_mul.ScalarList(Tensor[] self, Scalar[] scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_mul_scalarlist_kernel_slow","    CUDA: foreach_tensor_mul_scalarlist_kernel_cuda","","- func: _foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_mul_scalarlist_kernel_slow_","    CUDA: foreach_tensor_mul_scalarlist_kernel_cuda_","  autogen: _foreach_mul.ScalarList_out","","- func: _foreach_mul.Tensor(Tensor[] self, Tensor other) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_mul_tensor_kernel_slow","    CUDA: foreach_tensor_mul_tensor_kernel_cuda","    MTIA: foreach_tensor_mul_tensor_kernel_mtia","","- func: _foreach_mul_.Tensor(Tensor(a!)[] self, Tensor other) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_mul_tensor_kernel_slow_","    CUDA: foreach_tensor_mul_tensor_kernel_cuda_","    MTIA: foreach_tensor_mul_tensor_kernel_mtia_","  autogen: _foreach_mul.Tensor_out","","- func: _foreach_div.Scalar(Tensor[] self, Scalar scalar) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_div_scalar_kernel_slow","    CUDA: foreach_tensor_div_scalar_kernel_cuda","","- func: _foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_div_scalar_kernel_slow_","    CUDA: foreach_tensor_div_scalar_kernel_cuda_","  autogen: _foreach_div.Scalar_out","","- func: _foreach_div.List(Tensor[] self, Tensor[] other) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_div_list_kernel_slow","    CUDA: foreach_tensor_div_list_kernel_cuda","","- func: _foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_div_list_kernel_slow_","    CUDA: foreach_tensor_div_list_kernel_cuda_","  autogen: _foreach_div.List_out","","- func: _foreach_div.ScalarList(Tensor[] self, Scalar[] scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_div_scalarlist_kernel_slow","    CUDA: foreach_tensor_div_scalarlist_kernel_cuda","","- func: _foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_div_scalarlist_kernel_slow_","    CUDA: foreach_tensor_div_scalarlist_kernel_cuda_","  autogen: _foreach_div.ScalarList_out","","- func: _foreach_div.Tensor(Tensor[] self, Tensor other) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_div_tensor_kernel_slow","    CUDA: foreach_tensor_div_tensor_kernel_cuda","","- func: _foreach_div_.Tensor(Tensor(a!)[] self, Tensor other) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_div_tensor_kernel_slow_","    CUDA: foreach_tensor_div_tensor_kernel_cuda_","  autogen: _foreach_div.Tensor_out","","- func: _foreach_clamp_max.Scalar(Tensor[] self, Scalar scalar) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_scalar_kernel_slow","    CUDA: foreach_tensor_clamp_max_scalar_kernel_cuda","","- func: _foreach_clamp_max_.Scalar(Tensor(a!)[] self, Scalar scalar) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_scalar_kernel_slow_","    CUDA: foreach_tensor_clamp_max_scalar_kernel_cuda_","  autogen: _foreach_clamp_max.Scalar_out","","- func: _foreach_clamp_max.List(Tensor[] self, Tensor[] other) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_list_kernel_slow","    CUDA: foreach_tensor_clamp_max_list_kernel_cuda","","- func: _foreach_clamp_max_.List(Tensor(a!)[] self, Tensor[] other) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_list_kernel_slow_","    CUDA: foreach_tensor_clamp_max_list_kernel_cuda_","  autogen: _foreach_clamp_max.List_out","","- func: _foreach_clamp_max.ScalarList(Tensor[] self, Scalar[] scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_scalarlist_kernel_slow","    CUDA: foreach_tensor_clamp_max_scalarlist_kernel_cuda","","- func: _foreach_clamp_max_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_scalarlist_kernel_slow_","    CUDA: foreach_tensor_clamp_max_scalarlist_kernel_cuda_","  autogen: _foreach_clamp_max.ScalarList_out","","- func: _foreach_clamp_min.Scalar(Tensor[] self, Scalar scalar) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_scalar_kernel_slow","    CUDA: foreach_tensor_clamp_min_scalar_kernel_cuda","","- func: _foreach_clamp_min_.Scalar(Tensor(a!)[] self, Scalar scalar) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_scalar_kernel_slow_","    CUDA: foreach_tensor_clamp_min_scalar_kernel_cuda_","  autogen: _foreach_clamp_min.Scalar_out","","- func: _foreach_clamp_min.List(Tensor[] self, Tensor[] other) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_list_kernel_slow","    CUDA: foreach_tensor_clamp_min_list_kernel_cuda","","- func: _foreach_clamp_min_.List(Tensor(a!)[] self, Tensor[] other) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_list_kernel_slow_","    CUDA: foreach_tensor_clamp_min_list_kernel_cuda_","  autogen: _foreach_clamp_min.List_out","","- func: _foreach_clamp_min.ScalarList(Tensor[] self, Scalar[] scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_scalarlist_kernel_slow","    CUDA: foreach_tensor_clamp_min_scalarlist_kernel_cuda","","- func: _foreach_clamp_min_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_scalarlist_kernel_slow_","    CUDA: foreach_tensor_clamp_min_scalarlist_kernel_cuda_","  autogen: _foreach_clamp_min.ScalarList_out","","# foreach_minimum/maximum dispatches to clamp_max/min","- func: _foreach_maximum.Scalar(Tensor[] self, Scalar scalar) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_scalar_kernel_slow","    CUDA: foreach_tensor_clamp_min_scalar_kernel_cuda","","- func: _foreach_maximum_.Scalar(Tensor(a!)[] self, Scalar scalar) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_scalar_kernel_slow_","    CUDA: foreach_tensor_clamp_min_scalar_kernel_cuda_","  autogen: _foreach_maximum.Scalar_out","","# foreach_minimum/maximum dispatches to clamp_max/min","- func: _foreach_maximum.List(Tensor[] self, Tensor[] other) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_list_kernel_slow","    CUDA: foreach_tensor_clamp_min_list_kernel_cuda","","- func: _foreach_maximum_.List(Tensor(a!)[] self, Tensor[] other) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_list_kernel_slow_","    CUDA: foreach_tensor_clamp_min_list_kernel_cuda_","  autogen: _foreach_maximum.List_out","","# foreach_minimum/maximum dispatches to clamp_max/min","- func: _foreach_maximum.ScalarList(Tensor[] self, Scalar[] scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_scalarlist_kernel_slow","    CUDA: foreach_tensor_clamp_min_scalarlist_kernel_cuda","","- func: _foreach_maximum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_min_scalarlist_kernel_slow_","    CUDA: foreach_tensor_clamp_min_scalarlist_kernel_cuda_","  autogen: _foreach_maximum.ScalarList_out","","- func: _foreach_minimum.Scalar(Tensor[] self, Scalar scalar) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_scalar_kernel_slow","    CUDA: foreach_tensor_clamp_max_scalar_kernel_cuda","","- func: _foreach_minimum_.Scalar(Tensor(a!)[] self, Scalar scalar) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_scalar_kernel_slow_","    CUDA: foreach_tensor_clamp_max_scalar_kernel_cuda_","  autogen: _foreach_minimum.Scalar_out","","- func: _foreach_minimum.List(Tensor[] self, Tensor[] other) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_list_kernel_slow","    CUDA: foreach_tensor_clamp_max_list_kernel_cuda","","- func: _foreach_minimum_.List(Tensor(a!)[] self, Tensor[] other) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_list_kernel_slow_","    CUDA: foreach_tensor_clamp_max_list_kernel_cuda_","  autogen: _foreach_minimum.List_out","","- func: _foreach_minimum.ScalarList(Tensor[] self, Scalar[] scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_scalarlist_kernel_slow","    CUDA: foreach_tensor_clamp_max_scalarlist_kernel_cuda","","- func: _foreach_minimum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_clamp_max_scalarlist_kernel_slow_","    CUDA: foreach_tensor_clamp_max_scalarlist_kernel_cuda_","  autogen: _foreach_minimum.ScalarList_out","","- func: _foreach_addcdiv.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcdiv_scalar_slow","    CUDA: foreach_tensor_addcdiv_scalar_cuda","","- func: _foreach_addcdiv.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcdiv_scalarlist_slow","    CUDA: foreach_tensor_addcdiv_scalarlist_cuda","","- func: _foreach_addcdiv.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcdiv_tensor_slow","    CUDA: foreach_tensor_addcdiv_tensor_cuda","","- func: _foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcdiv_scalar_slow_","    CUDA: foreach_tensor_addcdiv_scalar_cuda_","  autogen: _foreach_addcdiv.Scalar_out","","- func: _foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcdiv_scalarlist_slow_","    CUDA: foreach_tensor_addcdiv_scalarlist_cuda_","  autogen: _foreach_addcdiv.ScalarList_out","","- func: _foreach_addcdiv_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcdiv_tensor_slow_","    CUDA: foreach_tensor_addcdiv_tensor_cuda_","  autogen: _foreach_addcdiv.Tensor_out","","- func: _foreach_addcmul.Scalar(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcmul_scalar_slow","    CUDA: foreach_tensor_addcmul_scalar_cuda","    MTIA: foreach_tensor_addcmul_scalar_mtia","","- func: _foreach_addcmul.ScalarList(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcmul_scalarlist_slow","    CUDA: foreach_tensor_addcmul_scalarlist_cuda","","- func: _foreach_addcmul.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcmul_tensor_slow","    CUDA: foreach_tensor_addcmul_tensor_cuda","","- func: _foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcmul_scalar_slow_","    CUDA: foreach_tensor_addcmul_scalar_cuda_","    MTIA: foreach_tensor_addcmul_scalar_mtia_","  autogen: _foreach_addcmul.Scalar_out","","- func: _foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcmul_scalarlist_slow_","    CUDA: foreach_tensor_addcmul_scalarlist_cuda_","  autogen: _foreach_addcmul.ScalarList_out","","- func: _foreach_addcmul_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_addcmul_tensor_slow_","    CUDA: foreach_tensor_addcmul_tensor_cuda_","  autogen: _foreach_addcmul.Tensor_out","","- func: _foreach_abs(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_abs_slow","    CUDA: foreach_tensor_abs_cuda","    MTIA: foreach_tensor_abs_mtia","","- func: _foreach_abs_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_abs_slow_","    CUDA: foreach_tensor_abs_cuda_","    MTIA: foreach_tensor_abs_mtia_","  autogen: _foreach_abs.out","","- func: _foreach_acos(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_acos_slow","    CUDA: foreach_tensor_acos_cuda","","- func: _foreach_acos_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_acos_slow_","    CUDA: foreach_tensor_acos_cuda_","  autogen: _foreach_acos.out","","- func: _foreach_asin(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_asin_slow","    CUDA: foreach_tensor_asin_cuda","","- func: _foreach_asin_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_asin_slow_","    CUDA: foreach_tensor_asin_cuda_","  autogen: _foreach_asin.out","","- func: _foreach_atan(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_atan_slow","    CUDA: foreach_tensor_atan_cuda","","- func: _foreach_atan_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_atan_slow_","    CUDA: foreach_tensor_atan_cuda_","  autogen: _foreach_atan.out","","- func: _foreach_ceil(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_ceil_slow","    CUDA: foreach_tensor_ceil_cuda","","- func: _foreach_ceil_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_ceil_slow_","    CUDA: foreach_tensor_ceil_cuda_","  autogen: _foreach_ceil.out","","- func: _foreach_cos(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_cos_slow","    CUDA: foreach_tensor_cos_cuda","","- func: _foreach_cos_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_cos_slow_","    CUDA: foreach_tensor_cos_cuda_","  autogen: _foreach_cos.out","","- func: _foreach_cosh(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_cosh_slow","    CUDA: foreach_tensor_cosh_cuda","","- func: _foreach_cosh_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_cosh_slow_","    CUDA: foreach_tensor_cosh_cuda_","  autogen: _foreach_cosh.out","","- func: _foreach_erf(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_erf_slow","    CUDA: foreach_tensor_erf_cuda","","- func: _foreach_erf_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_erf_slow_","    CUDA: foreach_tensor_erf_cuda_","  autogen: _foreach_erf.out","","- func: _foreach_erfc(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_erfc_slow","    CUDA: foreach_tensor_erfc_cuda","","- func: _foreach_erfc_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_erfc_slow_","    CUDA: foreach_tensor_erfc_cuda_","  autogen: _foreach_erfc.out","","- func: _foreach_exp(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_exp_slow","    CUDA: foreach_tensor_exp_cuda","","- func: _foreach_exp_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_exp_slow_","    CUDA: foreach_tensor_exp_cuda_","  autogen: _foreach_exp.out","","- func: _foreach_expm1(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_expm1_slow","    CUDA: foreach_tensor_expm1_cuda","","- func: _foreach_expm1_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_expm1_slow_","    CUDA: foreach_tensor_expm1_cuda_","  autogen: _foreach_expm1.out","","- func: _foreach_floor(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_floor_slow","    CUDA: foreach_tensor_floor_cuda","","- func: _foreach_floor_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_floor_slow_","    CUDA: foreach_tensor_floor_cuda_","  autogen: _foreach_floor.out","","- func: _foreach_frac(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_frac_slow","    CUDA: foreach_tensor_frac_cuda","","- func: _foreach_frac_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_frac_slow_","    CUDA: foreach_tensor_frac_cuda_","  autogen: _foreach_frac.out","","- func: _foreach_lerp.List(Tensor[] self, Tensor[] tensors1, Tensor[] weights) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensors are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_ternary_lerp_slow","    CUDA: foreach_tensor_lerp_ternary_cuda","  autogen: _foreach_lerp.List_out","","- func: _foreach_lerp_.List(Tensor(a!)[] self, Tensor[] tensors1, Tensor[] weights) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensors are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_ternary_lerp_slow_","    CUDA: foreach_tensor_lerp_ternary_cuda_","  autogen: _foreach_lerp.List_out","","- func: _foreach_lerp.Scalar(Tensor[] self, Tensor[] tensors1, Scalar weight) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensors are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_lerp_list_kernel_slow","    CUDA: foreach_tensor_lerp_list_cuda","  autogen: _foreach_lerp.Scalar_out","","- func: _foreach_lerp_.Scalar(Tensor(a!)[] self, Tensor[] tensors1, Scalar weight) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensors are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_lerp_list_kernel_slow_","    CUDA: foreach_tensor_lerp_list_cuda_","  autogen: _foreach_lerp.Scalar_out","","- func: _foreach_lerp.ScalarList(Tensor[] self, Tensor[] tensors1, Scalar[] weight) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensors are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_lerp_scalarlist_kernel_slow","    CUDA: foreach_tensor_lerp_scalarlist_cuda","  autogen: _foreach_lerp.ScalarList_out","","- func: _foreach_lerp_.ScalarList(Tensor(a!)[] self, Tensor[] tensors1, Scalar[] weight) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensors are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_lerp_scalarlist_kernel_slow_","    CUDA: foreach_tensor_lerp_scalarlist_cuda_","  autogen: _foreach_lerp.ScalarList_out","","- func: _foreach_lgamma(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_lgamma_slow","    CUDA: foreach_tensor_lgamma_cuda","","- func: _foreach_lgamma_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_lgamma_slow_","    CUDA: foreach_tensor_lgamma_cuda_","  autogen: _foreach_lgamma.out","","- func: _foreach_log(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_log_slow","    CUDA: foreach_tensor_log_cuda","","- func: _foreach_log_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_log_slow_","    CUDA: foreach_tensor_log_cuda_","  autogen: _foreach_log.out","","- func: _foreach_log10(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_log10_slow","    CUDA: foreach_tensor_log10_cuda","","- func: _foreach_log10_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_log10_slow_","    CUDA: foreach_tensor_log10_cuda_","  autogen: _foreach_log10.out","","- func: _foreach_log1p(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_log1p_slow","    CUDA: foreach_tensor_log1p_cuda","","- func: _foreach_log1p_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_log1p_slow_","    CUDA: foreach_tensor_log1p_cuda_","  autogen: _foreach_log1p.out","","- func: _foreach_log2(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_log2_slow","    CUDA: foreach_tensor_log2_cuda","","- func: _foreach_log2_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_log2_slow_","    CUDA: foreach_tensor_log2_cuda_","  autogen: _foreach_log2.out","","- func: _foreach_max(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_max_slow","    CUDA: foreach_tensor_max_cuda","  autogen: _foreach_max.out","","- func: _foreach_neg(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_neg_slow","    CUDA: foreach_tensor_neg_cuda","","- func: _foreach_neg_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_neg_slow_","    CUDA: foreach_tensor_neg_cuda_","  autogen: _foreach_neg.out","","- func: _foreach_norm.Scalar(Tensor[] self, Scalar ord=2, ScalarType? dtype=None) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_norm_slow","    CUDA: foreach_tensor_norm_cuda","    MTIA: foreach_tensor_norm_mtia","  autogen: _foreach_norm.Scalar_out","","- func: _foreach_pow.List(Tensor[] self, Tensor[] exponent) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_pow_list_kernel_slow","    CUDA: foreach_tensor_pow_list_kernel_cuda","","- func: _foreach_pow.Scalar(Tensor[] self, Scalar exponent) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_pow_scalar_kernel_slow","    CUDA: foreach_tensor_pow_scalar_kernel_cuda","","- func: _foreach_pow.ScalarList(Tensor[] self, Scalar[] exponent) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_pow_scalarlist_kernel_slow","    CUDA: foreach_tensor_pow_scalarlist_kernel_cuda","","- func: _foreach_pow.ScalarAndTensor(Scalar self, Tensor[] exponent) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_scalar_pow_list_kernel_slow","    CUDA: foreach_scalar_pow_list_kernel_cuda","","- func: _foreach_pow_.List(Tensor(a!)[] self, Tensor[] exponent) -\u003e ()","  device_check: NoCheck","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_pow_list_kernel_slow_","    CUDA: foreach_tensor_pow_list_kernel_cuda_","  autogen: _foreach_pow.List_out","","- func: _foreach_pow_.Scalar(Tensor(a!)[] self, Scalar exponent) -\u003e ()","  device_check: NoCheck","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_pow_scalar_kernel_slow_","    CUDA: foreach_tensor_pow_scalar_kernel_cuda_","  autogen: _foreach_pow.Scalar_out","","- func: _foreach_pow_.ScalarList(Tensor(a!)[] self, Scalar[] exponent) -\u003e ()","  device_check: NoCheck","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_pow_scalarlist_kernel_slow_","    CUDA: foreach_tensor_pow_scalarlist_kernel_cuda_","  autogen: _foreach_pow.ScalarList_out","","- func: _foreach_reciprocal(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_reciprocal_slow","    CUDA: foreach_tensor_reciprocal_cuda","","- func: _foreach_reciprocal_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_reciprocal_slow_","    CUDA: foreach_tensor_reciprocal_cuda_","  autogen: _foreach_reciprocal.out","","- func: _foreach_round(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_round_slow","    CUDA: foreach_tensor_round_cuda","","- func: _foreach_round_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_round_slow_","    CUDA: foreach_tensor_round_cuda_","  autogen: _foreach_round.out","","- func: _foreach_rsqrt(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_rsqrt_slow","    CUDA: foreach_tensor_rsqrt_cuda","","- func: _foreach_rsqrt_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_rsqrt_slow_","    CUDA: foreach_tensor_rsqrt_cuda_","  autogen: _foreach_rsqrt.out","","- func: _foreach_sigmoid(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sigmoid_slow","    CUDA: foreach_tensor_sigmoid_cuda","","- func: _foreach_sigmoid_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sigmoid_slow_","    CUDA: foreach_tensor_sigmoid_cuda_","  autogen: _foreach_sigmoid.out","","- func: _foreach_sign(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sign_slow","    CUDA: foreach_tensor_sign_cuda","","- func: _foreach_sign_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sign_slow_","    CUDA: foreach_tensor_sign_cuda_","  autogen: _foreach_sign.out","","- func: _foreach_sin(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sin_slow","    CUDA: foreach_tensor_sin_cuda","","- func: _foreach_sin_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sin_slow_","    CUDA: foreach_tensor_sin_cuda_","  autogen: _foreach_sin.out","","- func: _foreach_sinh(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sinh_slow","    CUDA: foreach_tensor_sinh_cuda","","- func: _foreach_sinh_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sinh_slow_","    CUDA: foreach_tensor_sinh_cuda_","  autogen: _foreach_sinh.out","","- func: _foreach_sqrt(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sqrt_slow","    CUDA: foreach_tensor_sqrt_cuda","","- func: _foreach_sqrt_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_sqrt_slow_","    CUDA: foreach_tensor_sqrt_cuda_","    MTIA: foreach_tensor_sqrt_mtia_","  autogen: _foreach_sqrt.out","","- func: _foreach_tan(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_tan_slow","    CUDA: foreach_tensor_tan_cuda","","- func: _foreach_tan_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_tan_slow_","    CUDA: foreach_tensor_tan_cuda_","  autogen: _foreach_tan.out","","- func: _foreach_tanh(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_tanh_slow","    CUDA: foreach_tensor_tanh_cuda","","- func: _foreach_tanh_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_tanh_slow_","    CUDA: foreach_tensor_tanh_cuda_","  autogen: _foreach_tanh.out","","- func: _foreach_trunc(Tensor[] self) -\u003e Tensor[]","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_trunc_slow","    CUDA: foreach_tensor_trunc_cuda","","- func: _foreach_trunc_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_trunc_slow_","    CUDA: foreach_tensor_trunc_cuda_","  autogen: _foreach_trunc.out","","- func: _foreach_zero_(Tensor(a!)[] self) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_zero_slow_","    CUDA: foreach_tensor_zero_cuda_","  autogen: _foreach_zero, _foreach_zero.out","","- func: _foreach_copy_(Tensor(a!)[] self, Tensor[] src, bool non_blocking=False) -\u003e ()","  device_check: NoCheck   # foreach kernels fall back to slow path when tensor are on different devices","  variants: function","  dispatch:","    CompositeExplicitAutograd: foreach_tensor_copy_list_kernel_slow_","    CUDA: foreach_tensor_copy_list_kernel_cuda_","    MTIA: foreach_tensor_copy_list_kernel_mtia_","  autogen: _foreach_copy.out","","- func: _foreach_copy(Tensor[] self, Tensor[] src, bool non_blocking=False) -\u003e Tensor[] self_out","  device_check: NoCheck","  variants: function","  dispatch:","    CompositeExplicitAutograd: _foreach_copy","    MTIA: foreach_tensor_copy_list_kernel_mtia","","- func: bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -\u003e Tensor","  dispatch:","    CPU: bucketize_cpu","    CUDA: bucketize_cuda","    MPS: bucketize_mps","","- func: bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU: bucketize_out_cpu","    CUDA: bucketize_out_cuda","    MPS: bucketize_out_mps","","- func: bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -\u003e Tensor","  dispatch:","    CPU: bucketize_cpu","    CUDA: bucketize_cuda","    MPS: bucketize_mps","  autogen: bucketize.Scalar_out","","- func: searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -\u003e Tensor","  dispatch:","    CPU: searchsorted_cpu","    CUDA: searchsorted_cuda","    MPS: searchsorted_mps","","- func: searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU: searchsorted_out_cpu","    CUDA: searchsorted_out_cuda","    MPS: searchsorted_out_mps","","- func: searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -\u003e Tensor","  dispatch:","    CPU: searchsorted_cpu","    CUDA: searchsorted_cuda","    MPS: searchsorted_mps","","- func: searchsorted.Scalar_out(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU: searchsorted_out_cpu","    CUDA: searchsorted_out_cuda","    MPS: searchsorted_out_mps","","- func: _convert_indices_from_coo_to_csr(Tensor self, int size, *, bool out_int32=False) -\u003e Tensor","  structured_delegate: _convert_indices_from_coo_to_csr.out","","- func: _convert_indices_from_coo_to_csr.out(Tensor self, int size, *, bool out_int32=False, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: _convert_indices_from_coo_to_csr_structured_cpu","    CUDA: _convert_indices_from_coo_to_csr_structured_cuda","","- func: _convert_indices_from_csr_to_coo(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False) -\u003e Tensor","  structured_delegate: _convert_indices_from_csr_to_coo.out","","- func: _convert_indices_from_csr_to_coo.out(Tensor crow_indices, Tensor col_indices, *, bool out_int32=False, bool transpose=False, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  dispatch:","    CPU: _convert_indices_from_csr_to_coo_structured_cpu","    CUDA: _convert_indices_from_csr_to_coo_structured_cuda","","## NN wrappers","","- func: mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA: mse_loss_out","    MPS: mse_loss_out_mps","","- func: mse_loss(Tensor self, Tensor target, int reduction=Mean) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: mse_loss.out","  python_module: nn","","- func: mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU, CUDA: mse_loss_backward_out","    MPS: mse_loss_backward_out_mps","","- func: mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU, CUDA: mse_loss_backward","    MPS: mse_loss_backward_mps","","- func: l1_loss(Tensor self, Tensor target, int reduction=Mean) -\u003e Tensor","  python_module: nn","","- func: multi_margin_loss.out(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: multi_margin_loss_cpu_out","    CUDA: multi_margin_loss_cuda_out","","- func: multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: multi_margin_loss_cpu","    CUDA: multi_margin_loss_cuda","","- func: multi_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: multi_margin_loss_cpu_backward_out","    CUDA: multi_margin_loss_cuda_backward_out","","- func: multi_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, Scalar p, Scalar margin, Tensor? weight=None, int reduction=Mean) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: multi_margin_loss_cpu_backward","    CUDA: multi_margin_loss_cuda_backward","","- func: multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","","- func: multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -\u003e Tensor","  python_module: nn","","- func: multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -\u003e (Tensor(a!), Tensor(b!))","  python_module: nn","  dispatch:","    CPU: multilabel_margin_loss_forward_out_cpu","    CUDA: multilabel_margin_loss_forward_out_cuda","","- func: multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -\u003e (Tensor output, Tensor is_target)","  python_module: nn","  dispatch:","    CPU: multilabel_margin_loss_forward_cpu","    CUDA: multilabel_margin_loss_forward_cuda","","- func: multilabel_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: multilabel_margin_loss_backward_cpu_out","    CUDA: multilabel_margin_loss_backward_cuda_out","","- func: multilabel_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, Tensor is_target) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: multilabel_margin_loss_backward_cpu","    CUDA: multilabel_margin_loss_backward_cuda","","- func: nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","","- func: nll_loss_nd(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: nll_loss_nd_symint","","- func: nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: nll_loss_symint","","- func: nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -\u003e (Tensor(a!), Tensor(b!))","  python_module: nn","  structured: True","  dispatch:","    CPU: nll_loss_forward_out_cpu","    CUDA: nll_loss_forward_out_cuda","    MPS: nll_loss_forward_out_mps","","- func: nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -\u003e (Tensor output, Tensor total_weight)","  python_module: nn","  structured_delegate: nll_loss_forward.output","","- func: nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: nll_loss_backward_out_cpu","    CUDA: nll_loss_backward_out_cuda","    MPS: nll_loss_backward_out_mps","","- func: nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -\u003e Tensor","  python_module: nn","  structured_delegate: nll_loss_backward.grad_input","","- func: nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","","- func: nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: nll_loss2d_symint","","- func: nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -\u003e (Tensor(a!), Tensor(b!))","  python_module: nn","  dispatch:","    CPU: nll_loss2d_forward_out_cpu","    CUDA: nll_loss2d_forward_out_cuda","    MPS: nll_loss2d_forward_out_mps","","- func: nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -\u003e (Tensor output, Tensor total_weight)","  python_module: nn","  dispatch:","    CPU: nll_loss2d_forward_cpu","    CUDA: nll_loss2d_forward_cuda","    MPS: nll_loss2d_forward_mps","","- func: nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: nll_loss2d_backward_out_cpu","    CUDA: nll_loss2d_backward_out_cuda","    MPS: nll_loss2d_backward_out_mps","","- func: nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: nll_loss2d_backward_cpu","    CUDA: nll_loss2d_backward_cuda","    MPS: nll_loss2d_backward_mps","","- func: smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA: smooth_l1_loss_out","    MPS: smooth_l1_loss_out_mps","","- func: smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  structured_delegate: smooth_l1_loss.out","  python_module: nn","","- func: smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: smooth_l1_loss_backward_out","    CUDA: smooth_l1_loss_backward_out","    MPS: smooth_l1_loss_backward_out_mps","","- func: smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeExplicitAutograd: smooth_l1_loss_backward","","- func: huber_loss.out(Tensor self, Tensor target, int reduction=Mean, float delta=1.0, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU, CUDA: huber_loss_out","    MPS: huber_loss_out_mps","","- func: huber_loss(Tensor self, Tensor target, int reduction=Mean, float delta=1.0) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU, CUDA: huber_loss","    MPS: huber_loss_mps","","- func: huber_loss_backward.out(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU, CUDA: huber_loss_backward_out","    MPS: huber_loss_backward_out_mps","","- func: huber_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float delta) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeExplicitAutograd: huber_loss_backward","","- func: soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CompositeExplicitAutograd: soft_margin_loss_out","","- func: soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeExplicitAutograd: soft_margin_loss","","- func: soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CompositeExplicitAutograd: soft_margin_loss_backward_out","","- func: soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeExplicitAutograd: soft_margin_loss_backward","","- func: elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA: elu_out","    MPS: elu_out_mps","","- func: elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -\u003e Tensor","  structured_delegate: elu.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","  tags: [core, pointwise]","","- func: elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA: elu_backward_out","    MPS: elu_backward_out_mps","","- func: elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -\u003e Tensor","  structured_delegate: elu_backward.grad_input","  python_module: nn","","- func: elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -\u003e Tensor(a!)","  structured_delegate: elu.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","","- func: glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA: glu_out","    MPS: glu_out_mps","","- func: glu(Tensor self, int dim=-1) -\u003e Tensor","  structured_delegate: glu.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","","- func: glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: glu_backward_cpu_out","    CUDA: glu_backward_cuda_out","    MPS: glu_backward_mps_out","","- func: glu_backward(Tensor grad_output, Tensor self, int dim) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: glu_backward_cpu","    CUDA: glu_backward_cuda","    MPS: glu_backward_mps","","- func: glu_jvp(Tensor glu, Tensor x, Tensor dx, int dim) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU, CUDA: glu_jvp","  autogen: glu_jvp.out","","- func: glu_backward_jvp(Tensor grad_x, Tensor grad_glu, Tensor x, Tensor dgrad_glu, Tensor dx, int dim) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU, CUDA: glu_backward_jvp","  autogen: glu_backward_jvp.out","","- func: hardsigmoid.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: hardsigmoid_out","    QuantizedCPU: hardsigmoid_out_quantized_cpu","","- func: hardsigmoid(Tensor self) -\u003e Tensor","  structured_delegate: hardsigmoid.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    QuantizedCPU: hardsigmoid_quantized_cpu","  tags: pointwise","","- func: hardsigmoid_(Tensor(a!) self) -\u003e Tensor(a!)","  structured_delegate: hardsigmoid.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","","- func: hardsigmoid_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: hardsigmoid_backward_out","","- func: hardsigmoid_backward(Tensor grad_output, Tensor self) -\u003e Tensor","  structured_delegate: hardsigmoid_backward.grad_input","  python_module: nn","","- func: hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: hardtanh_out","    QuantizedCPU: hardtanh_out_quantized_cpu","","- func: hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: hardtanh","    QuantizedCPU: hardtanh_quantized_cpu","  tags: [pointwise, core]","","- func: hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU, CUDA: hardtanh_backward_out","    MPS: hardtanh_backward_out_mps","","- func: hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU, CUDA: hardtanh_backward","    MPS: hardtanh_backward_mps","","- func: hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: hardtanh_","    QuantizedCPU: hardtanh_quantized_cpu_","","- func: hardswish.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: hardswish_out","","- func: hardswish(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: hardswish","","- func: hardswish_(Tensor(a!) self) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: hardswish_","","- func: hardswish_backward(Tensor grad_output, Tensor self) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: hardswish_backward","  autogen: hardswish_backward.out","","- func: leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: leaky_relu_out","    QuantizedCPU: leaky_relu_out_quantized_cpu","","- func: leaky_relu(Tensor self, Scalar negative_slope=0.01) -\u003e Tensor","  structured_delegate: leaky_relu.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    QuantizedCPU: leaky_relu_quantized_cpu","  tags: core","","- func: leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: leaky_relu_backward_out","","- func: leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -\u003e Tensor","  structured_delegate: leaky_relu_backward.grad_input","  python_module: nn","","- func: leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -\u003e Tensor(a!)","  structured_delegate: leaky_relu.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    QuantizedCPU: leaky_relu_quantized_cpu_","","- func: log_sigmoid.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: nn","","- func: log_sigmoid(Tensor self) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: nn","","- func: log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -\u003e (Tensor(a!), Tensor(b!))","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU: log_sigmoid_forward_out_cpu","    CUDA: log_sigmoid_forward_out_cuda","    MPS: log_sigmoid_forward_out_mps","","- func: log_sigmoid_forward(Tensor self) -\u003e (Tensor output, Tensor buffer)","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU: log_sigmoid_forward_cpu","    CUDA: log_sigmoid_forward_cuda","    MPS: log_sigmoid_forward_mps","","- func: log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: log_sigmoid_backward_cpu_out","    CUDA: log_sigmoid_backward_cuda_out","    MPS: log_sigmoid_backward_mps_out","","- func: log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: log_sigmoid_backward_cpu","    CUDA: log_sigmoid_backward_cuda","    MPS: log_sigmoid_backward_mps","","- func: rrelu_with_noise.out(Tensor self, Tensor(b!) noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  tags: nondeterministic_seeded","  dispatch:","    CPU: rrelu_with_noise_out_cpu","    CUDA: rrelu_with_noise_out_cuda","","- func: rrelu_with_noise(Tensor self, Tensor(b!) noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: rrelu_with_noise_cpu","    CUDA: rrelu_with_noise_cuda","  tags: nondeterministic_seeded","  autogen: rrelu_with_noise_functional","","- func: rrelu_with_noise_backward(Tensor grad_output, Tensor self, Tensor noise, Scalar lower, Scalar upper, bool training, bool self_is_result) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeExplicitAutograd: rrelu_with_noise_backward","  autogen: rrelu_with_noise_backward.out","","- func: rrelu_with_noise_(Tensor(a!) self, Tensor(b!) noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -\u003e Tensor(a!)","  python_module: nn","  tags: nondeterministic_seeded","  dispatch:","    CPU: rrelu_with_noise_cpu_","    CUDA: rrelu_with_noise_cuda_","","- func: softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA: softplus_out","    MPS: softplus_out_mps","","- func: softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -\u003e Tensor","  structured_delegate: softplus.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","  tags: pointwise","","- func: softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA: softplus_backward_out","    MPS: softplus_backward_out_mps","","- func: softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold) -\u003e Tensor","  structured_delegate: softplus_backward.grad_input","  python_module: nn","","- func: softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  device_check: NoCheck   # TensorIterator","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: softshrink_out","","- func: softshrink(Tensor self, Scalar lambd=0.5) -\u003e Tensor","  structured_delegate: softshrink.out","  device_check: NoCheck   # TensorIterator","  python_module: nn","  tags: pointwise","","- func: softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: nn","  dispatch:","    CPU, CUDA, MPS: softshrink_backward_out","","- func: softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -\u003e Tensor","  structured_delegate: softshrink_backward.grad_input","  python_module: nn","","- func: adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: adaptive_avg_pool2d_out_cpu","    CUDA: adaptive_avg_pool2d_out_cuda","    MPS: adaptive_avg_pool2d_out_mps","    MkldnnCPU: mkldnn_adaptive_avg_pool2d_out_stub","","- func: adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: adaptive_avg_pool2d_symint","","- func: mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -\u003e Tensor","  dispatch:","    MkldnnCPU: mkldnn_adaptive_avg_pool2d","","- func: mkldnn_adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    MkldnnCPU: mkldnn_adaptive_avg_pool2d_out","","- func: mkldnn_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -\u003e Tensor","  dispatch:","    MkldnnCPU: mkldnn_adaptive_avg_pool2d_backward","  autogen: mkldnn_adaptive_avg_pool2d_backward.out","","- func: _adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -\u003e Tensor","  dispatch:","    CPU: adaptive_avg_pool2d_cpu","    CUDA: adaptive_avg_pool2d_cuda","    MPS: adaptive_avg_pool2d_mps","    QuantizedCPU: adaptive_avg_pool2d_quantized_cpu","    QuantizedCUDA: adaptive_avg_pool2d_quantized_cuda","  autogen: _adaptive_avg_pool2d.out","  tags: core","","- func: _adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: adaptive_avg_pool2d_backward_cpu","    CUDA: adaptive_avg_pool2d_backward_cuda","    MPS: adaptive_avg_pool2d_backward_mps","  autogen: _adaptive_avg_pool2d_backward.out","  tags: core","","- func: adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: adaptive_avg_pool3d_out_cpu","    CUDA: adaptive_avg_pool3d_out_cuda","    QuantizedCPU: adaptive_avg_pool3d_out_quantized_cpu","","- func: adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: adaptive_avg_pool3d_symint","","- func: _adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -\u003e Tensor","  dispatch:","    CPU: adaptive_avg_pool3d_cpu","    CUDA: adaptive_avg_pool3d_cuda","    QuantizedCPU: adaptive_avg_pool3d_quantized_cpu","  autogen: _adaptive_avg_pool3d.out","  tags: core","","- func: adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: adaptive_avg_pool3d_backward_out_cpu","    CUDA: adaptive_avg_pool3d_backward_out_cuda","","- func: _adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: adaptive_avg_pool3d_backward_cpu","    CUDA: adaptive_avg_pool3d_backward_cuda","  autogen: _adaptive_avg_pool3d_backward.out","","# Return: (Tensor output, Tensor indices)","- func: adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -\u003e (Tensor(a!), Tensor(b!))","  python_module: nn","  structured: True","  dispatch:","    CPU: adaptive_max_pool2d_out_cpu","    CUDA: adaptive_max_pool2d_out_cuda","    MPS: adaptive_max_pool2d_out_mps","","# Return: (Tensor output, Tensor indices)","- func: adaptive_max_pool2d(Tensor self, int[2] output_size) -\u003e (Tensor, Tensor)","  python_module: nn","  structured_delegate: adaptive_max_pool2d.out","","- func: adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: adaptive_max_pool2d_backward_out_cpu","    CUDA: adaptive_max_pool2d_backward_out_cuda","    MPS: adaptive_max_pool2d_backward_out_mps","","- func: adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -\u003e Tensor","  python_module: nn","  structured_delegate: adaptive_max_pool2d_backward.grad_input","","# Return: (Tensor output, Tensor indices)","- func: adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -\u003e (Tensor(a!), Tensor(b!))","  python_module: nn","  structured: True","  dispatch:","    CPU: adaptive_max_pool3d_out_cpu","    CUDA: adaptive_max_pool3d_out_cuda","","# Return: (Tensor output, Tensor indices)","- func: adaptive_max_pool3d(Tensor self, int[3] output_size) -\u003e (Tensor, Tensor)","  python_module: nn","  structured_delegate: adaptive_max_pool3d.out","","- func: adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: adaptive_max_pool3d_backward_out_cpu","    CUDA: adaptive_max_pool3d_backward_out_cuda","","- func: adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -\u003e Tensor","  python_module: nn","  structured_delegate: adaptive_max_pool3d_backward.grad_input","","- func: avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  precomputed:","  - kernel_size -\u003e int kH, int kW","  - stride -\u003e int dH, int dW","  - padding -\u003e int padH, int padW","  dispatch:","    CPU: avg_pool2d_out_cpu","    CUDA: avg_pool2d_out_cuda","    MPS: avg_pool2d_out_mps","    MkldnnCPU: mkldnn_avg_pool2d_out","","- func: avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -\u003e Tensor","  python_module: nn","  structured_delegate: avg_pool2d.out","  dispatch:","    MkldnnCPU: mkldnn_avg_pool2d","    QuantizedCPU: avg_pool2d_quantized_cpu","  tags: core","","- func: avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: avg_pool2d_backward_out_cpu","    CUDA: avg_pool2d_backward_out_cuda","    MPS: avg_pool2d_backward_out_mps","    MkldnnCPU: mkldnn_avg_pool2d_backward_out","","- func: avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -\u003e Tensor","  python_module: nn","  structured_delegate: avg_pool2d_backward.grad_input","  dispatch:","    MkldnnCPU: mkldnn_avg_pool2d_backward","  tags: core","","- func: avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: avg_pool3d_out_cpu","    CUDA: avg_pool3d_out_cuda","    MPS: avg_pool3d_out_mps","    MkldnnCPU: mkldnn_avg_pool3d_out","","- func: avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -\u003e Tensor","  python_module: nn","  structured_delegate: avg_pool3d.out","  dispatch:","    MkldnnCPU: mkldnn_avg_pool3d","    QuantizedCPU: avg_pool3d_quantized_cpu","  tags: core","","- func: avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: avg_pool3d_backward_out_cpu","    CUDA: avg_pool3d_backward_out_cuda","    MPS: avg_pool3d_backward_out_mps","    MkldnnCPU: mkldnn_avg_pool3d_backward_out","","- func: avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -\u003e Tensor","  python_module: nn","  structured_delegate: avg_pool3d_backward.grad_input","  dispatch:","    MkldnnCPU: mkldnn_avg_pool3d_backward","","# Return: (Tensor output, Tensor indices)","- func: fractional_max_pool2d.output(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -\u003e (Tensor(a!), Tensor(b!))","  python_module: nn","  structured: True","  dispatch:","    CPU: fractional_max_pool2d_out_cpu","    CUDA: fractional_max_pool2d_out_cuda","","# Return: (Tensor output, Tensor indices)","- func: fractional_max_pool2d(Tensor self, int[2] kernel_size, int[2] output_size, Tensor random_samples) -\u003e (Tensor, Tensor)","  python_module: nn","  structured_delegate: fractional_max_pool2d.output","","- func: fractional_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: fractional_max_pool2d_backward_cpu","    CUDA: fractional_max_pool2d_backward_cuda","","- func: fractional_max_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] output_size, Tensor indices) -\u003e Tensor","  python_module: nn","  structured_delegate: fractional_max_pool2d_backward.grad_input","","# Return: (Tensor output, Tensor indices)","- func: fractional_max_pool3d.output(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples, *, Tensor(a!) output, Tensor(b!) indices) -\u003e (Tensor(a!), Tensor(b!))","  python_module: nn","  structured: True","  precomputed:","  - kernel_size -\u003e int poolSizeT, int poolSizeH, int poolSizeW","  - output_size -\u003e int outputT, int outputH, int outputW","  - int numBatch, int numPlanes, int inputT, int inputH, int inputW","  dispatch:","    CPU: fractional_max_pool3d_out_cpu","    CUDA: fractional_max_pool3d_out_cuda","","# Return: (Tensor output, Tensor indices)","- func: fractional_max_pool3d(Tensor self, int[3] kernel_size, int[3] output_size, Tensor random_samples) -\u003e (Tensor, Tensor)","  python_module: nn","  structured_delegate: fractional_max_pool3d.output","","- func: fractional_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: fractional_max_pool3d_backward_out_cpu","    CUDA: fractional_max_pool3d_backward_out_cuda","","- func: fractional_max_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] output_size, Tensor indices) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: fractional_max_pool3d_backward_cpu","    CUDA: fractional_max_pool3d_backward_cuda","","# Return: (Tensor output, Tensor indices)","- func: max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -\u003e (Tensor(a!), Tensor(b!))","  python_module: nn","  structured: True","  dispatch:","    CPU: max_pool2d_with_indices_out_cpu","    CUDA: max_pool2d_with_indices_out_cuda","    MPS: max_pool2d_with_indices_out_mps","","# Return: (Tensor output, Tensor indices)","- func: max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -\u003e (Tensor, Tensor)","  python_module: nn","  structured_delegate: max_pool2d_with_indices.out","  tags: core","","- func: max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: max_pool2d_with_indices_backward_out_cpu","    CUDA: max_pool2d_with_indices_backward_out_cuda","    MPS: max_pool2d_with_indices_backward_out_mps","","- func: max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -\u003e Tensor","  python_module: nn","  structured_delegate: max_pool2d_with_indices_backward.grad_input","  tags: core","","# Return: (Tensor output, Tensor indices)","- func: max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -\u003e (Tensor(a!), Tensor(b!))","  python_module: nn","  dispatch:","    CPU: max_pool3d_with_indices_out_cpu","    CUDA: max_pool3d_with_indices_out_cuda","    MPS: max_pool3d_with_indices_out_mps","","# Return: (Tensor output, Tensor indices)","- func: max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -\u003e (Tensor, Tensor)","  python_module: nn","  dispatch:","    CPU: max_pool3d_with_indices_cpu","    CUDA: max_pool3d_with_indices_cuda","    MPS: max_pool3d_with_indices_mps","  tags: core","","- func: max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: max_pool3d_with_indices_backward_out_cpu","    CUDA: max_pool3d_with_indices_backward_out_cuda","    MPS: max_pool3d_with_indices_backward_out_mps","","- func: max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: max_pool3d_with_indices_backward_cpu","    CUDA: max_pool3d_with_indices_backward_cuda","    MPS: max_pool3d_with_indices_backward_mps","","- func: max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: max_unpooling2d_forward_out_cpu","    CUDA: max_unpooling2d_forward_out_cuda","    MPS: max_unpooling2d_forward_out_mps","","- func: max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: max_unpooling2d_forward_cpu","    CUDA: max_unpooling2d_forward_cuda","    MPS: max_unpooling2d_forward_mps","","- func: max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: max_unpooling3d_forward_out_cpu","    CUDA: max_unpooling3d_forward_out_cuda","    MPS: max_unpooling3d_forward_out_mps","","- func: max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: max_unpooling3d_forward_cpu","    CUDA: max_unpooling3d_forward_cuda","    MPS: max_unpooling3d_forward_mps","","- func: reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: reflection_pad1d_out_cpu","    QuantizedCPU: reflection_pad1d_out_quantized_cpu","    CUDA: reflection_pad1d_out_cuda","    MPS: reflection_pad1d_out_mps","","- func: reflection_pad1d(Tensor self, SymInt[2] padding) -\u003e Tensor","  python_module: nn","  structured_delegate: reflection_pad1d.out","  tags: core","","- func: reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: reflection_pad1d_backward_out_cpu","    CUDA: reflection_pad1d_backward_out_cuda","    MPS: reflection_pad1d_backward_out_mps","","- func: reflection_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -\u003e Tensor","  python_module: nn","  structured_delegate: reflection_pad1d_backward.grad_input","","- func: reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU, QuantizedCPU: reflection_pad2d_out_cpu","    CUDA: reflection_pad2d_out_cuda","    MPS: reflection_pad2d_out_mps","","- func: reflection_pad2d(Tensor self, SymInt[4] padding) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: reflection_pad2d_cpu","    QuantizedCPU: reflection_pad2d_quantized_cpu","    CUDA: reflection_pad2d_cuda","    MPS: reflection_pad2d_mps","  tags: core","","- func: reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: reflection_pad2d_backward_out_cpu","    CUDA: reflection_pad2d_backward_out_cuda","    MPS: reflection_pad2d_backward_out_mps","","- func: reflection_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: reflection_pad2d_backward_cpu","    CUDA: reflection_pad2d_backward_cuda","    MPS: reflection_pad2d_backward_mps","","- func: reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: reflection_pad3d_out_cpu","    CUDA: reflection_pad3d_out_cuda","    MPS: reflection_pad3d_out_mps","","- func: reflection_pad3d(Tensor self, SymInt[6] padding) -\u003e Tensor","  python_module: nn","  structured_delegate: reflection_pad3d.out","  tags: core","","- func: reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: reflection_pad3d_backward_out_cpu","    CUDA: reflection_pad3d_backward_out_cuda","    MPS: reflection_pad3d_backward_out_mps","","- func: reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -\u003e Tensor","  python_module: nn","  structured_delegate: reflection_pad3d_backward.grad_input","","- func: replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: replication_pad1d_out_cpu","    CUDA: replication_pad1d_out_cuda","    MPS: replication_pad1d_out_mps","","- func: replication_pad1d(Tensor self, SymInt[2] padding) -\u003e Tensor","  python_module: nn","  structured_delegate: replication_pad1d.out","","- func: replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: replication_pad1d_backward_out_cpu","    CUDA: replication_pad1d_backward_out_cuda","    MPS: replication_pad1d_backward_out_mps","","- func: replication_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -\u003e Tensor","  python_module: nn","  structured_delegate: replication_pad1d_backward.grad_input","","- func: replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: replication_pad2d_out_cpu","    CUDA: replication_pad2d_out_cuda","    MPS: replication_pad2d_out_mps","","- func: replication_pad2d(Tensor self, SymInt[4] padding) -\u003e Tensor","  python_module: nn","  structured_delegate: replication_pad2d.out","  tags: core","","- func: replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: replication_pad2d_backward_out_cpu","    CUDA: replication_pad2d_backward_out_cuda","    MPS: replication_pad2d_backward_out_mps","","- func: replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: replication_pad2d_backward_cpu","    CUDA: replication_pad2d_backward_cuda","    MPS: replication_pad2d_backward_mps","","- func: replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: replication_pad3d_out_cpu","    CUDA: replication_pad3d_out_cuda","    MPS: replication_pad3d_out_mps","","- func: replication_pad3d(Tensor self, SymInt[6] padding) -\u003e Tensor","  python_module: nn","  structured_delegate: replication_pad3d.out","  tags: core","","","- func: replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: replication_pad3d_backward_out_cpu","    CUDA: replication_pad3d_backward_out_cuda","    MPS: replication_pad3d_backward_out_mps","","- func: replication_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: replication_pad3d_backward_cpu","    CUDA: replication_pad3d_backward_cuda","    MPS: replication_pad3d_backward_mps","","- func: _pad_circular(Tensor self, SymInt[] pad) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: _pad_circular_symint","","- func: _pad_enum(Tensor self, SymInt[] pad, int mode, float? value=None) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: _pad_enum_symint","","- func: pad(Tensor self, SymInt[] pad, str mode=\"constant\", float? value=None) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeImplicitAutograd: pad_symint","","- func: upsample_linear1d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: upsample_linear1d.vec_out","","- func: upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: upsample_bilinear2d.vec_out","  tags: core","","- func: _upsample_bilinear2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: _upsample_bilinear2d_aa.vec_out","","- func: upsample_trilinear3d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: upsample_trilinear3d.vec_out","","- func: upsample_bicubic2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: upsample_bicubic2d.vec_out","","- func: _upsample_bicubic2d_aa.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: _upsample_bicubic2d_aa.vec_out","","- func: upsample_nearest1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: upsample_nearest1d.vec_out","","- func: _upsample_nearest_exact1d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: _upsample_nearest_exact1d.vec_out","","- func: upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: upsample_nearest2d.vec_out","  tags: core","","- func: _upsample_nearest_exact2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: _upsample_nearest_exact2d.vec_out","","- func: upsample_nearest3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: upsample_nearest3d.vec_out","","- func: _upsample_nearest_exact3d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -\u003e Tensor","  python_module: nn","  autogen: _upsample_nearest_exact3d.vec_out","","# NOTE: all of the non-\"vec\" upsample overloads are only kept for backward compatibility.","- func: upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_linear1d_out_cpu","    CUDA: upsample_linear1d_out_cuda","    MPS: upsample_linear1d_out_mps","","- func: upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_linear1d.out","","- func: upsample_linear1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_linear1d_backward_out_cpu","    CUDA: upsample_linear1d_backward_out_cuda","    MPS: upsample_linear1d_backward_out_mps","","- func: upsample_linear1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_linear1d_backward.grad_input","","- func: upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_bilinear2d_out_cpu","    CUDA: upsample_bilinear2d_out_cuda","    MPS: upsample_bilinear2d_out_mps","","- func: upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_bilinear2d.out","  dispatch:","    QuantizedCPU: upsample_bilinear2d_quantized_cpu","","- func: upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_bilinear2d_backward_out_cpu","    CUDA: upsample_bilinear2d_backward_out_cuda","    MPS: upsample_bilinear2d_backward_out_mps","","- func: upsample_bilinear2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_bilinear2d_backward.grad_input","","- func: _upsample_bilinear2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: _upsample_bilinear2d_aa_out_cpu","    CUDA: _upsample_bilinear2d_aa_out_cuda","    MPS: _upsample_bilinear2d_aa_out_mps","","- func: _upsample_bilinear2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: _upsample_bilinear2d_aa.out","","- func: _upsample_bilinear2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: _upsample_bilinear2d_aa_backward_out_cpu","    CUDA: _upsample_bilinear2d_aa_backward_out_cuda","","- func: _upsample_bilinear2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: _upsample_bilinear2d_aa_backward.grad_input","","- func: upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_bicubic2d_out_cpu","    CUDA: upsample_bicubic2d_out_cuda","    MPS: upsample_bicubic2d_out_mps","","- func: upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_bicubic2d.out","","- func: upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_bicubic2d_backward_out_cpu","    CUDA: upsample_bicubic2d_backward_out_cuda","    MPS: upsample_bicubic2d_backward_out_mps","","- func: upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_bicubic2d_backward.grad_input","","- func: _upsample_bicubic2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: _upsample_bicubic2d_aa_out_cpu","    CUDA: _upsample_bicubic2d_aa_out_cuda","    MPS: _upsample_bicubic2d_aa_out_mps","","- func: _upsample_bicubic2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: _upsample_bicubic2d_aa.out","","- func: _upsample_bicubic2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: _upsample_bicubic2d_aa_backward_out_cpu","    CUDA: _upsample_bicubic2d_aa_backward_out_cuda","","- func: _upsample_bicubic2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: _upsample_bicubic2d_aa_backward.grad_input","","- func: upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_trilinear3d_out_cpu","    CUDA: upsample_trilinear3d_out_cuda","    MPS: upsample_trilinear3d_out_mps","","- func: upsample_trilinear3d(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_trilinear3d.out","","- func: upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_trilinear3d_backward_out_cpu","    CUDA: upsample_trilinear3d_backward_out_cuda","    MPS: upsample_trilinear3d_backward_out_mps","","- func: upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_trilinear3d_backward.grad_input","","- func: upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_nearest1d_out_cpu","    CUDA: upsample_nearest1d_out_cuda","    MPS: upsample_nearest1d_out_mps","","- func: _upsample_nearest_exact1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: _upsample_nearest_exact1d_out_cpu","    CUDA: _upsample_nearest_exact1d_out_cuda","    MPS: _upsample_nearest_exact1d_out_mps","","- func: upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_nearest1d.out","","- func: _upsample_nearest_exact1d(Tensor self, SymInt[1] output_size, float? scales=None) -\u003e Tensor","  python_module: nn","  structured_delegate: _upsample_nearest_exact1d.out","","- func: upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_nearest1d_backward_out_cpu","    CUDA: upsample_nearest1d_backward_out_cuda","    MPS: upsample_nearest1d_backward_out_mps","","- func: _upsample_nearest_exact1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: _upsample_nearest_exact1d_backward_out_cpu","    CUDA: _upsample_nearest_exact1d_backward_out_cuda","    MPS: _upsample_nearest_exact1d_backward_out_mps","","- func: upsample_nearest1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_nearest1d_backward.grad_input","","- func: _upsample_nearest_exact1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -\u003e Tensor","  python_module: nn","  structured_delegate: _upsample_nearest_exact1d_backward.grad_input","","- func: upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_nearest2d_out_cpu","    CUDA: upsample_nearest2d_out_cuda","    MPS: upsample_nearest2d_out_mps","","- func: _upsample_nearest_exact2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: _upsample_nearest_exact2d_out_cpu","    CUDA: _upsample_nearest_exact2d_out_cuda","    MPS: _upsample_nearest_exact2d_out_mps","","- func: upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_nearest2d.out","  dispatch:","    QuantizedCPU: upsample_nearest2d_quantized_cpu","","- func: _upsample_nearest_exact2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: _upsample_nearest_exact2d.out","  dispatch:","    QuantizedCPU: _upsample_nearest_exact2d_quantized_cpu","","- func: upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_nearest2d_backward_out_cpu","    CUDA: upsample_nearest2d_backward_out_cuda","    MPS: upsample_nearest2d_backward_out_mps","","- func: _upsample_nearest_exact2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: _upsample_nearest_exact2d_backward_out_cpu","    CUDA: _upsample_nearest_exact2d_backward_out_cuda","    MPS: _upsample_nearest_exact2d_backward_out_mps","","- func: upsample_nearest2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_nearest2d_backward.grad_input","","- func: _upsample_nearest_exact2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: _upsample_nearest_exact2d_backward.grad_input","","- func: upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_nearest3d_out_cpu","    CUDA: upsample_nearest3d_out_cuda","    MPS: upsample_nearest3d_out_mps","","- func: _upsample_nearest_exact3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: _upsample_nearest_exact3d_out_cpu","    CUDA: _upsample_nearest_exact3d_out_cuda","    MPS: _upsample_nearest_exact3d_out_mps","","- func: upsample_nearest3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_nearest3d.out","  dispatch:","    QuantizedCPU: upsample_nearest3d_quantized_cpu","","- func: _upsample_nearest_exact3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: _upsample_nearest_exact3d.out","  dispatch:","    QuantizedCPU: _upsample_nearest_exact3d_quantized_cpu","","- func: upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: upsample_nearest3d_backward_out_cpu","    CUDA: upsample_nearest3d_backward_out_cuda","    MPS: upsample_nearest3d_backward_out_mps","","- func: _upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: _upsample_nearest_exact3d_backward_out_cpu","    CUDA: _upsample_nearest_exact3d_backward_out_cuda","    MPS: _upsample_nearest_exact3d_backward_out_mps","","- func: upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: upsample_nearest3d_backward.grad_input","","- func: _upsample_nearest_exact3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -\u003e Tensor","  python_module: nn","  structured_delegate: _upsample_nearest_exact3d_backward.grad_input","","- func: sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: sigmoid_backward_out","    MPS: sigmoid_backward_out_mps","  tags: pointwise","","- func: sigmoid_backward(Tensor grad_output, Tensor output) -\u003e Tensor","  python_module: nn","  structured_delegate: sigmoid_backward.grad_input","  tags: pointwise","","- func: logit_backward.grad_input(Tensor grad_output, Tensor self, float? eps=None, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: logit_backward_out","    MPS: logit_backward_out_mps","  tags: pointwise","","- func: logit_backward(Tensor grad_output, Tensor self, float? eps=None) -\u003e Tensor","  python_module: nn","  structured_delegate: logit_backward.grad_input","  tags: pointwise","","- func: tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MTIA: tanh_backward_out","    MPS: tanh_backward_out_mps","  tags: pointwise","","- func: tanh_backward(Tensor grad_output, Tensor output) -\u003e Tensor","  python_module: nn","  structured_delegate: tanh_backward.grad_input","","# What's a thnn_conv_ versus a slow_conv_?","#","# Historically, we have inefficient implementations of convolutions","# coming from the THNN/THCUNN library.  These convolutions typically","# operated by computing the Toeplitz matrix and then doing a matrix","# multiply with the input; this is very memory inefficient!  However,","# occasionally, we really don't have anything better, so it's helpful","# to have these fallbacks when there is no more optimized implementation","# in cudnn or mkldnn, etc.  Both thnn_ and slow_ convolutions fall","# into this bucket.","#","# The difference between these two designations, is that thnn_ refers","# to a convolution that is still written in the \"legacy\" style; that is,","# C code in the THNN/ or THCUNN/ directory.  A slow_ convolution is","# one that is written in the native style: modern C++.  Algorithmically,","# these are the same thing, but we give them different prefixes to","# make the operational distinction clear.","  tags: pointwise","","- func: slow_conv_transpose2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  structured: True","  dispatch:","    CPU: slow_conv_transpose2d_structured_cpu","    CUDA: slow_conv_transpose2d_structured_cuda","","- func: slow_conv_transpose2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, SymInt[2] dilation=1) -\u003e Tensor","  python_module: nn","  structured_delegate: slow_conv_transpose2d.out","","- func: slow_conv_transpose3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, SymInt[3] dilation=1, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: slow_conv_transpose3d_out_cpu","    CUDA: slow_conv_transpose3d_out_cuda","","- func: slow_conv_transpose3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, SymInt[3] dilation=1) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: slow_conv_transpose3d_cpu","    CUDA: slow_conv_transpose3d_cuda","","- func: thnn_conv2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","","- func: thnn_conv2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0) -\u003e Tensor","  python_module: nn","","- func: _slow_conv2d_forward.output(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) output) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: slow_conv2d_forward_out_cpu","    CUDA: slow_conv2d_forward_out_cuda","","- func: _slow_conv2d_forward(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: slow_conv2d_forward_cpu","    CUDA: slow_conv2d_forward_cuda","","- func: _slow_conv2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, *, Tensor(a!) grad_input, Tensor(b!) grad_weight, Tensor(c!) grad_bias) -\u003e (Tensor(a!), Tensor(b!), Tensor(c!))","  python_module: nn","  dispatch:","    CPU: slow_conv2d_backward_out_cpu","    CUDA: slow_conv2d_backward_out_cuda","","- func: _slow_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, SymInt[2] kernel_size, SymInt[2] stride, SymInt[2] padding, bool[3] output_mask) -\u003e (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)","  python_module: nn","  dispatch:","    CPU: slow_conv2d_backward_cpu","    CUDA: slow_conv2d_backward_cuda","  autogen: _slow_conv2d_backward.output_mask_out","","- func: _conv_depthwise2d.out(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CUDA: conv_depthwise2d_cuda_out","","- func: _conv_depthwise2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias, SymInt[2] stride, SymInt[2] padding, SymInt[2] dilation) -\u003e Tensor","  python_module: nn","  dispatch:","    CUDA: conv_depthwise2d_cuda","","- func: conv_depthwise3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, SymInt[3] dilation) -\u003e Tensor","  python_module: nn","  dispatch:","    CUDA: conv_depthwise3d_cuda","  autogen: conv_depthwise3d.out","","- func: slow_conv3d.out(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","","- func: slow_conv3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0) -\u003e Tensor","  python_module: nn","","- func: slow_conv3d_forward.output(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding, *, Tensor(a!) output) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: slow_conv3d_forward_out_cpu","","- func: slow_conv3d_forward(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias, SymInt[3] stride, SymInt[3] padding) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: slow_conv3d_forward_cpu","","- func: slow_conv_dilated2d(Tensor self, Tensor weight, SymInt[2] kernel_size, Tensor? bias=None, SymInt[2] stride=1, SymInt[2] padding=0, SymInt[2] dilation=1) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: slow_conv_dilated2d_cpu","    CUDA: slow_conv_dilated2d_cuda","  autogen: slow_conv_dilated2d.out","","- func: slow_conv_dilated3d(Tensor self, Tensor weight, SymInt[3] kernel_size, Tensor? bias=None, SymInt[3] stride=1, SymInt[3] padding=0, SymInt[3] dilation=1) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: slow_conv_dilated3d_cpu","    CUDA: slow_conv_dilated3d_cuda","  autogen: slow_conv_dilated3d.out","","- func: col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: col2im_out_cpu","    CUDA: col2im_out_cuda","    MPS: col2im_out_mps","","- func: col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: col2im_cpu","    CUDA: col2im_cuda","    MPS: col2im_mps","  tags: core","","- func: column_stack(Tensor[] tensors) -\u003e Tensor","","- func: column_stack.out(Tensor[] tensors, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: nn","  dispatch:","    CPU: im2col_out_cpu","    CUDA: im2col_out_cuda","    MPS: im2col_out_mps","","- func: im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: im2col_cpu","    CUDA: im2col_cuda","    MPS: im2col_mps","","- func: isfinite(Tensor self) -\u003e Tensor","  variants: function, method","  device_check: NoCheck","  device_guard: False","  tags: pointwise","","- func: isinf(Tensor self) -\u003e Tensor","  variants: function, method","  device_check: NoCheck","  device_guard: False","  dispatch:","    CompositeExplicitAutograd: isinf","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_isinf","    SparseCPU, SparseCUDA, SparseMPS: isinf_sparse","    SparseMeta: isinf_sparse_meta","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: isinf_sparse_csr","  autogen: isinf.out","  tags: [core, pointwise]","","- func: record_stream(Tensor(a!) self, Stream s) -\u003e ()","  variants: method","  dispatch:","    CUDA: record_stream_cuda","","- func: isposinf(Tensor self) -\u003e Tensor","  variants: function, method","  structured_delegate: isposinf.out","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_isposinf","    SparseCPU, SparseCUDA, SparseMPS: isposinf_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: isposinf_sparse_csr","  tags: pointwise","","- func: isposinf.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: isposinf_out","    SparseCPU, SparseCUDA, SparseMPS: isposinf_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: isposinf_sparse_csr_out","  tags: pointwise","","- func: isneginf(Tensor self) -\u003e Tensor","  variants: function, method","  structured_delegate: isneginf.out","  dispatch:","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: NestedTensor_isneginf","    SparseCPU, SparseCUDA, SparseMPS: isneginf_sparse","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: isneginf_sparse_csr","  tags: pointwise","","- func: isneginf.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: isneginf_out","    SparseCPU, SparseCUDA, SparseMPS: isneginf_sparse_out","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: isneginf_sparse_csr_out","  tags: pointwise","","# NOTE [_add_batch_dim and _remove_batch_dim]","# _add_batch_dim and _remove_batch_dim are meant to be used in the implementation","# of the vmap frontend API (see torch/_vmap_internals.py). They are not","# user-facing, hence the leading underscore. Please don't use them them anywhere else.","- func: _add_batch_dim(Tensor self, int batch_dim, int level) -\u003e Tensor","  variants: function","","# See NOTE [_add_batch_dim and _remove_batch_dim]","- func: _remove_batch_dim(Tensor self, int level, SymInt batch_size, int out_dim) -\u003e Tensor","  variants: function","","## Functions related to the `torch.special` namespace","# Note [special namespace binding]","# Functions in the special python module should have their names start with","#   \"special_\" underscore and be bound to the desired Python name in","#   torch/special/__init__.py, and the desired C++ name in torch/csrc/api/include/torch/special.h.","#   The \"special_\" names should be hidden from the user and not documented.","","- func: special_entr(Tensor self) -\u003e Tensor","  structured_delegate: special_entr.out","  python_module: special","  variants: function","  tags: pointwise","","- func: special_entr.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: special","  variants: function","  dispatch:","    CPU, CUDA, MPS: special_entr_out","  tags: pointwise","","- func: special_ndtri(Tensor self) -\u003e Tensor","  structured_delegate: special_ndtri.out","  python_module: special","  variants: function","  tags: pointwise","","- func: special_ndtri.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: special","  variants: function","  dispatch:","    CPU, CUDA: special_ndtri_out","  tags: pointwise","","- func: special_log_ndtr(Tensor self) -\u003e Tensor","  structured_delegate: special_log_ndtr.out","  python_module: special","  variants: function","  tags: pointwise","","- func: special_log_ndtr.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: special","  variants: function","  dispatch:","    CPU, CUDA: special_log_ndtr_out","  tags: pointwise","","- func: special_expm1(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_expm1.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_exp2(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_exp2.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_psi(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_psi.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_digamma(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_digamma.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_gammaln(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_gammaln.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_erf(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_erf.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_erfc(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_erfc.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","","- func: special_erfcx(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","  structured_delegate: special_erfcx.out","  tags: pointwise","","- func: special_erfcx.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA: special_erfcx_out","  tags: pointwise","","- func: special_erfinv(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_erfinv.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","","- func: special_ndtr(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_ndtr.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_xlog1py(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","  structured_delegate: special_xlog1py.out","  tags: pointwise","","- func: special_xlog1py.self_scalar(Scalar self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","  dispatch:","    CompositeExplicitAutograd: special_xlog1py","  tags: pointwise","","- func: special_xlog1py.other_scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","  dispatch:","    CompositeExplicitAutograd: special_xlog1py","  tags: pointwise","","- func: special_xlog1py.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: special","  variants: function","  dispatch:","    CPU, CUDA, MPS: special_xlog1py_out","  tags: pointwise","","- func: special_xlog1py.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","  dispatch:","    CompositeExplicitAutograd: special_xlog1py_out","  tags: pointwise","","- func: special_xlog1py.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","  dispatch:","    CompositeExplicitAutograd: special_xlog1py_out","  tags: pointwise","","- func: special_xlogy(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","","- func: special_xlogy.self_scalar(Scalar self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","","- func: special_xlogy.other_scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","","- func: special_xlogy.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","","- func: special_xlogy.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","","- func: special_xlogy.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","","- func: special_zeta(Tensor self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","  structured_delegate: special_zeta.out","  tags: pointwise","","- func: special_zeta.self_scalar(Scalar self, Tensor other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","  dispatch:","    CompositeExplicitAutograd: special_zeta","  tags: pointwise","","- func: special_zeta.other_scalar(Tensor self, Scalar other) -\u003e Tensor","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","  dispatch:","    CompositeExplicitAutograd: special_zeta","  tags: pointwise","","- func: special_zeta.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  structured: True","  structured_inherits: TensorIteratorBase","  python_module: special","  variants: function","  dispatch:","    CPU, CUDA, MPS: special_zeta_out","  tags: pointwise","","- func: special_zeta.self_scalar_out(Scalar self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","  dispatch:","    CompositeExplicitAutograd: special_zeta_out","  tags: pointwise","","- func: special_zeta.other_scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck   # TensorIterator","  python_module: special","  variants: function","  dispatch:","    CompositeExplicitAutograd: special_zeta_out","  tags: pointwise","","- func: special_i0(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_i0.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_i0e(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","  structured_delegate: special_i0e.out","  tags: pointwise","","- func: special_i0e.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: special_i0e_out","  tags: pointwise","","- func: special_i1(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","  structured_delegate: special_i1.out","  tags: pointwise","","- func: special_i1.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: special_i1_out","  tags: pointwise","","- func: special_i1e(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","  structured_delegate: special_i1e.out","  tags: pointwise","","- func: special_i1e.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  structured: True","  structured_inherits: TensorIteratorBase","  dispatch:","    CPU, CUDA, MPS: special_i1e_out","  tags: pointwise","","- func: special_logit(Tensor self, float? eps=None) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_logit.out(Tensor self, float? eps=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","","- func: special_polygamma(int n, Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_polygamma.out(int n, Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","","- func: special_logsumexp(Tensor self, int[1] dim, bool keepdim=False) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","","- func: special_expit(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_expit.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_sinc(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_sinc.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_round(Tensor self, *, int decimals=0) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_round.out(Tensor self, *, int decimals=0, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_log1p(Tensor self) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_log1p.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_log_softmax(Tensor self, int dim, *, ScalarType? dtype=None) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_gammainc.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_gammainc(Tensor self, Tensor other) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_gammaincc.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_gammaincc(Tensor self, Tensor other) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_multigammaln(Tensor self, int p) -\u003e Tensor","  python_module: special","  variants: function","","- func: special_multigammaln.out(Tensor self, int p, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: special","  variants: function","","- func: special_softmax(Tensor self, int dim, ScalarType? dtype=None) -\u003e Tensor","  python_module: special","  variants: function","","## Functions related to the fast Fourier transform and the torch.fft namespace","# Note [FFT namespace binding]","# Functions in the fft python module should have their names start with","#   \"fft_\" underscore and be bound to the desired Python name in","#   torch/fft/__init__.py, and the desired C++ name in torch/csrc/api/include/torch/fft.h.","#   The \"fft_\" names should be hidden from the user and not documented.","#","# See fft_fft as an example.","","# torch.fft.fft","# NOTE: NOT an alias for torch.fft, which has different semantics","- func: fft_fft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_fft_symint","","- func: fft_fft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_fft_symint_out","","- func: fft_ifft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ifft_symint","","- func: fft_ifft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ifft_symint_out","","- func: fft_rfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_rfft_symint","","- func: fft_rfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_rfft_symint_out","","- func: fft_irfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_irfft_symint","","- func: fft_irfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_irfft_symint_out","","- func: fft_hfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_hfft_symint","","- func: fft_hfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_hfft_symint_out","","- func: fft_ihfft(Tensor self, SymInt? n=None, int dim=-1, str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ihfft_symint","","- func: fft_ihfft.out(Tensor self, SymInt? n=None, int dim=-1, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ihfft_symint_out","","- func: fft_fft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_fft2_symint","","- func: fft_fft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_fft2_symint_out","","- func: fft_ifft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ifft2_symint","","- func: fft_ifft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ifft2_symint_out","","- func: fft_rfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_rfft2_symint","","- func: fft_rfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_rfft2_symint_out","","- func: fft_irfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_irfft2_symint","","- func: fft_irfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_irfft2_symint_out","","- func: fft_hfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -\u003e Tensor","  use_const_ref_for_mutable_tensors: True","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_hfft2_symint","","- func: fft_hfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_hfft2_symint_out","","- func: fft_ihfft2(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None) -\u003e Tensor","  use_const_ref_for_mutable_tensors: True","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ihfft2_symint","","- func: fft_ihfft2.out(Tensor self, SymInt[1]? s=None, int[1] dim=[-2,-1], str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ihfft2_symint_out","","- func: fft_fftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_fftn_symint","","- func: fft_fftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_fftn_symint_out","","- func: fft_ifftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ifftn_symint","","- func: fft_ifftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ifftn_symint_out","","- func: fft_rfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_rfftn_symint","","- func: fft_rfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_rfftn_symint_out","","- func: fft_irfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_irfftn_symint","","- func: fft_irfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_irfftn_symint_out","","- func: fft_hfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -\u003e Tensor","  use_const_ref_for_mutable_tensors: True","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_hfftn_symint","","- func: fft_hfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_hfftn_symint_out","","- func: fft_ihfftn(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None) -\u003e Tensor","  use_const_ref_for_mutable_tensors: True","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ihfftn_symint","","- func: fft_ihfftn.out(Tensor self, SymInt[1]? s=None, int[1]? dim=None, str? norm=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeImplicitAutograd: fft_ihfftn_symint_out","","- func: fft_fftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeExplicitAutograd: fft_fftfreq","","- func: fft_fftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeExplicitAutograd: fft_fftfreq_out","","- func: fft_rfftfreq(int n, float d=1.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  python_module: fft","  variants: function","  dispatch:","    CompositeExplicitAutograd: fft_rfftfreq","","- func: fft_rfftfreq.out(int n, float d=1.0, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: fft","  variants: function","  dispatch:","    CompositeExplicitAutograd: fft_rfftfreq_out","","- func: fft_fftshift(Tensor self, int[1]? dim=None) -\u003e Tensor","  python_module: fft","  variants: function","","- func: fft_ifftshift(Tensor self, int[1]? dim=None) -\u003e Tensor","  python_module: fft","  variants: function","","## Functions for linear algebra and the torch.linalg namespace","# Note [linalg namespace binding]","# Functions in the linalg python module should have their names start with","#   \"linalg_\" and be bound to the desired Python name in","#   torch/linalg/__init__.py, and the desired C++ name in torch/csrc/api/include/torch/linalg.h.","#   The \"linalg_\" names should be hidden from the user and not documented.","#","# See linalg_det as an example.","","# \"_ex\" stands for experimental","- func: linalg_cholesky_ex(Tensor self, *, bool upper=False, bool check_errors=False) -\u003e (Tensor L, Tensor info)","  python_module: linalg","  structured_delegate: linalg_cholesky_ex.L","","- func: linalg_cholesky_ex.L(Tensor self, *, bool upper=False, bool check_errors=False, Tensor(a!) L, Tensor(b!) info) -\u003e (Tensor(a!) L, Tensor(b!) info)","  python_module: linalg","  structured: True","  dispatch:","    CPU, CUDA, MPS: linalg_cholesky_ex_out","","- func: linalg_cholesky(Tensor self, *, bool upper=False) -\u003e Tensor","  python_module: linalg","","- func: linalg_cholesky.out(Tensor self, *, bool upper=False, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","- func: linalg_cross(Tensor self, Tensor other, *, int dim=-1) -\u003e Tensor","  python_module: linalg","  variants: function","  structured_delegate: linalg_cross.out","  dispatch:","    ZeroTensor: linalg_cross_zerotensor","","- func: linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  structured: True","  dispatch:","    CPU, CUDA, MPS: linalg_cross_out","","# linalg.lu_factor","- func: linalg_lu_factor(Tensor A, *, bool pivot=True) -\u003e (Tensor LU, Tensor pivots)","  python_module: linalg","  variants: function","  dispatch:","    CompositeImplicitAutograd: linalg_lu_factor","    MPS: linalg_lu_factor_mps","","- func: linalg_lu_factor.out(Tensor A, *, bool pivot=True, Tensor(a!) LU, Tensor(b!) pivots) -\u003e (Tensor(a!) LU, Tensor(b!) pivots)","  python_module: linalg","  variants: function","  dispatch:","    CompositeImplicitAutograd: linalg_lu_factor_out","    MPS: linalg_lu_factor_out_mps","","- func: linalg_lu_factor_ex(Tensor A, *, bool pivot=True, bool check_errors=False) -\u003e (Tensor LU, Tensor pivots, Tensor info)","  python_module: linalg","  structured_delegate: linalg_lu_factor_ex.out","  variants: function","","- func: linalg_lu_factor_ex.out(Tensor A, *, bool pivot=True, bool check_errors=False, Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info) -\u003e (Tensor(a!) LU, Tensor(b!) pivots, Tensor(c!) info)","  python_module: linalg","  variants: function","  structured: True","  dispatch:","    CPU, CUDA: linalg_lu_factor_ex_out","    MPS: linalg_lu_factor_ex_out_mps","","# linalg.lu","- func: linalg_lu(Tensor A, *, bool pivot=True) -\u003e (Tensor P, Tensor L, Tensor U)","  python_module: linalg","  structured_delegate: linalg_lu.out","  variants: function","","- func: linalg_lu.out(Tensor A, *, bool pivot=True, Tensor(a!) P, Tensor(b!) L, Tensor(c!) U) -\u003e (Tensor(a!) P, Tensor(b!) L, Tensor(c!) U)","  python_module: linalg","  variants: function","  structured: True","  dispatch:","    CPU, CUDA: linalg_lu_out","","# linalg.lu_solve","- func: linalg_lu_solve(Tensor LU, Tensor pivots, Tensor B, *, bool left=True, bool adjoint=False) -\u003e Tensor","  python_module: linalg","  structured_delegate: linalg_lu_solve.out","  variants: function","","- func: linalg_lu_solve.out(Tensor LU, Tensor pivots, Tensor B, *, bool left=True, bool adjoint=False, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","  structured: True","  dispatch:","    CPU, CUDA: linalg_lu_solve_out","","# linalg.det","- func: _linalg_det(Tensor A) -\u003e (Tensor result, Tensor LU, Tensor pivots)","  structured_delegate: _linalg_det.result","","- func: _linalg_det.result(Tensor A, *, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots) -\u003e (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots)","  structured: True","  dispatch:","    CPU, CUDA, MPS: _linalg_det_out","","- func: linalg_det(Tensor A) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_det.out(Tensor A, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","# torch.det, alias for torch.linalg.det","- func: det(Tensor self) -\u003e Tensor","  variants: function, method","","- func: linalg_ldl_factor_ex(Tensor self, *, bool hermitian=False, bool check_errors=False) -\u003e (Tensor LD, Tensor pivots, Tensor info)","  structured_delegate: linalg_ldl_factor_ex.out","  python_module: linalg","  variants: function","","- func: linalg_ldl_factor_ex.out(Tensor self, *, bool hermitian=False, bool check_errors=False, Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info) -\u003e (Tensor(a!) LD, Tensor(b!) pivots, Tensor(c!) info)","  structured: True","  python_module: linalg","  variants: function","  dispatch:","    CPU, CUDA: linalg_ldl_factor_ex_out","","- func: linalg_ldl_factor(Tensor self, *, bool hermitian=False) -\u003e (Tensor LD, Tensor pivots)","  python_module: linalg","  variants: function","","- func: linalg_ldl_factor.out(Tensor self, *, bool hermitian=False, Tensor(a!) LD, Tensor(b!) pivots) -\u003e (Tensor(a!) LD, Tensor(b!) pivots)","  python_module: linalg","  variants: function","","- func: linalg_ldl_solve(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False) -\u003e Tensor","  structured_delegate: linalg_ldl_solve.out","  python_module: linalg","  variants: function","","- func: linalg_ldl_solve.out(Tensor LD, Tensor pivots, Tensor B, *, bool hermitian=False, Tensor(a!) out) -\u003e Tensor(a!)","  structured: True","  python_module: linalg","  variants: function","  dispatch:","    CPU, CUDA: linalg_ldl_solve_out","","- func: linalg_lstsq(Tensor self, Tensor b, float? rcond=None, *, str? driver=None) -\u003e (Tensor solution, Tensor residuals, Tensor rank, Tensor singular_values)","  python_module: linalg","  variants: function","  dispatch:","    CompositeExplicitAutograd: linalg_lstsq","  tags: dynamic_output_shape","","- func: linalg_lstsq.out(Tensor self, Tensor b, float? rcond=None, *, str? driver=None, Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values) -\u003e (Tensor(a!) solution, Tensor(b!) residuals, Tensor(c!) rank, Tensor(d!) singular_values)","  python_module: linalg","  variants: function","  dispatch:","    CPU, CUDA: linalg_lstsq_out","  tags: dynamic_output_shape","","# torch.linalg.matmul, alias for torch.matmul","- func: linalg_matmul(Tensor self, Tensor other) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","- func: linalg_vecdot(Tensor x, Tensor y, *, int dim=-1) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_vecdot.out(Tensor x, Tensor y, *, int dim=-1, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","- func: linalg_matrix_exp(Tensor self) -\u003e Tensor","  python_module: linalg","  variants: function","  dispatch:","    CPU, CUDA: linalg_matrix_exp","  autogen: linalg_matrix_exp.out","","- func: _linalg_slogdet(Tensor A) -\u003e (Tensor sign, Tensor logabsdet, Tensor LU, Tensor pivots)","  structured_delegate: _linalg_slogdet.sign","","- func: _linalg_slogdet.sign(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet, Tensor(c!) LU, Tensor(d!) pivots) -\u003e (Tensor(a!) sign, Tensor(b!) logabsdet, Tensor(c!) LU, Tensor(d!) pivots)","  structured: True","  dispatch:","    CPU, CUDA, MPS: _linalg_slogdet_out","","- func: linalg_slogdet(Tensor A) -\u003e (Tensor sign, Tensor logabsdet)","  python_module: linalg","","- func: linalg_slogdet.out(Tensor A, *, Tensor(a!) sign, Tensor(b!) logabsdet) -\u003e (Tensor(a!) sign, Tensor(b!) logabsdet)","  python_module: linalg","","- func: slogdet(Tensor self) -\u003e (Tensor sign, Tensor logabsdet)","  variants: function, method","","- func: slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -\u003e (Tensor(a!) sign, Tensor(b!) logabsdet)","  variants: function","","- func: logdet(Tensor self) -\u003e Tensor","  variants: function, method","","- func: linalg_eig(Tensor self) -\u003e (Tensor eigenvalues, Tensor eigenvectors)","  python_module: linalg","  variants: function","  dispatch:","    CPU, CUDA: linalg_eig","","- func: linalg_eig.out(Tensor self, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -\u003e (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)","  python_module: linalg","  dispatch:","    CPU, CUDA: linalg_eig_out","","- func: _linalg_eigvals(Tensor self) -\u003e Tensor","  python_module: linalg","  dispatch:","    CPU, CUDA: _linalg_eigvals","","- func: linalg_eigvals(Tensor self) -\u003e Tensor","  python_module: linalg","","- func: linalg_eigvals.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  dispatch:","    CPU, CUDA: linalg_eigvals_out","","# This function is exposes the `compute_v` flag, which is then used to implement `linalg.eigh` and","# `linalg.eigvalsh` as composite functions that call this one","- func: _linalg_eigh(Tensor A, str UPLO=\"L\", bool compute_v=True) -\u003e (Tensor eigenvalues, Tensor eigenvectors)","  structured_delegate: _linalg_eigh.eigenvalues","","- func: _linalg_eigh.eigenvalues(Tensor A, str UPLO=\"L\", bool compute_v=True, *, Tensor(a!) eigenvalues, Tensor(b!) eigenvectors) -\u003e (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)","  structured: True","  dispatch:","    CPU, CUDA: _linalg_eigh_out","","- func: linalg_eigh(Tensor self, str UPLO=\"L\") -\u003e (Tensor eigenvalues, Tensor eigenvectors)","  python_module: linalg","","- func: linalg_eigh.eigvals(Tensor self, str UPLO=\"L\", *, Tensor(a!) eigvals, Tensor(b!) eigvecs) -\u003e (Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)","  python_module: linalg","","- func: linalg_eigvalsh(Tensor self, str UPLO=\"L\") -\u003e Tensor","  python_module: linalg","","- func: linalg_eigvalsh.out(Tensor self, str UPLO=\"L\", *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","- func: linalg_householder_product(Tensor input, Tensor tau) -\u003e Tensor","  python_module: linalg","  variants: function","  dispatch:","    CPU, CUDA: linalg_householder_product","","- func: linalg_householder_product.out(Tensor input, Tensor tau, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  dispatch:","    CPU, CUDA: linalg_householder_product_out","","- func: linalg_inv_ex(Tensor A, *, bool check_errors=False) -\u003e (Tensor inverse, Tensor info)","  python_module: linalg","  structured_delegate: linalg_inv_ex.inverse","","- func: linalg_inv_ex.inverse(Tensor A, *, bool check_errors=False, Tensor(a!) inverse, Tensor(b!) info) -\u003e (Tensor(a!) inverse, Tensor(b!) info)","  python_module: linalg","  structured: True","  dispatch:","    CPU, CUDA: linalg_inv_ex_out","    MPS: linalg_inv_ex_out_mps","","- func: linalg_inv(Tensor A) -\u003e Tensor","  python_module: linalg","","- func: linalg_inv.out(Tensor A, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","- func: inverse(Tensor self) -\u003e Tensor","  variants: function, method","","- func: inverse.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: inner(Tensor self, Tensor other) -\u003e Tensor","  variants: function, method","","- func: inner.out(Tensor self, Tensor other, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: outer(Tensor self, Tensor vec2) -\u003e Tensor","  variants: function, method","","- func: outer.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -\u003e Tensor(a!)","","# torch.ger, alias for torch.outer","- func: ger(Tensor self, Tensor vec2) -\u003e Tensor","  variants: function, method","","- func: ger.out(Tensor self, Tensor vec2, *, Tensor(a!) out) -\u003e Tensor(a!)","","- func: linalg_norm(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_norm.ord_str(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_norm.out(Tensor self, Scalar? ord=None, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_norm.ord_str_out(Tensor self, str ord, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  python_module: linalg","  variants: function","  structured_delegate: linalg_vector_norm.out","","- func: linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  structured: True","  dispatch:","    CPU, CUDA: linalg_vector_norm_out","    MPS: linalg_vector_norm_out_mps","","- func: linalg_matrix_norm(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  python_module: linalg","","- func: linalg_matrix_norm.out(Tensor self, Scalar ord, int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","- func: linalg_matrix_norm.str_ord(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor","  python_module: linalg","","- func: linalg_matrix_norm.str_ord_out(Tensor self, str ord='fro', int[] dim=[-2,-1], bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","# This function is exposes the `compute_uv` flag, which is then used to implement `linalg.svd` and","# `linalg.svdvals` as composite functions that call this one","- func: _linalg_svd(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None) -\u003e (Tensor U, Tensor S, Tensor Vh)","  variants: function","  structured_delegate: _linalg_svd.U","","- func: _linalg_svd.U(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -\u003e (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)","  structured: True","  dispatch:","    CPU, CUDA: _linalg_svd_out","","- func: linalg_svd(Tensor A, bool full_matrices=True, *, str? driver=None) -\u003e (Tensor U, Tensor S, Tensor Vh)","  python_module: linalg","  variants: function","","- func: linalg_svd.U(Tensor A, bool full_matrices=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -\u003e (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)","  python_module: linalg","  variants: function","","- func: linalg_svdvals(Tensor A, *, str? driver=None) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_svdvals.out(Tensor A, *, str? driver=None, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_cond(Tensor self, Scalar? p=None) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_cond.out(Tensor self, Scalar? p=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_cond.p_str(Tensor self, str p) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_cond.p_str_out(Tensor self, str p, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_pinv.atol_rtol_tensor(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -\u003e Tensor","  python_module: linalg","  variants: function","  dispatch:","    # calls svd, which calls mH() (view op)","    # also calls narrow()","    CompositeExplicitAutogradNonFunctional: linalg_pinv","","- func: linalg_pinv.atol_rtol_tensor_out(Tensor self, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","  dispatch:","    CompositeExplicitAutograd: linalg_pinv_out","","- func: linalg_pinv.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -\u003e Tensor","  cpp_no_default_args: ['atol', 'rtol']","  python_module: linalg","  variants: function","","- func: linalg_pinv.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -\u003e Tensor(a!)","  cpp_no_default_args: ['atol', 'rtol']","  python_module: linalg","  variants: function","","- func: linalg_pinv(Tensor self, float rcond, bool hermitian=False) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_pinv.rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_pinv.out(Tensor self, float rcond, bool hermitian=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_pinv.out_rcond_tensor(Tensor self, Tensor rcond, bool hermitian=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: _linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -\u003e (Tensor result, Tensor LU, Tensor pivots, Tensor info)","  structured_delegate: _linalg_solve_ex.result","","- func: _linalg_solve_ex.result(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots, Tensor(d!) info) -\u003e (Tensor(a!) result, Tensor(b!) LU, Tensor(c!) pivots, Tensor(d!) info)","  structured: True","  dispatch:","    CPU, CUDA: _linalg_solve_ex_out","    MPS: _linalg_solve_ex_out_mps","","- func: linalg_solve_ex(Tensor A, Tensor B, *, bool left=True, bool check_errors=False) -\u003e (Tensor result, Tensor info)","  python_module: linalg","","- func: linalg_solve_ex.out(Tensor A, Tensor B, *, bool left=True, bool check_errors=False, Tensor(a!) result, Tensor(b!) info) -\u003e (Tensor(a!) result, Tensor(b!) info)","  python_module: linalg","","- func: linalg_solve(Tensor A, Tensor B, *, bool left=True) -\u003e Tensor","  python_module: linalg","","- func: _spsolve(Tensor A, Tensor B, *, bool left=True) -\u003e Tensor","  python_module: sparse","  dispatch:","    SparseCsrCUDA: _sparse_csr_linear_solve","","- func: linalg_solve.out(Tensor A, Tensor B, *, bool left=True, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","- func: linalg_tensorinv(Tensor self, int ind=2) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_tensorinv.out(Tensor self, int ind=2, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_tensorsolve(Tensor self, Tensor other, int[]? dims=None) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_tensorsolve.out(Tensor self, Tensor other, int[]? dims=None, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_qr(Tensor A, str mode='reduced') -\u003e (Tensor Q, Tensor R)","  python_module: linalg","  variants: function","  structured_delegate: linalg_qr.out","","- func: linalg_qr.out(Tensor A, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -\u003e (Tensor(a!) Q, Tensor(b!) R)","  python_module: linalg","  structured: True","  dispatch:","    CPU, CUDA: linalg_qr_out","","- func: linalg_matrix_power(Tensor self, int n) -\u003e Tensor","  python_module: linalg","","- func: linalg_matrix_power.out(Tensor self, int n, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","- func: linalg_matrix_rank.atol_rtol_tensor(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_matrix_rank.atol_rtol_tensor_out(Tensor input, *, Tensor? atol=None, Tensor? rtol=None, bool hermitian=False, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_matrix_rank.atol_rtol_float(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False) -\u003e Tensor","  cpp_no_default_args: ['atol', 'rtol']","  python_module: linalg","  variants: function","","- func: linalg_matrix_rank.atol_rtol_float_out(Tensor self, *, float? atol=None, float? rtol=None, bool hermitian=False, Tensor(a!) out) -\u003e Tensor(a!)","  cpp_no_default_args: ['atol', 'rtol']","  python_module: linalg","  variants: function","","- func: linalg_matrix_rank(Tensor self, float tol, bool hermitian=False) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_matrix_rank.out(Tensor self, float tol, bool hermitian=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_matrix_rank.tol_tensor(Tensor input, Tensor tol, bool hermitian=False) -\u003e Tensor","  python_module: linalg","  variants: function","","- func: linalg_matrix_rank.out_tol_tensor(Tensor input, Tensor tol, bool hermitian=False, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","  variants: function","","- func: linalg_multi_dot(Tensor[] tensors) -\u003e Tensor","  python_module: linalg","","- func: linalg_multi_dot.out(Tensor[] tensors, *, Tensor(a!) out) -\u003e Tensor(a!)","  python_module: linalg","","## Functions related to the `torch.nested` namespace","# Note [nested namespace binding]","# Functions in the nested python module should have their names start with","#   \"nested_\" underscore and be bound to the desired Python name in","#   torch/nested/__init__.py, and the desired C++ name in torch/csrc/api/include/torch/nested.h.","#   The \"nested_\" names should be hidden from the user and not documented.","","- func: nested_to_padded_tensor(Tensor self, float padding, int[]? output_size=None) -\u003e Tensor","  python_module: nested","  variants: function","","## Functions that are only for testing","# It is undocumented and should not be used outside of tests.","- func: _test_serialization_subcmul(Tensor self, Tensor other, Scalar alpha=1) -\u003e Tensor","","# Note: for testing COW materialization within `at::parallel_for` loop function","- func: _test_parallel_materialize(Tensor self, int num_parallel, bool skip_first=False) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: _test_parallel_materialize","","# Note: this function is only for testing.","- func: _test_optional_intlist(Tensor values, int[]? addends) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: _test_optional_intlist","  autogen: _test_optional_intlist.out","","# Note: this function is only for testing.","- func: _test_optional_filled_intlist(Tensor values, int[2]? addends) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: _test_optional_intlist","  autogen: _test_optional_filled_intlist.out","","# Note: this function is only for testing.","- func: _test_optional_floatlist(Tensor values, float[]? addends) -\u003e Tensor","  python_module: nn","  dispatch:","    CPU: _test_optional_floatlist","  autogen: _test_optional_floatlist.out","","# Note: this function is only for testing.","- func: _test_string_default(Tensor dummy, str a=\"\\\"'\\\\\", str b='\"\\'\\\\') -\u003e Tensor","  python_module: nn","","# Note: this function is only for testing.","- func: _test_ambiguous_defaults.a(Tensor dummy, int a=1, int b=1) -\u003e Tensor","  python_module: nn","","# Note: this function is only for testing.","- func: _test_ambiguous_defaults.b(Tensor dummy, int a=2, str b=\"2\") -\u003e Tensor","  cpp_no_default_args: ['a', 'b']","  python_module: nn","","# Note: this function is only for testing.","- func: _test_warn_in_autograd(Tensor self) -\u003e Tensor","  python_module: nn","  dispatch:","    CompositeExplicitAutograd: _test_warn_in_autograd","  autogen: _test_warn_in_autograd.out","","# Note: this function is only for testing.","- func: _test_autograd_multiple_dispatch.fullcoverage(Tensor self) -\u003e Tensor","  dispatch:","    # the NestedTensor keys are necessary because NestedTensor has been removed","    # from the CompositeExplicitAutograd keyset see Note [NestedTensor Not Included in Backend Keys]","    CompositeExplicitAutograd, NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: _test_autograd_multiple_dispatch_fullcoverage","  autogen: _test_autograd_multiple_dispatch.fullcoverage_out","","# Note: this function is only for testing.","- func: _test_autograd_multiple_dispatch.ntonly(Tensor self, bool b) -\u003e Tensor","  dispatch:","    CompositeImplicitAutograd, NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: _test_autograd_multiple_dispatch_ntonly","","# Note: this function is only for testing.","- func: _test_autograd_multiple_dispatch_view(Tensor(a) self) -\u003e Tensor(a)","  dispatch:","    CompositeExplicitAutograd: _test_autograd_multiple_dispatch_view","","# Note: this function is only for testing.","- func: _test_autograd_multiple_dispatch_view_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: _test_autograd_multiple_dispatch_view_copy","  tags: view_copy","  autogen: _test_autograd_multiple_dispatch_view_copy.out","","- func: segment_reduce(Tensor data, str reduce, *, Tensor? lengths=None, Tensor? indices=None, Tensor? offsets=None, int axis=0, bool unsafe=False, Scalar? initial=None) -\u003e Tensor","  variants: function","  dispatch:","    CPU, CUDA: segment_reduce_kernel","  autogen: segment_reduce.out","","- func: _segment_reduce_backward(Tensor grad, Tensor output, Tensor data, str reduce, *, Tensor? lengths=None, Tensor? offsets=None, int axis=0, Scalar? initial=None) -\u003e Tensor","  variants: function","  dispatch:","    CPU, CUDA: _segment_reduce_backward_kernel","  autogen: _segment_reduce_backward.out","","- func: pad_sequence(Tensor[] sequences, bool batch_first=False, float padding_value=0.0, str padding_side=\"right\") -\u003e Tensor","  python_module: nn","  variants: function","","- func: flatten_dense_tensors(Tensor[] tensors) -\u003e Tensor","  variants: function","  python_module: nn","","- func: unflatten_dense_tensors(Tensor flat, Tensor[] tensors) -\u003e Tensor[]","  variants: function","  python_module: nn","","- func: _nested_tensor_from_tensor_list(Tensor[] list, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutograd: _nested_tensor_from_tensor_list","  autogen: _nested_tensor_from_tensor_list.out","","- func: _fw_primal_copy(Tensor self, int level) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: _fw_primal_copy","  tags: view_copy","  autogen: _fw_primal_copy.out","","- func: _make_dual_copy(Tensor primal, Tensor tangent, int level) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: _make_dual_copy","  tags: view_copy","  autogen: _make_dual_copy.out","","- func: view_as_real_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: view_as_real_copy","  tags: view_copy","  autogen: view_as_real_copy.out","","- func: view_as_complex_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: view_as_complex_copy","  tags: view_copy","  autogen: view_as_complex_copy.out","","- func: _conj_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: _conj_copy","  tags: view_copy","  autogen: _conj_copy.out","","- func: _neg_view_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: _neg_view_copy","  tags: view_copy","  autogen: _neg_view_copy.out","","- func: as_strided_copy(Tensor self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: as_strided_copy_symint","  tags: view_copy","  autogen: as_strided_copy.out","","- func: _sparse_broadcast_to_copy(Tensor self, int[] size) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: _sparse_broadcast_to_copy","  tags: view_copy","  autogen: _sparse_broadcast_to_copy.out","","- func: diagonal_copy(Tensor self, int offset=0, int dim1=0, int dim2=1) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: diagonal_copy","  tags: view_copy","  autogen: diagonal_copy.out","","- func: expand_copy(Tensor self, SymInt[] size, *, bool implicit=False) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: expand_copy_symint","  tags: view_copy","  autogen: expand_copy.out","","- func: permute_copy(Tensor self, int[] dims) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: permute_copy","  tags: view_copy","  autogen: permute_copy.out","","- func: _reshape_alias_copy(Tensor self, SymInt[] size, SymInt[] stride) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: _reshape_alias_copy_symint","  tags: view_copy","  autogen: _reshape_alias_copy.out","","- func: select_copy.int(Tensor self, int dim, SymInt index) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: select_copy_symint","    SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta: select_copy_sparse_csr","  tags: view_copy","  autogen: select_copy.int_out","","- func: detach_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: detach_copy","  tags: view_copy","  autogen: detach_copy.out","","- func: slice_copy.Tensor(Tensor self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: slice_copy_Tensor_symint","  tags: view_copy","  autogen: slice_copy.Tensor_out","","- func: split_copy.Tensor(Tensor self, SymInt split_size, int dim=0) -\u003e Tensor[]","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: split_copy_Tensor_symint","  tags: view_copy","","- func: split_with_sizes_copy(Tensor self, SymInt[] split_sizes, int dim=0) -\u003e Tensor[]","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: split_with_sizes_copy_symint","  tags: view_copy","","- func: squeeze_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: squeeze_copy","  tags: view_copy","  autogen: squeeze_copy.out","","- func: squeeze_copy.dim(Tensor self, int dim) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: squeeze_copy_dim","  tags: view_copy","  autogen: squeeze_copy.dim_out","","- func: squeeze_copy.dims(Tensor self, int[] dim) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: squeeze_copy_dims","  tags: view_copy","  autogen: squeeze_copy.dims_out","","- func: t_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: t_copy","  tags: view_copy","  autogen: t_copy.out","","- func: transpose_copy.int(Tensor self, int dim0, int dim1) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: transpose_copy_int","  tags: view_copy","  autogen: transpose_copy.int_out","","- func: unsqueeze_copy(Tensor self, int dim) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: unsqueeze_copy","  tags: view_copy","  autogen: unsqueeze_copy.out","","- func: _indices_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: _indices_copy","  tags: view_copy","  autogen: _indices_copy.out","","- func: _values_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: _values_copy","  tags: view_copy","  autogen: _values_copy.out","","- func: indices_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: indices_copy","  tags: view_copy","  autogen: indices_copy.out","","- func: values_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: values_copy","  tags: view_copy","  autogen: values_copy.out","","- func: crow_indices_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: crow_indices_copy","  tags: view_copy","  autogen: crow_indices_copy.out","","- func: col_indices_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: col_indices_copy","  tags: view_copy","  autogen: col_indices_copy.out","","- func: ccol_indices_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: ccol_indices_copy","  tags: view_copy","  autogen: ccol_indices_copy.out","","- func: row_indices_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: row_indices_copy","  tags: view_copy","  autogen: row_indices_copy.out","","- func: unbind_copy.int(Tensor self, int dim=0) -\u003e Tensor[]","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: unbind_copy_int","  tags: view_copy","","- func: unbind_copy.int_out(Tensor self, int dim=0, *, Tensor(a!)[] out) -\u003e ()","  variants: function","  dispatch:","    CompositeExplicitAutograd: unbind_copy_int_out","","- func: split_copy.Tensor_out(Tensor self, SymInt split_size, int dim=0, *, Tensor(a!)[] out) -\u003e ()","  variants: function","  dispatch:","    CompositeExplicitAutograd: split_copy_Tensor_out","","","- func: split_with_sizes_copy.out(Tensor self, SymInt[] split_sizes, int dim=0, *, Tensor(a!)[] out) -\u003e ()","  variants: function","  dispatch:","    CompositeExplicitAutograd: split_with_sizes_copy_out","    CUDA: split_with_sizes_copy_out_cuda","","- func: view_copy(Tensor self, SymInt[] size) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: view_copy_symint","  tags: view_copy","  autogen: view_copy.out","","- func: view_copy.dtype(Tensor self, ScalarType dtype) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: view_copy_dtype","  tags: view_copy","  autogen: view_copy.dtype_out","","- func: unfold_copy(Tensor self, int dimension, int size, int step) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: unfold_copy","  tags: view_copy","  autogen: unfold_copy.out","","- func: alias_copy(Tensor self) -\u003e Tensor","  variants: function","  dispatch:","    CompositeExplicitAutogradNonFunctional: alias_copy","  tags: view_copy","  autogen: alias_copy.out","","- func: to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -\u003e Tensor","  variants: method","  dispatch:","    NestedTensorCPU: NestedTensor_to_padded_tensor_generic","    NestedTensorCUDA: NestedTensor_to_padded_tensor_cuda","  autogen: to_padded_tensor.out","","- func: _jagged_to_padded_dense_forward(Tensor values, Tensor[] offsets, SymInt[] max_lengths, float padding_value=0.0) -\u003e Tensor","  variants: function","  dispatch:","    CUDA: _fbgemm_jagged_to_padded_dense_forward","    CPU: _jagged_to_padded_dense_forward_cpu","","- func: _padded_dense_to_jagged_forward(Tensor dense, Tensor[] offsets, SymInt? total_L=None) -\u003e Tensor","  variants: function","  dispatch:","    CUDA: _fbgemm_dense_to_jagged_forward_symint","    CPU: _padded_dense_to_jagged_forward_cpu","","- func: _nested_from_padded_tensor(Tensor padded, Tensor offsets, Tensor dummy, int ragged_idx=1, Tensor? min_seqlen=None, Tensor? max_seqlen=None, SymInt? sum_S=None) -\u003e Tensor","  variants: function","  device_check: NoCheck","  dispatch: {}","","- func: _nested_tensor_softmax_with_shape(Tensor self, Tensor query) -\u003e Tensor","  dispatch:","    NestedTensorCPU: NestedTensor_softmax_dropout","    NestedTensorCUDA: NestedTensor_softmax_dropout_cuda","  tags: nondeterministic_seeded","","- func: _safe_softmax(Tensor self, int dim, ScalarType? dtype=None) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: _safe_softmax","    NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: _safe_softmax","","# Apparently, putting \"forward\" in the name will cause Python bindings to be skipped, so \"fwd\" it is.","- func: _transformer_encoder_layer_fwd(Tensor src, int embed_dim, int num_heads, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, bool use_gelu, bool norm_first, float eps, Tensor norm_weight_1, Tensor norm_bias_1, Tensor norm_weight_2, Tensor norm_bias_2, Tensor ffn_weight_1, Tensor ffn_bias_1, Tensor ffn_weight_2, Tensor ffn_bias_2, Tensor? mask=None, int? mask_type=None) -\u003e Tensor","  variants: function","  dispatch:","    CPU, CUDA, NestedTensorCPU, NestedTensorHPU, NestedTensorCUDA: transformer_encoder_layer_forward","  autogen: _transformer_encoder_layer_fwd.out","","- func: _native_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None) -\u003e (Tensor, Tensor)","  variants: function","  dispatch:","    CPU, NestedTensorCPU: native_multi_head_attention_cpu","    CUDA, NestedTensorCUDA: native_multi_head_attention_cuda","  autogen: _native_multi_head_attention.out","","- func: scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None, bool enable_gqa=False) -\u003e Tensor","  python_module: nn","  variants: function","  autogen: scaled_dot_product_attention.out","  tags: nondeterministic_seeded","","# This aten function is kept so that we can test the choice function from Python","- func: _fused_sdp_choice(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None, bool enable_gqa=False) -\u003e int","  dispatch:","    Meta: _fused_sdp_choice_meta","    CPU, NestedTensorCPU: _fused_sdp_choice_cpp","    CUDA, NestedTensorCUDA: _fused_sdp_choice_cuda","    XPU: _fused_sdp_choice_xpu","  tags: nondeterministic_seeded","","- func: _scaled_dot_product_attention_math(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, Tensor? dropout_mask=None, *, float? scale=None, bool enable_gqa=False) -\u003e (Tensor, Tensor)","  variants: function","  tags: nondeterministic_seeded","","- func: _scaled_dot_product_attention_math_for_mps(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, Tensor? dropout_mask=None, *, float? scale=None) -\u003e (Tensor, Tensor)","  dispatch:","    MPS: _scaled_dot_product_attention_math_mps","  tags: nondeterministic_seeded","","- func: _scaled_dot_product_flash_attention(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -\u003e (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor rng_state, Tensor unused, Tensor debug_attn_mask)","  dispatch:","    CUDA: _scaled_dot_product_flash_attention_cuda","    NestedTensorCUDA: _scaled_dot_product_flash_attention_nestedtensor_cuda","  tags: nondeterministic_seeded","","- func: _scaled_dot_product_flash_attention_for_cpu(Tensor query, Tensor key, Tensor value, float dropout_p=0.0, bool is_causal=False, *, Tensor? attn_mask=None, float? scale=None) -\u003e (Tensor output, Tensor logsumexp)","  dispatch:","    CPU: _scaled_dot_product_flash_attention_cpu","  tags: nondeterministic_seeded","","- func: _scaled_dot_product_fused_attention_overrideable(Tensor query, Tensor key, Tensor value, Tensor? attn_bias=None, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -\u003e (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)","  dispatch:","    CompositeExplicitAutograd: _scaled_dot_product_fused_attention_overrideable","    XPU: _scaled_dot_product_fused_attention_overrideable_xpu","  tags: nondeterministic_seeded","","- func: _scaled_dot_product_flash_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, Tensor philox_seed, Tensor philox_offset, *, float? scale=None) -\u003e (Tensor grad_query, Tensor grad_key, Tensor grad_value)","  device_check: NoCheck","  variants: function","  dispatch:","    CUDA: _scaled_dot_product_flash_attention_backward_cuda","    NestedTensorCUDA: _scaled_dot_product_flash_attention_backward_nested","","- func: _scaled_dot_product_flash_attention_for_cpu_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, float dropout_p, bool is_causal, *, Tensor? attn_mask=None, float? scale=None) -\u003e (Tensor grad_query, Tensor grad_key, Tensor grad_value)","  device_check: NoCheck","  variants: function","  dispatch:","    CPU: _scaled_dot_product_flash_attention_cpu_backward","","- func: _scaled_dot_product_fused_attention_overrideable_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor attn_bias, bool[4] grad_input_mask, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, Tensor philox_seed, Tensor philox_offset, *, float? scale=None) -\u003e (Tensor grad_query, Tensor grad_key, Tensor grad_value, Tensor grad_attn_bias)","  device_check: NoCheck","  variants: function","  dispatch:","    CompositeExplicitAutograd: _scaled_dot_product_fused_attention_overrideable_backward","","- func: _scaled_dot_product_efficient_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, bool compute_log_sumexp, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -\u003e (Tensor output, Tensor log_sumexp, Tensor philox_seed, Tensor philox_offset)","  dispatch:","    CUDA: _scaled_dot_product_efficient_attention_cuda","    NestedTensorCUDA: _scaled_dot_product_efficient_attention_nestedtensor_cuda","  tags: nondeterministic_seeded","","- func: _scaled_dot_product_efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor attn_bias, Tensor out, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, float dropout_p, bool[4] grad_input_mask, bool is_causal=False, *, float? scale=None) -\u003e (Tensor, Tensor, Tensor, Tensor)","  device_check: NoCheck","  dispatch:","    CUDA: _scaled_dot_product_efficient_attention_backward_cuda","  tags: nondeterministic_seeded","","- func: _scaled_dot_product_cudnn_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, bool compute_log_sumexp, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -\u003e (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)","  dispatch:","    CUDA: _scaled_dot_product_cudnn_attention_cuda","    NestedTensorCUDA: _scaled_dot_product_cudnn_attention_nestedtensor_cuda","  tags: nondeterministic_seeded","","- func: _scaled_dot_product_cudnn_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, Tensor attn_bias, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, *, float? scale=None) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CUDA: _scaled_dot_product_cudnn_attention_backward_cuda","    NestedTensorCUDA: _scaled_dot_product_cudnn_attention_nestedtensor_backward_cuda","  tags: nondeterministic_seeded","","- func: _flash_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? cum_seq_q, Tensor? cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, bool return_debug_mask, *, float? scale=None, SymInt? window_size_left=None, SymInt? window_size_right=None, Tensor? seqused_k=None, Tensor? alibi_slopes=None) -\u003e (Tensor output, Tensor softmax_logsumexp, Tensor rng_state, Tensor unused, Tensor debug_attn_mask)","  variants: function","  dispatch:","    CUDA: _flash_attention_forward","  tags: nondeterministic_seeded","","- func: _flash_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, Tensor rng_state, Tensor unused, *, float? scale=None, SymInt? window_size_left=None, SymInt? window_size_right=None) -\u003e (Tensor, Tensor, Tensor)","  device_check: NoCheck","  variants: function","  dispatch:","    CUDA: _flash_attention_backward","","# Returns output, logsumexp if compute_logsumexp","- func: _efficient_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, SymInt? max_seqlen_q, SymInt? max_seqlen_k, float dropout_p, int custom_mask_type, bool compute_log_sumexp=False, *, float? scale=None, Tensor? seqlen_k=None, int? window_size=None) -\u003e (Tensor output, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, SymInt max_seqlen_batch_q, SymInt max_seqlen_batch_k)","  variants: function","  dispatch:","    CUDA: _efficient_attention_forward","  tags: nondeterministic_seeded","","- func: _efficient_attention_backward(Tensor grad_out_, Tensor query, Tensor key, Tensor value, Tensor? bias, Tensor out, Tensor? cu_seqlens_q, Tensor? cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, Tensor logsumexp, float dropout_p, Tensor philox_seed, Tensor philox_offset, int custom_mask_type, bool bias_requires_grad, *, float? scale=None, int? num_splits_key=None, int? window_size=None, bool shared_storage_dqdkdv=False) -\u003e (Tensor, Tensor, Tensor, Tensor)","  device_check: NoCheck","  variants: function","  dispatch:","    CUDA: _efficient_attention_backward","","- func: _cudnn_attention_forward(Tensor query, Tensor key, Tensor value, Tensor? attn_bias, Tensor? cum_seq_q, Tensor? cum_seq_k, SymInt max_q, SymInt max_k, bool compute_log_sumexp, float dropout_p=0.0, bool is_causal=False, bool return_debug_mask=False, *, float? scale=None) -\u003e (Tensor output, Tensor logsumexp, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, Tensor philox_seed, Tensor philox_offset, Tensor debug_attn_mask)","  dispatch:","    CUDA: _cudnn_attention_forward","  tags: nondeterministic_seeded","","- func: _cudnn_attention_backward(Tensor grad_out, Tensor query, Tensor key, Tensor value, Tensor out, Tensor logsumexp, Tensor philox_seed, Tensor philox_offset, Tensor attn_bias, Tensor cum_seq_q, Tensor cum_seq_k, SymInt max_q, SymInt max_k, float dropout_p, bool is_causal, *, float? scale=None) -\u003e (Tensor, Tensor, Tensor)","  dispatch:","    CUDA: _cudnn_attention_backward","  tags: nondeterministic_seeded","","- func: _triton_scaled_dot_attention(Tensor q, Tensor k, Tensor v, float dropout_p=0.0) -\u003e Tensor","  variants: function","  dispatch:","    CUDA: triton_scaled_dot_attention","  tags: nondeterministic_seeded","  autogen: _triton_scaled_dot_attention.out","","- func: _fill_mem_eff_dropout_mask_(Tensor(a!) self, float dropout_p, int seed, int offset) -\u003e Tensor(a!)","  variants: function","  dispatch:","    CUDA: _fill_mem_eff_dropout_mask_","  tags: nondeterministic_seeded","","- func: _triton_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None) -\u003e Tensor","  variants: function","  dispatch:","    CUDA: triton_multi_head_attention","  autogen: _triton_multi_head_attention.out","","- func: special_airy_ai(Tensor x) -\u003e Tensor","  python_module: special","  structured_delegate: special_airy_ai.out","  variants: function","  tags: pointwise","","- func: special_airy_ai.out(Tensor x, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA: special_airy_ai_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_bessel_j0(Tensor self) -\u003e Tensor","  python_module: special","  structured_delegate: special_bessel_j0.out","  variants: function","  tags: pointwise","","- func: special_bessel_j0.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_bessel_j0_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_bessel_j1(Tensor self) -\u003e Tensor","  python_module: special","  structured_delegate: special_bessel_j1.out","  variants: function","  tags: pointwise","","- func: special_bessel_j1.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_bessel_j1_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_bessel_y0(Tensor self) -\u003e Tensor","  python_module: special","  structured_delegate: special_bessel_y0.out","  variants: function","  tags: pointwise","","- func: special_bessel_y0.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_bessel_y0_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_bessel_y1(Tensor self) -\u003e Tensor","  python_module: special","  structured_delegate: special_bessel_y1.out","  variants: function","  tags: pointwise","","- func: special_bessel_y1.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_bessel_y1_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_t(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_chebyshev_polynomial_t.out","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_t","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_t","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA, MPS: special_chebyshev_polynomial_t_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_t_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_t_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_u(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_chebyshev_polynomial_u.out","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_u.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_u","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_u.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_u","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA, MPS: special_chebyshev_polynomial_u_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_u_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_u_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_v(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_chebyshev_polynomial_v.out","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_v","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_v","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA, MPS: special_chebyshev_polynomial_v_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_v_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_v_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_w(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_chebyshev_polynomial_w.out","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_w","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_w","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA, MPS: special_chebyshev_polynomial_w_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_w_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_chebyshev_polynomial_w_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_h(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_hermite_polynomial_h.out","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_h.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_hermite_polynomial_h","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_h.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_hermite_polynomial_h","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_h.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA, MPS: special_hermite_polynomial_h_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_h.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_hermite_polynomial_h_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_h.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_hermite_polynomial_h_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_he(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_hermite_polynomial_he.out","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_he.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_hermite_polynomial_he","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_he.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_hermite_polynomial_he","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_he.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA, MPS: special_hermite_polynomial_he_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_he.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_hermite_polynomial_he_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_hermite_polynomial_he.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_hermite_polynomial_he_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_laguerre_polynomial_l(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_laguerre_polynomial_l.out","  variants: function","  tags: pointwise","","- func: special_laguerre_polynomial_l.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_laguerre_polynomial_l","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_laguerre_polynomial_l.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_laguerre_polynomial_l","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_laguerre_polynomial_l.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA: special_laguerre_polynomial_l_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_laguerre_polynomial_l.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_laguerre_polynomial_l_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_laguerre_polynomial_l.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_laguerre_polynomial_l_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_legendre_polynomial_p(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_legendre_polynomial_p.out","  variants: function","  tags: pointwise","","- func: special_legendre_polynomial_p.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_legendre_polynomial_p","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_legendre_polynomial_p.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_legendre_polynomial_p","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_legendre_polynomial_p.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA: special_legendre_polynomial_p_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_legendre_polynomial_p.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_legendre_polynomial_p_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_legendre_polynomial_p.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_legendre_polynomial_p_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_modified_bessel_i0(Tensor self) -\u003e Tensor","  python_module: special","  structured_delegate: special_modified_bessel_i0.out","  variants: function","  tags: pointwise","","- func: special_modified_bessel_i0.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_modified_bessel_i0_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_modified_bessel_i1(Tensor self) -\u003e Tensor","  python_module: special","  structured_delegate: special_modified_bessel_i1.out","  variants: function","  tags: pointwise","","- func: special_modified_bessel_i1.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_modified_bessel_i1_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_modified_bessel_k0(Tensor self) -\u003e Tensor","  python_module: special","  structured_delegate: special_modified_bessel_k0.out","  variants: function","  tags: pointwise","","- func: special_modified_bessel_k0.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_modified_bessel_k0_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_modified_bessel_k1(Tensor self) -\u003e Tensor","  python_module: special","  structured_delegate: special_modified_bessel_k1.out","  variants: function","  tags: pointwise","","- func: special_modified_bessel_k1.out(Tensor self, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_modified_bessel_k1_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_scaled_modified_bessel_k0(Tensor x) -\u003e Tensor","  python_module: special","  structured_delegate: special_scaled_modified_bessel_k0.out","  variants: function","  tags: pointwise","","- func: special_scaled_modified_bessel_k0.out(Tensor x, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_scaled_modified_bessel_k0_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_scaled_modified_bessel_k1(Tensor x) -\u003e Tensor","  python_module: special","  structured_delegate: special_scaled_modified_bessel_k1.out","  variants: function","  tags: pointwise","","- func: special_scaled_modified_bessel_k1.out(Tensor x, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_scaled_modified_bessel_k1_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_t(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_shifted_chebyshev_polynomial_t.out","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_t.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_t","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_t.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_t","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_t.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA, MPS: special_shifted_chebyshev_polynomial_t_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_t.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_t_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_t.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_t_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_u(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_shifted_chebyshev_polynomial_u.out","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_u.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_u","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_u.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_u","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_u.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA, MPS: special_shifted_chebyshev_polynomial_u_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_u.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_u_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_u.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_u_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_v(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_shifted_chebyshev_polynomial_v.out","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_v.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_v","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_v.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_v","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_v.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA, MPS: special_shifted_chebyshev_polynomial_v_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_v.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_v_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_v.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_v_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_w(Tensor x, Tensor n) -\u003e Tensor","  device_check: NoCheck","  python_module: special","  structured_delegate: special_shifted_chebyshev_polynomial_w.out","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_w.x_scalar(Scalar x, Tensor n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_w","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_w.n_scalar(Tensor x, Scalar n) -\u003e Tensor","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_w","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_w.out(Tensor x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  device_check: NoCheck","  dispatch:","    CPU, CUDA, MPS: special_shifted_chebyshev_polynomial_w_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_w.x_scalar_out(Scalar x, Tensor n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_w_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_shifted_chebyshev_polynomial_w.n_scalar_out(Tensor x, Scalar n, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CompositeExplicitAutograd: special_shifted_chebyshev_polynomial_w_out","  device_check: NoCheck","  python_module: special","  variants: function","  tags: pointwise","","- func: special_spherical_bessel_j0(Tensor x) -\u003e Tensor","  python_module: special","  structured_delegate: special_spherical_bessel_j0.out","  variants: function","  tags: pointwise","","- func: special_spherical_bessel_j0.out(Tensor x, *, Tensor(a!) out) -\u003e Tensor(a!)","  dispatch:","    CPU, CUDA, MPS: special_spherical_bessel_j0_out","  python_module: special","  structured_inherits: TensorIteratorBase","  structured: True","  variants: function","  tags: pointwise","","# Aux function used in the test TestPythonDispatch.test_kwarg_only_and_positional_default","# within test/test_python_dispatch.py","- func: _foobar(Tensor self, bool arg1=True, bool arg2=True, *, bool arg3=True) -\u003e Tensor","  dispatch:","    CPU: foobar","  autogen: _foobar.out","","- func: _fused_adam_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -\u003e ()","  # Unlike \"foreach\" functions, lists of tensors should be guaranteed to be on the same device (for now).","  variants: function","  dispatch:","    CPU: _fused_adam_kernel_cpu_","    CUDA: _fused_adam_kernel_cuda_","    MPS: _fused_adam_kernel_mps_","  autogen: _fused_adam, _fused_adam.out","","- func: _fused_adam_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -\u003e ()","  # Unlike \"foreach\" functions, lists of tensors should be guaranteed to be on the same device (for now),","  # but still skip the device check as the Tensor LR can be on CPU","  device_check: NoCheck","  variants: function","  dispatch:","    CPU: _fused_adam_kernel_cpu_","    CUDA: _fused_adam_kernel_cuda_","    MPS: _fused_adam_kernel_mps_","  autogen: _fused_adam.tensor_lr, _fused_adam.tensor_lr_out","","- func: _fused_adamw_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -\u003e ()","  # Unlike \"foreach\" functions, lists of tensors should be guaranteed to be on the same device (for now).","  variants: function","  dispatch:","    CPU: _fused_adamw_kernel_cpu_","    CUDA: _fused_adamw_kernel_cuda_","    MPS: _fused_adamw_kernel_mps_","  autogen: _fused_adamw, _fused_adamw.out","","- func: _fused_adamw_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, Tensor lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -\u003e ()","  # Unlike \"foreach\" functions, lists of tensors should be guaranteed to be on the same device (for now),","  # but still skip the device check as the Tensor LR can be on CPU","  device_check: NoCheck","  variants: function","  dispatch:","    CPU: _fused_adamw_kernel_cpu_","    CUDA: _fused_adamw_kernel_cuda_","    MPS: _fused_adamw_kernel_mps_","  autogen: _fused_adamw.tensor_lr, _fused_adamw.tensor_lr_out","","- func: _fused_sgd_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] momentum_buffer_list, *, float weight_decay, float momentum, float lr, float dampening, bool nesterov, bool maximize, bool is_first_step, Tensor? grad_scale=None, Tensor? found_inf=None) -\u003e ()","  # Unlike \"foreach\" functions, lists of tensors should be guaranteed to be on the same device (for now).","  variants: function","  dispatch:","    CPU: _fused_sgd_kernel_cpu_","    CUDA: _fused_sgd_kernel_cuda_","    MPS: _fused_sgd_kernel_mps_","  autogen: _fused_sgd, _fused_sgd.out","","- func: _fused_sgd_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] momentum_buffer_list, *, float weight_decay, float momentum, Tensor lr, float dampening, bool nesterov, bool maximize, bool is_first_step, Tensor? grad_scale=None, Tensor? found_inf=None) -\u003e ()","  # Unlike \"foreach\" functions, lists of tensors should be guaranteed to be on the same device (for now).","  # but still skip the device check as the Tensor LR can be on CPU","  device_check: NoCheck","  variants: function","  dispatch:","    CPU: _fused_sgd_kernel_cpu_","    CUDA: _fused_sgd_kernel_cuda_","    MPS: _fused_sgd_kernel_mps_","  autogen: _fused_sgd.tensor_lr, _fused_sgd.tensor_lr_out","","- func: _fused_adagrad_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] state_sums, Tensor(d!)[] state_steps, *, float lr, float lr_decay, float weight_decay, float eps, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -\u003e ()","  variants: function","  dispatch:","    CPU: _fused_adagrad_kernel_cpu_","    CUDA: _fused_adagrad_kernel_cuda_","  autogen: _fused_adagrad, _fused_adagrad.out","","- func: _fused_adagrad_.tensor_lr(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] state_sums, Tensor[] state_steps, *, Tensor lr, float lr_decay, float weight_decay, float eps, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -\u003e ()","  device_check: NoCheck","  variants: function","  dispatch:","    CPU: _fused_adagrad_kernel_cpu_","    CUDA: _fused_adagrad_kernel_cuda_","  autogen: _fused_adagrad.tensor_lr, _fused_adagrad.tensor_lr_out","","# This op is ONLY used by pytorch/XLA in functionalization, and should never show up in vanilla eager mode or in any pytorch tracing contexts.","- func: _propagate_xla_data(Tensor input, Tensor output) -\u003e ()","  variants: function"],"stylingDirectives":null,"colorizedLines":null,"csv":null,"csvError":null,"copilotSWEAgentEnabled":false,"dependabotInfo":{"showConfigurationBanner":false,"configFilePath":null,"networkDependabotPath":"/pytorch/pytorch/network/updates","dismissConfigurationNoticePath":"/settings/dismiss-notice/dependabot_configuration_notice","configurationNoticeDismissed":null},"displayName":"native_functions.yaml","displayUrl":"https://github.com/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml?raw=true","headerInfo":{"blobSize":"597 KB","deleteTooltip":"You must be signed in to make or propose changes","editTooltip":"You must be signed in to make or propose changes","ghDesktopPath":null,"isGitLfs":false,"onBranch":false,"shortPath":"abb061a","siteNavLoginPath":"/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fblob%2Fv2.9.1%2Faten%2Fsrc%2FATen%2Fnative%2Fnative_functions.yaml","isCSV":false,"isRichtext":false,"toc":null,"lineInfo":{"truncatedLoc":"15942","truncatedSloc":"13261"},"mode":"file"},"image":false,"isCodeownersFile":null,"isPlain":false,"isValidLegacyIssueTemplate":false,"issueTemplate":null,"discussionTemplate":null,"language":"YAML","languageID":407,"large":false,"planSupportInfo":{"repoIsFork":null,"repoOwnedByCurrentUser":null,"requestFullPath":"/pytorch/pytorch/blob/v2.9.1/aten/src/ATen/native/native_functions.yaml","showFreeOrgGatedFeatureMessage":null,"showPlanSupportBanner":null,"upgradeDataAttributes":null,"upgradePath":null},"publishBannersInfo":{"dismissActionNoticePath":"/settings/dismiss-notice/publish_action_from_dockerfile","releasePath":"/pytorch/pytorch/releases/new?marketplace=true","showPublishActionBanner":false},"rawBlobUrl":"https://github.com/pytorch/pytorch/raw/refs/tags/v2.9.1/aten/src/ATen/native/native_functions.yaml","renderImageOrRaw":false,"richText":null,"renderedFileInfo":null,"shortPath":null,"symbolsEnabled":true,"tabSize":4,"topBannersInfo":{"overridingGlobalFundingFile":false,"globalPreferredFundingPath":null,"showInvalidCitationWarning":false,"citationHelpUrl":"https://docs.github.com/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-citation-files","actionsOnboardingTip":null},"truncated":false,"viewable":true,"workflowRedirectUrl":null,"symbols":{"timed_out":false,"not_analyzed":true,"symbols":[]}},"copilotInfo":null,"copilotAccessAllowed":false,"copilotSpacesEnabled":false,"modelsAccessAllowed":false,"modelsRepoIntegrationEnabled":false,"isMarketplaceEnabled":true,"csrf_tokens":{"/pytorch/pytorch/branches":{"post":"cVKaoXvMNYwHY3h7tFLmHVbZn0-Eydx2JB2hQKamXusM50QbKSfeZwxkq-zPNYTFkFioFGoNaYpE5MbOLa5Tkw"},"/repos/preferences":{"post":"mn5g8207-Sx_5MZVHqeJDitTt1OZb10EinDnmv81th69KxBUC3MNIj5JwM2UvQcRCAR2RYSq23ThxcNLkKy8sw"}}},"title":"pytorch/aten/src/ATen/native/native_functions.yaml at v2.9.1 · pytorch/pytorch","appPayload":{"helpUrl":"https://docs.github.com","findFileWorkerPath":"/assets-cdn/worker/find-file-worker-0cea8c6113ab.js","findInFileWorkerPath":"/assets-cdn/worker/find-in-file-worker-dc3831241a86.js","githubDevUrl":null,"enabled_features":{"code_nav_ui_events":false,"react_blob_overlay":false,"accessible_code_button":true}}}</script>
  <div data-target="react-app.reactRoot"></div>
</react-app>
</turbo-frame>



  </div>

</turbo-frame>

    </main>
  </div>

  </div>

          <footer class="footer pt-7 pb-6 f6 color-fg-muted color-border-subtle p-responsive" role="contentinfo" >
  <h2 class='sr-only'>Footer</h2>

  


  <div class="d-flex flex-justify-center flex-items-center flex-column-reverse flex-lg-row flex-wrap flex-lg-nowrap">
    <div class="d-flex flex-items-center flex-shrink-0 mx-2">
      <a aria-label="GitHub Homepage" class="footer-octicon mr-2" href="https://github.com">
        <svg aria-hidden="true" height="24" viewBox="0 0 24 24" version="1.1" width="24" data-view-component="true" class="octicon octicon-mark-github">
    <path d="M12 1C5.923 1 1 5.923 1 12c0 4.867 3.149 8.979 7.521 10.436.55.096.756-.233.756-.522 0-.262-.013-1.128-.013-2.049-2.764.509-3.479-.674-3.699-1.292-.124-.317-.66-1.293-1.127-1.554-.385-.207-.936-.715-.014-.729.866-.014 1.485.797 1.691 1.128.99 1.663 2.571 1.196 3.204.907.096-.715.385-1.196.701-1.471-2.448-.275-5.005-1.224-5.005-5.432 0-1.196.426-2.186 1.128-2.956-.111-.275-.496-1.402.11-2.915 0 0 .921-.288 3.024 1.128a10.193 10.193 0 0 1 2.75-.371c.936 0 1.871.123 2.75.371 2.104-1.43 3.025-1.128 3.025-1.128.605 1.513.221 2.64.111 2.915.701.77 1.127 1.747 1.127 2.956 0 4.222-2.571 5.157-5.019 5.432.399.344.743 1.004.743 2.035 0 1.471-.014 2.654-.014 3.025 0 .289.206.632.756.522C19.851 20.979 23 16.854 23 12c0-6.077-4.922-11-11-11Z"></path>
</svg>
</a>
      <span>
        &copy; 2026 GitHub,&nbsp;Inc.
      </span>
    </div>

    <nav aria-label="Footer">
      <h3 class="sr-only" id="sr-footer-heading">Footer navigation</h3>

      <ul class="list-style-none d-flex flex-justify-center flex-wrap mb-2 mb-lg-0" aria-labelledby="sr-footer-heading">

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to Terms&quot;,&quot;label&quot;:&quot;text:terms&quot;}" href="https://docs.github.com/site-policy/github-terms/github-terms-of-service" data-view-component="true" class="Link--secondary Link">Terms</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to privacy&quot;,&quot;label&quot;:&quot;text:privacy&quot;}" href="https://docs.github.com/site-policy/privacy-policies/github-privacy-statement" data-view-component="true" class="Link--secondary Link">Privacy</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to security&quot;,&quot;label&quot;:&quot;text:security&quot;}" href="https://github.com/security" data-view-component="true" class="Link--secondary Link">Security</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to status&quot;,&quot;label&quot;:&quot;text:status&quot;}" href="https://www.githubstatus.com/" data-view-component="true" class="Link--secondary Link">Status</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to community&quot;,&quot;label&quot;:&quot;text:community&quot;}" href="https://github.community/" data-view-component="true" class="Link--secondary Link">Community</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to docs&quot;,&quot;label&quot;:&quot;text:docs&quot;}" href="https://docs.github.com/" data-view-component="true" class="Link--secondary Link">Docs</a>
          </li>

          <li class="mx-2">
            <a data-analytics-event="{&quot;category&quot;:&quot;Footer&quot;,&quot;action&quot;:&quot;go to contact&quot;,&quot;label&quot;:&quot;text:contact&quot;}" href="https://support.github.com?tags=dotcom-footer" data-view-component="true" class="Link--secondary Link">Contact</a>
          </li>

          <li class="mx-2" >
  <cookie-consent-link>
    <button
      type="button"
      class="Link--secondary underline-on-hover border-0 p-0 color-bg-transparent"
      data-action="click:cookie-consent-link#showConsentManagement"
      data-analytics-event="{&quot;location&quot;:&quot;footer&quot;,&quot;action&quot;:&quot;cookies&quot;,&quot;context&quot;:&quot;subfooter&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;cookies_link_subfooter_footer&quot;}"
    >
       Manage cookies
    </button>
  </cookie-consent-link>
</li>

<li class="mx-2">
  <cookie-consent-link>
    <button
      type="button"
      class="Link--secondary underline-on-hover border-0 p-0 color-bg-transparent text-left"
      data-action="click:cookie-consent-link#showConsentManagement"
      data-analytics-event="{&quot;location&quot;:&quot;footer&quot;,&quot;action&quot;:&quot;dont_share_info&quot;,&quot;context&quot;:&quot;subfooter&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;dont_share_info_link_subfooter_footer&quot;}"
    >
      Do not share my personal information
    </button>
  </cookie-consent-link>
</li>

      </ul>
    </nav>
  </div>
</footer>



    <ghcc-consent id="ghcc" class="position-fixed bottom-0 left-0" style="z-index: 999999"
      data-locale="en"
      data-initial-cookie-consent-allowed=""
      data-cookie-consent-required="true"
    ></ghcc-consent>




  <div id="ajax-error-message" class="ajax-error-message flash flash-error" hidden>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-alert">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
    <button type="button" class="flash-close js-ajax-error-dismiss" aria-label="Dismiss error">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
    </button>
    You can’t perform that action at this time.
  </div>

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open>
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog>
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    <div class="Popover js-hovercard-content position-absolute" style="display: none; outline: none;">
  <div class="Popover-message Popover-message--bottom-left Popover-message--large Box color-shadow-large" style="width:360px;">
  </div>
</div>

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div>
    <div id="js-global-screen-reader-notice" class="sr-only mt-n1" aria-live="polite" aria-atomic="true" ></div>
    <div id="js-global-screen-reader-notice-assertive" class="sr-only mt-n1" aria-live="assertive" aria-atomic="true"></div>
  </body>
</html>

