/*
 * Generated by emx-pytorch2c export_generic_c.
 * Options:
 *   function_name: 'entry'
 *   truncate_weights_after: 10
 *   temp_allocation_threshold: 1024
 *   variable_dim_inputs: {}
 *   variable_dim_outputs: {}
 */
#include <stdint.h>
#include <sys/types.h>
#include <stdbool.h>
#include <stdlib.h>
#include <float.h>
#include <math.h>

static const float weight_Conv_8_weight[20][1][5][5] = {
    {
        {
            {
                -0x1.012de7p-2f, -0x1.294a77p-2f, 0x1.6d9813p-4f, 0x1.1947f9p-2f, 0x1.21cd4bp-3f
            },
            {
                -0x1.5237a4p-3f, -0x1.08c456p-3f, 0x1.2ae7e9p-2f, 0x1.0344c0p-1f, 0x1.735e1dp-3f
            },
            {
                ...
            }
        }
    }
};

static const float weight_Conv_10_weight[12][20][3][3] = {
    {
        {
            {
                -0x1.29fb62p-3f, -0x1.73fab8p-4f, 0x1.177409p-4f
            },
            {
                0x1.74ded0p-4f, 0x1.55001dp-4f, 0x1.14d81bp-4f
            },
            {
                0x1.62ad76p-4f, 0x1.214d93p-4f, 0x1.2a39bap-5f
            }
        },
        {
            {
                0x1.7e125ap-9f, ...
            }
        }
    }
};

static const int64_t weight_tensor_constant0[1] = {
    1
};

static const int64_t weight_tensor_constant1[1] = {
    -1
};

static const float weight_Gemm_network_output_weight[10][108] = {
    {
        -0x1.5ffb6ep-4f, -0x1.36f87dp-4f, -0x1.2bf395p-3f, -0x1.456cc4p-3f, 0x1.5894a3p-6f, -0x1.7af660p-5f, -0x1.1c7d25p-3f, 0x1.3819e1p-4f,
        0x1.1eaf43p-6f, -0x1.1e73d9p-4f, ...
    }
};

static const float weight_Gemm_network_output_bias[10] = {
    -0x1.33b636p-2f, 0x1.5cf820p-4f, 0x1.4eb986p-4f, 0x1.404075p-7f, -0x1.639f6ap-4f, 0x1.5646ccp-2f, -0x1.1be03ep-3f, 0x1.0e9310p-7f,
    -0x1.553391p-4f, 0x1.1ae21ep-4f
};

#ifndef REF_PI_F
#define REF_PI_F 3.14159265358979323846f
#endif
#ifndef REF_PI_D
#define REF_PI_D 3.14159265358979323846
#endif

static inline float ref_scalar_f32_relu(float a) {
    return a > 0.0f ? a : 0.0f;
}

/*
* op: conv2d (kind: conv2d)
* inputs: [shape=(1, 1, 14, 14), size=196, shape=(20, 1, 5, 5), size=500]
* output: shape=(1, 20, 5, 5), size=500
* params: {'stride': (2, 2), 'padding': (0, 0), 'dilation': (1, 1), 'groups': 1, 'transposed': False, 'output_padding': (0, 0), 'has_bias': False}
*/
void node1_conv2d_f32(const float input[1][1][14][14], const float weight[20][1][5][5], float out[1][20][5][5]) {
    ssize_t in_per_group = 1 / 1;
    ssize_t out_per_group = 20 / 1;
    ssize_t out_pad_h = 0;
    ssize_t out_pad_w = 0;
    (void)out_pad_h;
    (void)out_pad_w;
    for (ssize_t n = 0; n < 1; ++n) {
        for (ssize_t oc = 0; oc < 20; ++oc) {
            ssize_t group = (ssize_t)oc / out_per_group;
            for (ssize_t oh = 0; oh < 5; ++oh) {
                for (ssize_t ow = 0; ow < 5; ++ow) {
                    float acc = 0.0f;
                    ssize_t in_h_base = (ssize_t)oh * 2 - 0;
                    ssize_t in_w_base = (ssize_t)ow * 2 - 0;
                    for (ssize_t ic = 0; ic < in_per_group; ++ic) {
                        ssize_t in_c = group * in_per_group + (ssize_t)ic;
                        for (ssize_t kh = 0; kh < 5; ++kh) {
                            ssize_t in_h_idx = in_h_base + (ssize_t)kh * 1;
                            if (in_h_idx < 0 || in_h_idx >= 14) {
                                continue;
                            }
                            for (ssize_t kw = 0; kw < 5; ++kw) {
                                ssize_t in_w_idx = in_w_base + (ssize_t)kw * 1;
                                if (in_w_idx < 0 || in_w_idx >= 14) {
                                    continue;
                                }
                                acc += input[n][in_c][in_h_idx][in_w_idx] * weight[oc][ic][kh][kw];
                            }
                        }
                    }
                    out[n][oc][oh][ow] = acc;
                }
            }
        }
    }
}

/*
* op: relu (kind: unary)
* inputs: [shape=(1, 20, 5, 5), size=500]
* output: shape=(1, 20, 5, 5), size=500
* params: {}
*/
void node2_relu_f32(const float a[1][20][5][5], float out[1][20][5][5]) {
    for (ssize_t i0 = 0; i0 < 1; ++i0) {
        for (ssize_t i1 = 0; i1 < 20; ++i1) {
            for (ssize_t i2 = 0; i2 < 5; ++i2) {
                for (ssize_t i3 = 0; i3 < 5; ++i3) {
                    out[i0][i1][i2][i3] = ref_scalar_f32_relu(a[i0][i1][i2][i3]);
                }
            }
        }
    }
}

/*
* op: conv2d (kind: conv2d)
* inputs: [shape=(1, 20, 5, 5), size=500, shape=(12, 20, 3, 3), size=2160]
* output: shape=(1, 12, 3, 3), size=108
* params: {'stride': (1, 1), 'padding': (0, 0), 'dilation': (1, 1), 'groups': 1, 'transposed': False, 'output_padding': (0, 0), 'has_bias': False}
*/
void node3_conv2d_f32(const float input[1][20][5][5], const float weight[12][20][3][3], float out[1][12][3][3]) {
    ssize_t in_per_group = 20 / 1;
    ssize_t out_per_group = 12 / 1;
    ssize_t out_pad_h = 0;
    ssize_t out_pad_w = 0;
    (void)out_pad_h;
    (void)out_pad_w;
    for (ssize_t n = 0; n < 1; ++n) {
        for (ssize_t oc = 0; oc < 12; ++oc) {
            ssize_t group = (ssize_t)oc / out_per_group;
            for (ssize_t oh = 0; oh < 3; ++oh) {
                for (ssize_t ow = 0; ow < 3; ++ow) {
                    float acc = 0.0f;
                    ssize_t in_h_base = (ssize_t)oh * 1 - 0;
                    ssize_t in_w_base = (ssize_t)ow * 1 - 0;
                    for (ssize_t ic = 0; ic < in_per_group; ++ic) {
                        ssize_t in_c = group * in_per_group + (ssize_t)ic;
                        for (ssize_t kh = 0; kh < 3; ++kh) {
                            ssize_t in_h_idx = in_h_base + (ssize_t)kh * 1;
                            if (in_h_idx < 0 || in_h_idx >= 5) {
                                continue;
                            }
                            for (ssize_t kw = 0; kw < 3; ++kw) {
                                ssize_t in_w_idx = in_w_base + (ssize_t)kw * 1;
                                if (in_w_idx < 0 || in_w_idx >= 5) {
                                    continue;
                                }
                                acc += input[n][in_c][in_h_idx][in_w_idx] * weight[oc][ic][kh][kw];
                            }
                        }
                    }
                    out[n][oc][oh][ow] = acc;
                }
            }
        }
    }
}

/*
* op: reshape (kind: view)
* inputs: [shape=(1, 12, 3, 3), size=108]
* output: shape=(1, 108), size=108
* params: {'size': (1, 108), 'view_strides': (108, 1), 'storage_offset': 0}
*/
void node4_reshape_f32(const float a[1][12][3][3], float out[1][108]) {
    const float* a_ptr = (const float*)a;
    for (ssize_t i0 = 0; i0 < 1; ++i0) {
        for (ssize_t i1 = 0; i1 < 108; ++i1) {
            ssize_t offset = (ssize_t)i0 * (ssize_t)108 + (ssize_t)i1 * (ssize_t)1;
            out[i0][i1] = a_ptr[offset];
        }
    }
}

/*
* op: linear (kind: linear)
* inputs: [shape=(1, 108), size=108, shape=(10, 108), size=1080, shape=(10,), size=10]
* output: shape=(1, 10), size=10
* params: {'has_bias': True}
*/
void node5_linear_f32(const float input[1][108], const float weight[10][108], const float bias[10], float out[1][10]) {
    for (ssize_t i0 = 0; i0 < 1; ++i0) {
        for (ssize_t j = 0; j < 10; ++j) {
            float acc = 0.0f;
            for (ssize_t t = 0; t < 108; ++t) {
                acc += input[i0][t] * weight[j][t];
            }
            out[i0][j] = acc + bias[j];
        }
    }
}

void ref_codegen_main_f32(const float input_0[1][1][14][14], const float input_1[20][1][5][5], const float input_2[12][20][3][3], const float input_3[10][108], const float input_4[10], float out[1][10]) {
    float tmp_2[1][12][3][3];
    float tmp_3[1][108];
    float (*tmp_0)[20][5][5] = malloc(sizeof(float) * 1 * 20 * 5 * 5);
    float (*tmp_1)[20][5][5] = malloc(sizeof(float) * 1 * 20 * 5 * 5);
    node1_conv2d_f32(input_0, input_1, tmp_0);
    node2_relu_f32(tmp_0, tmp_1);
    node3_conv2d_f32(tmp_1, input_2, tmp_2);
    node4_reshape_f32(tmp_2, tmp_3);
    node5_linear_f32(tmp_3, input_3, input_4, out);
    free(tmp_0);
    free(tmp_1);
}

void entry(const float in0[1][1][14][14], float out0[1][10]) {
    ref_codegen_main_f32(in0, weight_Conv_8_weight, weight_Conv_10_weight, weight_Gemm_network_output_weight, weight_Gemm_network_output_bias, out0);
}
